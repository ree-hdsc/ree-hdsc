{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acff869-67df-4b20-a939-86b1b609059e",
   "metadata": {},
   "source": [
    "# Info fields via machine learning\n",
    "\n",
    "Extract persons from the info fields StartEntryInfo and EndEntryInfo of the [slave registers of Suriname](https://datasets.iisg.amsterdam/dataset.xhtml?persistentId=hdl:10622/CSPBHO) via machine learning\n",
    "\n",
    "See: https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afe6d3-87bf-4c94-b48d-9cf0b81c1a4c",
   "metadata": {},
   "source": [
    "## 1. Annotating info fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39531b3-deb1-4d22-9586-de290836ee04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8ca70-3fd4-4d1b-8cff-a2815842e39c",
   "metadata": {},
   "source": [
    "### 1.1 Read relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa999620-9243-433e-9e92-63ffa5607c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/suriname/Dataset Suriname Slave and Emancipation Registers Version 1.1.csv\"\n",
    "DATA_COLUMN = \"EndEntryInfo\"\n",
    "\n",
    "data = pd.read_csv(DATA_FILE, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cdd66-c9bc-40f4-a145-73228319d7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_tokens(train):\n",
    "    train[\"tokens\"] = [ nltk.word_tokenize(text) for text in train[\"text\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3f692-e25b-4fca-aed5-98c2b8211153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_labels(train):\n",
    "    train[\"labels\"] = [ len(tokens) * [ \"O\" ] for tokens in train[\"tokens\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22f9b5-a3c3-46d7-b0fb-34a7b2f37581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_numeric_labels(train, numeric_labels):\n",
    "    train[\"numeric_labels\"] = [ [ numeric_labels[label] for label in labels ] for labels in train[\"labels\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17279108-eb94-46c0-87ed-655e84eeeef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_date(day, month, year):\n",
    "    return regex.search(r\"^\\d\\d\\d\\d\\b\", year) and regex.search(r\"^\\d\\d?$\", day) and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09845b-a11a-46e3-8041-ac4128800650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_date_tags_to_labels(labels, index):\n",
    "    labels[index - 2], labels[index - 1], labels[index] = \"B-DATE\", \"I-DATE\", \"I-DATE\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ca68a-b24e-4f94-b1d0-a05737cc549e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_dates(train):\n",
    "    for index, row in train.iterrows():\n",
    "        for i in range(2, len(row[\"tokens\"])):\n",
    "            if is_date(row[\"tokens\"][i-2], row[\"tokens\"][i-1], row[\"tokens\"][i]):\n",
    "                add_date_tags_to_labels(row[\"labels\"], i)\n",
    "    return train       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333bfb9-6b81-4ce5-9927-12a1e86d7d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_annotations(train):\n",
    "    for index in range(0, len(train)):\n",
    "        for i in range(0, len(train[\"labels\"][index])):\n",
    "            print(train[\"tokens\"][index][i], end=\"\")\n",
    "            if train[\"labels\"][index][i] != \"O\":\n",
    "                print(\"/\" + train[\"labels\"][index][i], end=\"\")\n",
    "            print(\" \", end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fe91b-e883-406e-b7af-eb8f7a198a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train(data, nbr_of_lines=100):\n",
    "    if nbr_of_lines > 0:\n",
    "        train = pd.DataFrame(data[DATA_COLUMN].value_counts()[:nbr_of_lines])\n",
    "    else:\n",
    "        train = pd.DataFrame(data[DATA_COLUMN].value_counts())\n",
    "    train = train.rename(columns={DATA_COLUMN: \"frequency\"})\n",
    "    train[\"text\"] = train.index\n",
    "    train[\"index\"] = range(0, len(train))\n",
    "    train = train.set_index(\"index\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aff36-ce1e-472c-963c-c23dcf63b316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_data_train = make_train(data, nbr_of_lines=0)\n",
    "info_data_train = add_column_tokens(info_data_train)\n",
    "info_data_train = add_column_labels(info_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d70616-ba02-4cb8-9c7e-6c0eebbebc05",
   "metadata": {},
   "source": [
    "### 1.2 Make data for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a1e97-4794-457a-ba96-383fac6fc17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SELECTED_FREQUENT = 100\n",
    "SELECTED_RANDOM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ded55-aebe-4f6f-b071-7263a04d4a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_selected_data_ids(info_data_train, selected_frequent=SELECTED_FREQUENT, selected_random=SELECTED_RANDOM):\n",
    "    selected_data_ids = list(range(0, selected_frequent))\n",
    "    while len(selected_data_ids) < selected_frequent + selected_random:\n",
    "        selected_data_id = random.randint(selected_frequent, len(info_data_train) - 1)\n",
    "        if selected_data_id not in selected_data_ids:\n",
    "            selected_data_ids.append(selected_data_id)\n",
    "    return selected_data_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c2de4-534d-4447-b1af-3180ea593242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_selected_data_flags(info_data_train, selected_data_ids):\n",
    "    selected_data_flags = len(info_data_train) * [ False ]\n",
    "    for id_value in selected_data_ids:\n",
    "        selected_data_flags[id_value] = True\n",
    "    return selected_data_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f518425-2956-41d4-a8fa-04a382eade0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_annotated_data(info_data_train, selected_data_flags):\n",
    "    out_file = open(\"outfile.json\", \"w\")\n",
    "    selected_data = []\n",
    "    for index, row in info_data_train[selected_data_flags].iterrows():\n",
    "        text = \" \".join(row[\"tokens\"])\n",
    "        selected_data.append({ \"eid\": DATA_COLUMN[0] + str(index), \"text\": text, \"label\": [] })\n",
    "        print(selected_data[-1], file=out_file)\n",
    "    out_file.close()\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c62af9-2653-48fe-b0ca-49bc18752081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data(info_data_train, selected_frequent=SELECTED_FREQUENT, selected_random=SELECTED_RANDOM):\n",
    "    random.seed(42)\n",
    "    selected_data_ids = make_selected_data_ids(info_data_train, selected_frequent, selected_random)\n",
    "    selected_data_flags = make_selected_data_flags(info_data_train, selected_data_ids)\n",
    "    selected_data = save_annotated_data(info_data_train, selected_data_flags)\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6687544-ff14-4a42-936b-90ff68cd0af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_data = make_data(info_data_train, selected_frequent=SELECTED_FREQUENT, selected_random=SELECTED_RANDOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa63a6-fd28-4e27-bcf0-13b35794976c",
   "metadata": {},
   "source": [
    "## 2. Machine learning\n",
    "\n",
    "Based on tutorial https://huggingface.co/transformers/v3.2.0/custom_datasets.html#token-classification-with-w-nut-emerging-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed17038-ae39-4b27-9185-f14863fcbfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import regex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy import displacy\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416ec63-1bf8-4d78-8324-4e690d8b8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_text(text, entities):\n",
    "    displacy.render({ \"text\": regex.sub(\"\\\\n\", \" \", text), \n",
    "                      \"ents\": entities }, \n",
    "                      options = { \"colors\": { \"fuzzy_match\": \"yellow\"} }, style = \"ent\", manual = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b9dcc-b26a-49a2-b5ed-ea25e6cd6d1b",
   "metadata": {},
   "source": [
    "### 2.1 Read annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2394b87-dfac-4e39-afd8-9dec35cb68dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ANNOTATIONS_FILE = \"../../data/annotated/700.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b241a2c-4d89-4bd3-99b3-2883a61737bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_offset2label_pos(text):\n",
    "    offset2label_pos = {}\n",
    "    offset = 0\n",
    "    token_counter = 0\n",
    "    for token in text.split():\n",
    "        offset2label_pos[offset] = token_counter\n",
    "        offset += len(token) + 1\n",
    "        token_counter += 1\n",
    "    return offset2label_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e4822-9585-48ba-82bf-88416b1f6201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_label_start_not_token_initial(text, label_start):\n",
    "    while regex.search(\" \", text[label_start]):\n",
    "        label_start += 1\n",
    "    while label_start > 0 and not regex.search(\" \", text[label_start - 1]):\n",
    "        label_start -= 1\n",
    "    return label_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff681a-79fc-488d-830b-11e36a17bccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    text = data[\"data\"]\n",
    "    labels = [ \"O\" for token in text.split() ]\n",
    "    offset2label_pos = make_offset2label_pos(text)\n",
    "    for label in data[\"label\"]:\n",
    "        label[0] = fix_label_start_not_token_initial(text, label[0])\n",
    "        if label[0] not in offset2label_pos:\n",
    "            raise Exception(f\"{label[0]} not found in labels {offset2label_pos} of text {text}\")\n",
    "        else:\n",
    "            labels[offset2label_pos[label[0]]] = \"B-\" + label[2]\n",
    "            for i in range(label[0] + 1, label[1] + 1):\n",
    "                if i in offset2label_pos:\n",
    "                    labels[offset2label_pos[i]] = \"I-\" + label[2]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea277fd-a04c-421e-a4e9-b43c95fcf353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_jsonl_file(file_name):\n",
    "    annotations_file = open(file_name, \"r\")\n",
    "    texts = []\n",
    "    tags = []\n",
    "    for line in annotations_file:\n",
    "        data = json.loads(line)\n",
    "        texts.append(data[\"data\"].split())\n",
    "        tags.append(make_labels(data))\n",
    "    annotations_file.close()\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c528a02-d9a9-4eda-a22b-f04ab90d606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(annotated_texts, annotated_tags):\n",
    "    seen = {}\n",
    "    items_to_delete = []\n",
    "    for i in range(0, len(annotated_texts)):\n",
    "        text = annotated_texts[i]\n",
    "        if str(text) in seen:\n",
    "            print(text)\n",
    "            items_to_delete = [i] + items_to_delete\n",
    "        seen[str(text)] = True\n",
    "    return items_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a62f3-7f6f-474f-9a8b-4d0bdb12d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(annotated_texts, annotated_tags):\n",
    "    items_to_delete = find_duplicates(annotated_texts, annotated_tags)\n",
    "    for i in items_to_delete:\n",
    "        annotated_texts.pop(i)\n",
    "        annotated_tags.pop(i)\n",
    "    return annotated_texts, annotated_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87610b03-fd82-46fc-b491-af84c13c43a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotated_texts, annotated_tags = read_jsonl_file(ANNOTATIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee51689-1f0a-450f-b8b4-3261f0a46bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_texts, annotated_tags = remove_duplicates(annotated_texts, annotated_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593d8cf-b441-4ee9-aabe-c4c25caa6ee5",
   "metadata": {},
   "source": [
    "### 2.2 Convert data to train set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aea3f-482a-4421-ab09-6abb206d8c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_missing_I_tags(tags):\n",
    "    missing_tags = []\n",
    "    for tag in tags:\n",
    "        i_tag = regex.sub(r\"^B-\", \"I-\", tag)\n",
    "        if i_tag not in tags:\n",
    "            missing_tags.append(i_tag)\n",
    "    return list(tags) + missing_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cbe0e-f7ea-4bdc-9854-60cb9838fa55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_tags, val_tags = train_test_split(annotated_texts, \n",
    "                                                                annotated_tags, \n",
    "                                                                test_size=.2, \n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2ee85-783f-4af8-bac2-b5a1f012d16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in annotated_tags for tag in doc )\n",
    "unique_tags = add_missing_I_tags(unique_tags)\n",
    "unique_types = list(set([ regex.sub(r\"^[BI]-\", \"\", tag) for tag in unique_tags ]))\n",
    "tag2id = { tag: id for id, tag in enumerate(unique_tags) }\n",
    "id2tag = { id: tag for tag, id in tag2id.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974af327-db79-4dd6-b93a-ca1d707bdd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96866fb1-e997-46e9-9148-04e45c534e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)\n",
    "val_encodings =   tokenizer(val_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e4673-8459-42ee-a5aa-221306002e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_B_to_I_tag(tag):\n",
    "    return regex.sub(r\"^B\", \"I\", tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f431ba-9046-47cd-a82a-d0441193e59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_tags(tags_in, encodings):\n",
    "    tags_out = [ [] for _ in range(len(encodings.offset_mapping,)) ]\n",
    "    for encodings_doc, tags_in_doc, tags_out_doc in zip(encodings.offset_mapping, tags_in, tags_out):\n",
    "        CLS_seen = False\n",
    "        SEP_seen = False\n",
    "        tags_counter = 0\n",
    "        for encoding in encodings_doc:\n",
    "            if encoding[1] == 0:\n",
    "                if not CLS_seen:\n",
    "                    tags_out_doc.append(\"CLS\")\n",
    "                    CLS_seen = True\n",
    "                elif not SEP_seen:\n",
    "                    tags_out_doc.append(\"SEP\")\n",
    "                    SEP_seen = True\n",
    "                else:\n",
    "                    tags_out_doc.append(\"PAD\")\n",
    "            elif encoding[0] == 0:\n",
    "                tags_out_doc.append(tags_in_doc[tags_counter])\n",
    "                tags_counter += 1\n",
    "            else:\n",
    "                tags_out_doc.append(convert_B_to_I_tag(tags_in_doc[tags_counter - 1]))\n",
    "    return tags_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cee79-11ae-4a73-af85-82bba30cb4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tags_to_numbers(tags, tag2id):\n",
    "    return [ [ tag2id[tag] for tag in doc ] for doc in tags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ec5ba-1445-4714-b326-66c46eea3719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IGNORE_TAG_ID = -100\n",
    "\n",
    "extra_tags = { 'CLS': IGNORE_TAG_ID, 'SEP': IGNORE_TAG_ID, 'PAD': IGNORE_TAG_ID }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671b280-dcf8-4bce-9c1e-4a73b64305ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = tags_to_numbers( split_tags(train_tags, train_encodings),\n",
    "                                { **tag2id, **extra_tags})\n",
    "val_labels =   tags_to_numbers( split_tags(val_tags, val_encodings),\n",
    "                                { **tag2id, **extra_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba62d6-0caa-49f7-a63f-bfcc242e745c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346470a9-6199-45e6-8948-23b45c1f36d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aa58a-c202-4004-b27d-589a995c97f7",
   "metadata": {},
   "source": [
    "### 2.3 Fine-tune model with data\n",
    "\n",
    "Using Bertje as base model: https://huggingface.co/GroNLP/bert-base-dutch-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166a186-b964-4d3d-a044-cb6e621c4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('./model/cstom-setfit-model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0975a-154d-4e20-a9fb-8454a47b6822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"GroNLP/bert-base-dutch-cased\", num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b1689-657d-48a0-b9d6-926c9f609f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForTokenClassification.from_pretrained(\"./model\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7304d-d4fb-4314-bdd5-8451b149f87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=7,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,              # training arguments, defined above\n",
    "    train_dataset=train_dataset,     # training dataset\n",
    "    eval_dataset=val_dataset         # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ff458-e1b9-4f9d-a906-a4c764fd34bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_data = { \"400a\": { 10: [ 2.973100 , 2.855659], 20: [2.874400 ,2.633519], 30 : [2.598700 ,2.267503], 40 : [2.211500 ,1.768432], 50 : [1.778300 ,1.324655],\n",
    "60:[ 1.550900, 1.093353], 70: [ 1.220000, 0.903201], 80: [ 1.011200, 0.697839], 90 : [0.778800 ,0.554128], 100 : [0.623100 ,0.440024],\n",
    "110: [0.488200, 0.349560], 120: [ 0.343000, 0.288387], 130: [ 0.300600, 0.250258], 140: [ 0.226900, 0.229932], 150: [ 0.170800, 0.200462],\n",
    "160: [ 0.173200, 0.177333], 170: [ 0.101600, 0.182475], 180: [ 0.108400, 0.170770], 190: [ 0.074900, 0.172628], 200: [ 0.075700, 0.177106],\n",
    "210: [ 0.054100, 0.168911], 220: [ 0.047600, 0.179788], 230: [ 0.038200, 0.168373], 240: [ 0.039000, 0.165006], 250: [ 0.030100, 0.163445],\n",
    "260: [ 0.027400, 0.174015], 270: [ 0.018400, 0.171961], 280: [ 0.024600, 0.178531], 290: [ 0.015000, 0.189336], 300: [ 0.019300, 0.185390],\n",
    "310: [ 0.014100, 0.179444], 320: [ 0.009900, 0.196208], 330: [ 0.017200, 0.177975], 340: [ 0.007100, 0.188561], 350: [ 0.011200, 0.175972],\n",
    "360: [ 0.006600, 0.179566], 370: [ 0.007400, 0.180530], 380: [ 0.012900, 0.199687], 390: [ 0.005000, 0.191614], 400: [ 0.003700, 0.178989] },\n",
    "\"600a\": { 10: [ 2.853400, 2.788824], 20: [ \t2.739800, \t2.618280], 30: [ 2.518100, 2.352505], 40: [ 2.215600, 2.010691], 50: [ 1.876800, 1.643065],\n",
    "60: [ 1.569300, 1.363279], 70: [ 1.279300, 1.098443], 80: [ 1.041300, 0.855244], 90: [ 0.854200, 0.671153], 100: [ 0.616700, 0.533351],\n",
    "110: [ 0.562700, 0.431195], 120: [ 0.449400, 0.350870], 130: [ 0.334900, 0.301453], 140: [ 0.261000, 0.252131], 150: [ 0.290400, 0.220683],\n",
    "160: [ 0.190300, 0.203238], 170: [ 0.154600, 0.190547], 180: [ 0.185800, 0.167484], 190: [ 0.108400, 0.160793], 200: [ 0.101400, 0.145170],\n",
    "210: [ 0.088100, 0.133172], 220: [ 0.081000, 0.135661], 230: [ 0.068500, 0.159878], 240: [ 0.052700, 0.146953], 250: [ 0.038300, 0.169547],\n",
    "260: [ 0.037700, 0.152970], 270: [ 0.065600, 0.140576], 280: [ 0.027200, 0.173946], 290: [ 0.022900, 0.149843], 300: [ 0.023700, 0.155466] },\n",
    "\"600b\": {10: [2.97, 2.932976007461548], 20: [2.8959, 2.7588181495666504], 30: [2.6404, 2.4754464626312256], 40: [2.3043, 2.089912176132202],\n",
    "50: [1.9301, 1.6922649145126343], 60: [1.6257, 1.433002233505249], 70: [1.3531, 1.1809186935424805], 80: [1.1305, 0.9395545125007629],\n",
    "90: [0.9414, 0.735508143901825], 100: [0.673, 0.5727341771125793], 110: [0.6081, 0.4573941230773926], 120: [0.4872, 0.3719801604747772],\n",
    "130: [0.357, 0.3091298043727875], 140: [0.2731, 0.2592621445655823], 150: [0.3059, 0.23325742781162262], 160: [0.2005, 0.20716261863708496],\n",
    "170: [0.1691, 0.21426241099834442], 180: [0.1963, 0.17854894697666168], 190: [0.1302, 0.17267706990242004], 200: [0.1132, 0.1582951843738556],\n",
    "210: [0.0941, 0.1499401330947876]},\n",
    "\"600c\": {10: [3.0284, 2.9702017307281494], 20: [2.9019, 2.7836408615112305], 30: [2.6567, 2.4800596237182617], 40: [2.3059, 2.0758652687072754],\n",
    " 50: [1.9075, 1.65205717086792], 60: [1.583, 1.3605468273162842], 70: [1.285, 1.108488917350769], 80: [1.0662, 0.8703526258468628],\n",
    " 90: [0.8719, 0.6816078424453735], 100: [0.6273, 0.5537048578262329], 110: [0.5864, 0.4511786103248596], 120: [0.4745, 0.3668432831764221],\n",
    "130: [0.3503, 0.3144031763076782], 140: [0.2816, 0.264466255903244], 150: [0.3042, 0.2330472618341446], 160: [0.1985, 0.21090537309646606],\n",
    "170: [0.1735, 0.20016834139823914], 180: [0.1975, 0.17414356768131256], 190: [0.1271, 0.173195019364357], 200: [0.1204, 0.16945882141590118],\n",
    "210: [0.099, 0.1487908661365509], 220: [0.0798, 0.14753003418445587], 230: [0.0831, 0.14581818878650665], 240: [0.0679, 0.14583609998226166]},\n",
    "\"700a\": { 10:[ \t2.938300, \t2.900105], 20:[ \t2.838300, \t2.743521], 30:[ \t2.649400, \t2.491823], 40:[ \t2.343200, \t2.163887],\n",
    "50:[ \t2.032500, \t1.807033], 60:[ \t1.645100, \t1.522994], 70:[ \t1.423600, \t1.299871], 80:[ \t1.233000, \t1.048253],\n",
    "90:[ \t0.964700, \t0.846890], 100:[ \t0.770100, \t0.695937], 110:[ \t0.573500, \t0.582498], 120:[ \t0.458100, \t0.476454],\n",
    "130:[ \t0.457500, \t0.393108], 140:[ \t0.359800, \t0.332941], 150:[ \t0.248000, \t0.309698], 160:[ \t0.223600, \t0.275177],\n",
    "170:[ \t0.189700, \t0.240412], 180:[ \t0.171500, \t0.225659], 190:[ \t0.115100, \t0.214786], 200:[ \t0.094900, \t0.210268],\n",
    "210:[ \t0.094800, \t0.218391], 220:[ \t0.059200, \t0.219438], 230:[ \t0.052300, \t0.225003], 240:[ \t0.071400, \t0.240677],\n",
    "250:[ \t0.049300, \t0.207180], 260:[ \t0.040400, \t0.221386], 270:[ \t0.048800, \t0.221953], 280:[ \t0.028200, \t0.244117]},\n",
    "\"700b\": {10: [2.8077, 2.74599], 20: [2.7203, 2.598261], 30: [2.5386, 2.361184], 40: [2.2356, 2.056],\n",
    " 50: [1.9691, 1.759227], 60: [1.657, 1.536577], 70: [1.4585, 1.312596], 80: [1.2722, 1.060864],\n",
    "90: [0.9803, 0.850179], 100: [0.7778, 0.701402], 110: [0.5651, 0.578009], 120: [0.4599, 0.459326],\n",
    "130: [0.438, 0.379107], 140: [0.3474, 0.32722], 150: [0.235, 0.294267], 160: [0.2042, 0.260987],\n",
    "170: [0.1766, 0.228773], 180: [0.1504, 0.205617], 190: [0.1093, 0.202869], 200: [0.0851, 0.195545],\n",
    "210: [0.0764, 0.197944], 220: [0.0493, 0.202711], 230: [0.0456, 0.204082], 240: [0.0673, 0.206159],\n",
    "250: [0.0525, 0.193714], 260: [0.0417, 0.188306], 270: [0.042, 0.206414], 280: [0.0277, 0.206935]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4768a-402f-4c08-a480-29d1a082748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "def convert_eval_scores_to_dict(string):\n",
    "    eval_dict = {}\n",
    "    token_list = []\n",
    "    for token in string.split():\n",
    "        token_list.append(token)\n",
    "        if len(token_list) >= 3:\n",
    "            eval_dict[int(token_list[0])] = [ float(token_list[1]), float(token_list[2]) ]\n",
    "            token_list = []\n",
    "    if len(token_list) > 0:\n",
    "        print(f\"there were unprocessed tokens! ({token_lidst})\")\n",
    "    return eval_dict\n",
    "\n",
    "convert_eval_scores_to_dict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddd835-1afe-4b47-b040-4dc8ea85d24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ce19d-e088-4231-b0ef-8e165ee793fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_pretrained(save_directory=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9416f29a-d368-4150-80a4-9cbb5b4579bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.save_pretrained(save_directory=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f5b74-575a-4812-a5c8-82540ad46c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, './model/cstom-setfit-model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d72e6-12bf-4da8-aa6b-90c2e23b6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_data(trainer):\n",
    "    eval_data = {}\n",
    "    for data in trainer.state.log_history:\n",
    "        if data[\"step\"] not in eval_data:\n",
    "            eval_data[data[\"step\"]] = [0 , 0]\n",
    "        if \"loss\" in data:\n",
    "            eval_data[data[\"step\"]][0] = data[\"loss\"]\n",
    "        if \"eval_loss\" in data:\n",
    "            eval_data[data[\"step\"]][1] = data[\"eval_loss\"]\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf08f37-9349-4b86-a1ae-7b2b76aba367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_data(eval_data):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot([data_key for data_key in eval_data], [eval_data[data_key][0] for data_key in eval_data], label=\"training loss\")\n",
    "    plt.plot([data_key for data_key in eval_data], [eval_data[data_key][1] for data_key in eval_data], label=\"validation loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75d44c-f745-450c-9688-7b8b1719a6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_data(make_eval_data(trainer))\n",
    "#plot_eval_data(eval_data[\"600c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345b08f-d01b-4cb1-903d-ea198d164fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_eval_data(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f621a4-b656-4e2f-befb-4a7aa7ddafe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad9083-2c23-495d-b075-dbdc4102ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad5b4e-bff8-4957-a617-f918263a793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [ ner_pipeline(\" \".join(val_text)) for val_text in val_texts ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b087ff-6f39-44c5-87e1-6631aa60c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_entities(tag_id_list, token_id_list):\n",
    "    entities = []\n",
    "    token_counter = 0\n",
    "    current_tag = (\"\", -1)\n",
    "    for tag, token in zip(tag_id_list, token_id_list):\n",
    "        tag_start = tag[0]\n",
    "        tag_class = regex.sub(r\"^[BI]-\", \"\", tag)\n",
    "        current_tag_class = current_tag[0]\n",
    "        current_tag_start = current_tag[1]\n",
    "        if regex.search(r\"^##\", token):\n",
    "            token_counter -= 1\n",
    "        if current_tag_class != \"\" and not regex.search(r\"^##\", token):\n",
    "            if tag_class == \"O\" or tag_start == \"B\" or tag_class != current_tag_class:\n",
    "                entities.append((current_tag_class, current_tag_start, token_counter))\n",
    "                current_tag = (\"\", -1)\n",
    "                current_tag_class = \"\"\n",
    "                current_tag_start = -1\n",
    "        if tag_class != \"O\" and current_tag_class == \"\":\n",
    "            current_tag = (tag_class, token_counter)\n",
    "            if regex.search(r\"^##\", token) and (len(entities) == 0 or entities[-1][2] != token_counter):\n",
    "                current_tag = (tag_class, token_counter - 1)\n",
    "        token_counter += 1\n",
    "    if current_tag_class != \"\":\n",
    "        entities.append((current_tag_class, current_tag_start, token_counter))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495cd6f2-1232-46a3-984b-87f230ac8aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_precision_and_recall(correct_count, missed_count, wrong_count):\n",
    "    for tag in sorted(correct_count):\n",
    "        if correct_count[tag] > 0 or missed_count[tag] or wrong_count[tag] > 0:\n",
    "            precision = correct_count[tag]/(correct_count[tag] + wrong_count[tag])\n",
    "            recall = correct_count[tag]/(correct_count[tag] + missed_count[tag])\n",
    "            print(f\"precision: {int(100*precision):-3d}; recall: {int(100*recall):-3d}; count: {correct_count[tag] + missed_count[tag]:4d}; tag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fde91-b970-4723-9a69-04205ff675ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_ids(label_ids):\n",
    "    return [ id2tag[label_id] for label_id in label_ids if label_id != IGNORE_TAG_ID ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165aa77c-3d80-487d-9891-69dcc1bf6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_results(sentence_result):\n",
    "    return get_labels_from_ids([ int(regex.sub(\"^LABEL_\", \"\", token_result[\"entity\"])) for token_result in sentence_result ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78431472-36de-4da3-81cb-cc7d01ad9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_tokens_from_results(sentence_result):\n",
    "    return [ token_result[\"word\"] for token_result in sentence_result ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ead6a-e824-463e-aacb-8f739653bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_split_tokens(split_tokens):\n",
    "    combined_tokens = []\n",
    "    for token in split_tokens:\n",
    "        if not regex.search(r\"^##\", token):\n",
    "            combined_tokens.append(token)\n",
    "        else:\n",
    "            combined_tokens[-1] += regex.sub(r\"^##\", \"\", token)\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8945a-4286-4e78-995a-72e1f1b32dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results_per_entity(results, correct_label_ids):\n",
    "    correct_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    missed_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    wrong_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    errors_per_text = []\n",
    "    for sentence_result, correct_sentence_label_ids in zip(results, correct_label_ids):\n",
    "        guessed_labels = get_labels_from_results(sentence_result)\n",
    "        split_tokens = get_split_tokens_from_results(sentence_result)\n",
    "        correct_labels = get_labels_from_ids(correct_sentence_label_ids)\n",
    "        guessed_entities = results_to_entities(guessed_labels, split_tokens)\n",
    "        correct_entities = results_to_entities(correct_labels, split_tokens)\n",
    "        error_count = 0\n",
    "        for entity in correct_entities:\n",
    "            if entity in guessed_entities:\n",
    "                correct_count[entity[0]] += 1\n",
    "            else:\n",
    "                missed_count[entity[0]] += 1\n",
    "                error_count += 1\n",
    "        for entity in guessed_entities:\n",
    "            if entity not in correct_entities:\n",
    "                wrong_count[entity[0]] += 1\n",
    "                error_count += 1\n",
    "        errors_per_text.append(error_count)\n",
    "    return correct_count, missed_count, wrong_count, errors_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35201e78-1541-42f4-93b8-c0a16a87c368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct_count, missed_count, wrong_count , errors_per_text = evaluate_results_per_entity(results, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695bee3-8b41-4c83-bccb-0654b8628e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_precision_and_recall(correct_count, missed_count, wrong_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8d392-c47b-4de3-b3d3-6095b67c2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_results_1(results, encodings, errors_per_text, max_counter=0):\n",
    "    counter = 0\n",
    "    for guess_data, correct_data, token_data, error_count in zip(results[0], results[1], encodings, errors_per_text):\n",
    "        text = f\"{error_count} \"\n",
    "        tags = []\n",
    "        token_counter = 0\n",
    "        in_tag = False\n",
    "        for guess_values, correct_id, token in zip(guess_data, correct_data, tokenizer.convert_ids_to_tokens(token_data)):\n",
    "            guess_id = list(guess_values).index(max(guess_values))\n",
    "            if correct_id != IGNORE_TAG_ID:\n",
    "                if guess_id in [ IGNORE_TAG_ID, tag2id['O'] ]:\n",
    "                    in_tag = False\n",
    "                else:\n",
    "                    start = len(text)\n",
    "                    end = len(text) + len(token)\n",
    "                    label = regex.sub(r\"^[BI]-\", \"\", id2tag[guess_id])[0:2]\n",
    "                    if in_tag and tags[-1][\"label\"] == label:\n",
    "                        tags[-1][\"end\"] = end\n",
    "                    else:\n",
    "                        tags.append({ \"start\": start, \"end\": end, \"label\": label })\n",
    "                        in_tag = True\n",
    "                text =  text + regex.sub(r\"^##\", \"  \", token) + \" \"\n",
    "                token_counter += 1\n",
    "        render_text(text, tags)\n",
    "        counter += 1\n",
    "        if max_counter > 0 and counter >= max_counter:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99fbc9-933f-48bb-ae7b-17c50fc57e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_results(text_entities, text_tokens, error_count):\n",
    "    text = f\"({error_count})\"\n",
    "    tags = []\n",
    "    token_counter = 0\n",
    "    in_tag = False\n",
    "    for entity in text_entities:\n",
    "        entity_label, entity_token_start, entity_token_end = entity\n",
    "        #print(token_counter, entity_token_start, len(text_tokens), text_tokens)\n",
    "        for i in range(token_counter, entity_token_start):\n",
    "            text += \" \" + text_tokens[i]\n",
    "        entity_char_start = len(text) + 1\n",
    "        for i in range(entity_token_start, entity_token_end):\n",
    "            text += \" \" + text_tokens[i]\n",
    "        entity_char_end = len(text)\n",
    "        tags.append( { \"start\": entity_char_start, \"end\": entity_char_end, \"label\": entity_label } )\n",
    "        token_counter = entity_token_end\n",
    "    render_text(text, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d7a04-93e5-41ad-a4d2-2908fcecb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_tags = [ [ id2tag[list(guesses_per_token).index(max(guesses_per_token))]\n",
    "                   for guesses_per_token in guesses ] \n",
    "                   for guesses in results[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fe26e-46d8-49c0-8048-06b90fceef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20230723 \n",
    "# * standardize arguments of render_results so that it can be used for inspecting processed data\n",
    "# * check saving and loading fine-tuned model, perhaps use save_pretrained? what about saving tokenize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b4c61-9ae4-4023-959e-75e1e565a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_render = 10\n",
    "\n",
    "text_counter = 0\n",
    "for sentence_result, error_count in zip(results, errors_per_text):\n",
    "    guessed_labels = get_labels_from_results(sentence_result)\n",
    "    split_tokens = get_tokens_from_results(sentence_result)\n",
    "    guessed_entities = results_to_entities(guessed_labels, split_tokens)\n",
    "    render_results(guessed_entities, combine_split_tokens(split_tokens), error_count)\n",
    "    text_counter += 1\n",
    "    if text_counter >= max_render:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6affb-b542-4a7c-8697-b07cc62dfb72",
   "metadata": {},
   "source": [
    "### 2.4 Select extra data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e6e5e-d1b7-4850-84a6-69f6c1d2d187",
   "metadata": {},
   "source": [
    "Training data selection process:\n",
    "\n",
    "1. 100 most frequent data from each half and 100 randomly selected (total 400)\n",
    "2. 50 with most of ENSLAVED|FREED|OWNER tags and 50 random with one of these tags (total 200)\n",
    "3. 50 randomly selcted data of each half with one of the tags ENSLAVED|FREED (total 100)\n",
    "\n",
    "Total: 700 (1 duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf693eb-e340-456f-9bc1-986b240ec30b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c3a5e-9fde-4733-a60f-6a2d34af7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea0a7b-cbcd-4e67-8286-c29005fd746e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_data = make_data(info_data_train, selected_frequent=0, selected_random=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047da473-bb0f-4ccc-ac9b-cdc47b87f661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_extra_data = []\n",
    "for data in extra_data:\n",
    "    if tokenizer.tokenize(data[\"text\"]) not in annotated_texts:\n",
    "        tag_counter = 0\n",
    "        for entity in ner_pipeline(data[\"text\"]):\n",
    "            label = id2tag[int(regex.sub(\"LABEL_\", \"\", entity[\"entity\"]))]\n",
    "            if regex.search(\"(ENSLAVED|FREED)\", label):\n",
    "                tag_counter += 1\n",
    "        if tag_counter > 0:\n",
    "            selected_extra_data.append({ \"tag_counter\": tag_counter, \"data\": data })\n",
    "len(selected_extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be37cbe-1605-451d-acf9-f1d84a4a59e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_file = open(\"outfile.json\", \"w\")\n",
    "for data in sorted(selected_extra_data, key=lambda data: data[\"tag_counter\"], reverse=True)[:50]:\n",
    "    #data[\"data\"].pop(\"eid\", None)\n",
    "    print(json.dumps(data[\"data\"]), file=out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4413068-4100-48e4-8d4c-0db893a5abdf",
   "metadata": {},
   "source": [
    "### 2.5 Process other data with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d6018-a97f-40a6-82ce-ef7524f65728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(selected_entities):\n",
    "    for entity_list in selected_entities:\n",
    "        for entity in entity_list:\n",
    "            entity[\"label\"] = id2tag[int(regex.sub(r\"^LABEL_\", \"\", entity[\"entity\"]))]\n",
    "    return selected_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ab2f8-9169-4cef-b5a6-b15e85e492b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(selected_data[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e09ec3-06f2-49b6-b2e1-6c0e6bf55f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = make_data(info_data_train, selected_frequent=0, selected_random=10)\n",
    "selected_entities = [ ner_pipeline(data[\"text\"]) for data in selected_data ]\n",
    "selected_entities = add_labels(selected_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8f88e-1929-4d28-9ebb-5475774cbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_results(selected_entities, \n",
    "               [ tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data[\"text\"])) for data in selected_data],\n",
    "               len(entities) * [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecaf4e-07f8-48d3-8c4d-a2ca95d68398",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bcc63-b5ed-40c3-916c-a23d8ebee0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
