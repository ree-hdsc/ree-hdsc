{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acff869-67df-4b20-a939-86b1b609059e",
   "metadata": {},
   "source": [
    "# Info fields via machine learning\n",
    "\n",
    "Extract persons from the info fields StartEntryInfo and EndEntryInfo of the [slave registers of Suriname](https://datasets.iisg.amsterdam/dataset.xhtml?persistentId=hdl:10622/CSPBHO) via machine learning\n",
    "\n",
    "See: https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afe6d3-87bf-4c94-b48d-9cf0b81c1a4c",
   "metadata": {},
   "source": [
    "## 1. Annotating info fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39531b3-deb1-4d22-9586-de290836ee04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8ca70-3fd4-4d1b-8cff-a2815842e39c",
   "metadata": {},
   "source": [
    "### 1.1 Read relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa999620-9243-433e-9e92-63ffa5607c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/suriname/Dataset Suriname Slave and Emancipation Registers Version 1.1.csv\"\n",
    "DATA_COLUMN = \"EndEntryInfo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cdd66-c9bc-40f4-a145-73228319d7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_tokens(train):\n",
    "    train[\"tokens\"] = [ nltk.word_tokenize(text) for text in train[\"text\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3f692-e25b-4fca-aed5-98c2b8211153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_labels(train):\n",
    "    train[\"labels\"] = [ len(tokens) * [ \"O\" ] for tokens in train[\"tokens\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22f9b5-a3c3-46d7-b0fb-34a7b2f37581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_numeric_labels(train, numeric_labels):\n",
    "    train[\"numeric_labels\"] = [ [ numeric_labels[label] for label in labels ] for labels in train[\"labels\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17279108-eb94-46c0-87ed-655e84eeeef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_date(day, month, year):\n",
    "    return regex.search(r\"^\\d\\d\\d\\d\\b\", year) and regex.search(r\"^\\d\\d?$\", day) and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09845b-a11a-46e3-8041-ac4128800650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_date_tags_to_labels(labels, index):\n",
    "    labels[index - 2], labels[index - 1], labels[index] = \"B-DATE\", \"I-DATE\", \"I-DATE\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ca68a-b24e-4f94-b1d0-a05737cc549e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_dates(train):\n",
    "    for index, row in train.iterrows():\n",
    "        for i in range(2, len(row[\"tokens\"])):\n",
    "            if is_date(row[\"tokens\"][i-2], row[\"tokens\"][i-1], row[\"tokens\"][i]):\n",
    "                add_date_tags_to_labels(row[\"labels\"], i)\n",
    "    return train       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333bfb9-6b81-4ce5-9927-12a1e86d7d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_annotations(train):\n",
    "    for index in range(0, len(train)):\n",
    "        for i in range(0, len(train[\"labels\"][index])):\n",
    "            print(train[\"tokens\"][index][i], end=\"\")\n",
    "            if train[\"labels\"][index][i] != \"O\":\n",
    "                print(\"/\" + train[\"labels\"][index][i], end=\"\")\n",
    "            print(\" \", end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fe91b-e883-406e-b7af-eb8f7a198a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train(data, data_column=DATA_COLUMN, nbr_of_lines=100):\n",
    "    if nbr_of_lines > 0:\n",
    "        train = pd.DataFrame(data[data_column].value_counts()[:nbr_of_lines])\n",
    "    else:\n",
    "        train = pd.DataFrame(data[data_column].value_counts())\n",
    "    train = train.rename(columns={data_column: \"frequency\"})\n",
    "    train[\"text\"] = train.index\n",
    "    train[\"index\"] = range(0, len(train))\n",
    "    train = train.set_index(\"index\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aff36-ce1e-472c-963c-c23dcf63b316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_info_data_train(data_column=DATA_COLUMN):\n",
    "    data = pd.read_csv(DATA_FILE, low_memory=False)\n",
    "    info_data_train = make_train(data, data_column, nbr_of_lines=0)\n",
    "    info_data_train = add_column_tokens(info_data_train)\n",
    "    info_data_train = add_column_labels(info_data_train)\n",
    "    return info_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574100b4-9711-4f6d-bf56-40de98bdd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data_train = make_info_data_train(data_column=DATA_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d70616-ba02-4cb8-9c7e-6c0eebbebc05",
   "metadata": {},
   "source": [
    "### 1.2 Make data for initial annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a1e97-4794-457a-ba96-383fac6fc17c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SELECTED_FREQUENT = 100\n",
    "SELECTED_RANDOM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ded55-aebe-4f6f-b071-7263a04d4a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_selected_data_ids(info_data_train, selected_frequent=SELECTED_FREQUENT, selected_random=SELECTED_RANDOM):\n",
    "    selected_data_ids = list(range(0, selected_frequent))\n",
    "    while len(selected_data_ids) < selected_frequent + selected_random:\n",
    "        selected_data_id = random.randint(selected_frequent, len(info_data_train) - 1)\n",
    "        if selected_data_id not in selected_data_ids:\n",
    "            selected_data_ids.append(selected_data_id)\n",
    "    return selected_data_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c2de4-534d-4447-b1af-3180ea593242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_selected_data_flags(info_data_train, selected_data_ids):\n",
    "    selected_data_flags = len(info_data_train) * [ False ]\n",
    "    for id_value in selected_data_ids:\n",
    "        selected_data_flags[id_value] = True\n",
    "    return selected_data_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f518425-2956-41d4-a8fa-04a382eade0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_annotated_data(info_data_train, selected_data_flags):\n",
    "    out_file = open(\"outfile.json\", \"w\")\n",
    "    selected_data = []\n",
    "    for index, row in info_data_train[selected_data_flags].iterrows():\n",
    "        text = \" \".join(row[\"tokens\"])\n",
    "        selected_data.append({ \"eid\": DATA_COLUMN[0] + str(index), \"text\": text, \"label\": [] })\n",
    "        print(selected_data[-1], file=out_file)\n",
    "    out_file.close()\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c62af9-2653-48fe-b0ca-49bc18752081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data(info_data_train, selected_frequent=SELECTED_FREQUENT, selected_random=SELECTED_RANDOM):\n",
    "    random.seed(42)\n",
    "    selected_data_ids = make_selected_data_ids(info_data_train, selected_frequent, selected_random)\n",
    "    selected_data_flags = make_selected_data_flags(info_data_train, selected_data_ids)\n",
    "    selected_data = save_annotated_data(info_data_train, selected_data_flags)\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6687544-ff14-4a42-936b-90ff68cd0af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selected_data = make_data(info_data_train, selected_frequent=SELECTED_FREQUENT, selected_random=SELECTED_RANDOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa63a6-fd28-4e27-bcf0-13b35794976c",
   "metadata": {},
   "source": [
    "## 2. Machine learning\n",
    "\n",
    "Based on tutorial https://huggingface.co/transformers/v3.2.0/custom_datasets.html#token-classification-with-w-nut-emerging-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed17038-ae39-4b27-9185-f14863fcbfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import regex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy import displacy\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416ec63-1bf8-4d78-8324-4e690d8b8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_text(text, entities):\n",
    "    displacy.render({ \"text\": regex.sub(\"\\\\n\", \" \", text), \n",
    "                      \"ents\": entities }, \n",
    "                      options = { \"colors\": { \"fuzzy_match\": \"yellow\"} }, style = \"ent\", manual = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b9dcc-b26a-49a2-b5ed-ea25e6cd6d1b",
   "metadata": {},
   "source": [
    "### 2.1 Read annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2394b87-dfac-4e39-afd8-9dec35cb68dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ANNOTATIONS_FILE = \"../../data/annotated/1600.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b241a2c-4d89-4bd3-99b3-2883a61737bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_offset2label_pos(text):\n",
    "    offset2label_pos = {}\n",
    "    offset = 0\n",
    "    token_counter = 0\n",
    "    for token in text.split():\n",
    "        offset2label_pos[offset] = token_counter\n",
    "        offset += len(token) + 1\n",
    "        token_counter += 1\n",
    "    return offset2label_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e4822-9585-48ba-82bf-88416b1f6201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_label_start_not_token_initial(text, label_start):\n",
    "    while regex.search(\" \", text[label_start]):\n",
    "        label_start += 1\n",
    "    while label_start > 0 and not regex.search(\" \", text[label_start - 1]):\n",
    "        label_start -= 1\n",
    "    return label_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff681a-79fc-488d-830b-11e36a17bccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    text = data[\"data\"]\n",
    "    labels = [ \"O\" for token in text.split() ]\n",
    "    offset2label_pos = make_offset2label_pos(text)\n",
    "    for label in data[\"label\"]:\n",
    "        label[0] = fix_label_start_not_token_initial(text, label[0])\n",
    "        if label[0] not in offset2label_pos:\n",
    "            raise Exception(f\"{label[0]} not found in labels {offset2label_pos} of text {text}\")\n",
    "        else:\n",
    "            labels[offset2label_pos[label[0]]] = \"B-\" + label[2]\n",
    "            for i in range(label[0] + 1, label[1] + 1):\n",
    "                if i in offset2label_pos:\n",
    "                    labels[offset2label_pos[i]] = \"I-\" + label[2]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea277fd-a04c-421e-a4e9-b43c95fcf353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_jsonl_file(file_name):\n",
    "    annotations_file = open(file_name, \"r\")\n",
    "    texts = []\n",
    "    tags = []\n",
    "    for line in annotations_file:\n",
    "        data = json.loads(line)\n",
    "        texts.append(data[\"data\"].split())\n",
    "        tags.append(make_labels(data))\n",
    "    annotations_file.close()\n",
    "    return texts, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c528a02-d9a9-4eda-a22b-f04ab90d606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(annotated_texts, annotated_tags):\n",
    "    seen = {}\n",
    "    items_to_delete = []\n",
    "    for i in range(0, len(annotated_texts)):\n",
    "        text = annotated_texts[i]\n",
    "        if str(text) in seen:\n",
    "            print(text)\n",
    "            items_to_delete = [i] + items_to_delete\n",
    "        seen[str(text)] = True\n",
    "    return items_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a62f3-7f6f-474f-9a8b-4d0bdb12d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(annotated_texts, annotated_tags):\n",
    "    items_to_delete = find_duplicates(annotated_texts, annotated_tags)\n",
    "    for i in items_to_delete:\n",
    "        annotated_texts.pop(i)\n",
    "        annotated_tags.pop(i)\n",
    "    return annotated_texts, annotated_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87610b03-fd82-46fc-b491-af84c13c43a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotated_texts, annotated_tags = read_jsonl_file(ANNOTATIONS_FILE)\n",
    "len(annotated_texts), len(annotated_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee51689-1f0a-450f-b8b4-3261f0a46bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_texts, annotated_tags = remove_duplicates(annotated_texts, annotated_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593d8cf-b441-4ee9-aabe-c4c25caa6ee5",
   "metadata": {},
   "source": [
    "### 2.2 Convert data to train set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aea3f-482a-4421-ab09-6abb206d8c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_missing_I_tags(tags):\n",
    "    missing_tags = []\n",
    "    for tag in tags:\n",
    "        i_tag = regex.sub(r\"^B-\", \"I-\", tag)\n",
    "        if i_tag not in tags:\n",
    "            missing_tags.append(i_tag)\n",
    "    return list(tags) + missing_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cbe0e-f7ea-4bdc-9854-60cb9838fa55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_tags, val_tags = train_test_split(annotated_texts, \n",
    "                                                                annotated_tags, \n",
    "                                                                test_size=.2, \n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2ee85-783f-4af8-bac2-b5a1f012d16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in annotated_tags for tag in doc )\n",
    "unique_tags = add_missing_I_tags(unique_tags)\n",
    "unique_types = list(set([ regex.sub(r\"^[BI]-\", \"\", tag) for tag in unique_tags ]))\n",
    "tag2id = { tag: id for id, tag in enumerate(unique_tags) }\n",
    "id2tag = { id: tag for tag, id in tag2id.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974af327-db79-4dd6-b93a-ca1d707bdd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96866fb1-e997-46e9-9148-04e45c534e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)\n",
    "val_encodings =   tokenizer(val_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e4673-8459-42ee-a5aa-221306002e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_B_to_I_tag(tag):\n",
    "    return regex.sub(r\"^B\", \"I\", tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f431ba-9046-47cd-a82a-d0441193e59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_tags(tags_in, encodings):\n",
    "    tags_out = [ [] for _ in range(len(encodings.offset_mapping,)) ]\n",
    "    for encodings_doc, tags_in_doc, tags_out_doc in zip(encodings.offset_mapping, tags_in, tags_out):\n",
    "        CLS_seen = False\n",
    "        SEP_seen = False\n",
    "        tags_counter = 0\n",
    "        for encoding in encodings_doc:\n",
    "            if encoding[1] == 0:\n",
    "                if not CLS_seen:\n",
    "                    tags_out_doc.append(\"CLS\")\n",
    "                    CLS_seen = True\n",
    "                elif not SEP_seen:\n",
    "                    tags_out_doc.append(\"SEP\")\n",
    "                    SEP_seen = True\n",
    "                else:\n",
    "                    tags_out_doc.append(\"PAD\")\n",
    "            elif encoding[0] == 0:\n",
    "                tags_out_doc.append(tags_in_doc[tags_counter])\n",
    "                tags_counter += 1\n",
    "            else:\n",
    "                tags_out_doc.append(convert_B_to_I_tag(tags_in_doc[tags_counter - 1]))\n",
    "    return tags_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cee79-11ae-4a73-af85-82bba30cb4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tags_to_numbers(tags, tag2id):\n",
    "    return [ [ tag2id[tag] for tag in doc ] for doc in tags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ec5ba-1445-4714-b326-66c46eea3719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IGNORE_TAG_ID = -100\n",
    "\n",
    "extra_tags = { 'CLS': IGNORE_TAG_ID, 'SEP': IGNORE_TAG_ID, 'PAD': IGNORE_TAG_ID }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671b280-dcf8-4bce-9c1e-4a73b64305ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = tags_to_numbers( split_tags(train_tags, train_encodings),\n",
    "                                { **tag2id, **extra_tags})\n",
    "val_labels =   tags_to_numbers( split_tags(val_tags, val_encodings),\n",
    "                                { **tag2id, **extra_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba62d6-0caa-49f7-a63f-bfcc242e745c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346470a9-6199-45e6-8948-23b45c1f36d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aa58a-c202-4004-b27d-589a995c97f7",
   "metadata": {},
   "source": [
    "### 2.3 Fine-tune model with data\n",
    "\n",
    "Using Bertje as base model: https://huggingface.co/GroNLP/bert-base-dutch-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e480aab-c670-4c84-a9ca-70db30ef4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nbr_of_epochs=1):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=nbr_of_epochs,  # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,              # training arguments, defined above\n",
    "        train_dataset=train_dataset,     # training dataset\n",
    "        eval_dataset=val_dataset         # evaluation dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c520d-1ea5-44e9-8667-03b9de4eef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name):\n",
    "    # joblib.dump(model, './model/model-1000h.joblib') # 700d 560\n",
    "    print(f\"saving model {model_name}...\")\n",
    "    model.save_pretrained(f\"./models/{model_name}\")\n",
    "    tokenizer.save_pretrained(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0975a-154d-4e20-a9fb-8454a47b6822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"GroNLP/bert-base-dutch-cased\", num_labels=len(unique_tags))\n",
    "#model = joblib.load('./model/model-1000f.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ff458-e1b9-4f9d-a906-a4c764fd34bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_data = { \"400a\": { 10: [ 2.973100 , 2.855659], 20: [2.874400 ,2.633519], 30 : [2.598700 ,2.267503], 40 : [2.211500 ,1.768432], 50 : [1.778300 ,1.324655],\n",
    "60:[ 1.550900, 1.093353], 70: [ 1.220000, 0.903201], 80: [ 1.011200, 0.697839], 90 : [0.778800 ,0.554128], 100 : [0.623100 ,0.440024],\n",
    "110: [0.488200, 0.349560], 120: [ 0.343000, 0.288387], 130: [ 0.300600, 0.250258], 140: [ 0.226900, 0.229932], 150: [ 0.170800, 0.200462],\n",
    "160: [ 0.173200, 0.177333], 170: [ 0.101600, 0.182475], 180: [ 0.108400, 0.170770], 190: [ 0.074900, 0.172628], 200: [ 0.075700, 0.177106],\n",
    "210: [ 0.054100, 0.168911], 220: [ 0.047600, 0.179788], 230: [ 0.038200, 0.168373], 240: [ 0.039000, 0.165006], 250: [ 0.030100, 0.163445],\n",
    "260: [ 0.027400, 0.174015], 270: [ 0.018400, 0.171961], 280: [ 0.024600, 0.178531], 290: [ 0.015000, 0.189336], 300: [ 0.019300, 0.185390],\n",
    "310: [ 0.014100, 0.179444], 320: [ 0.009900, 0.196208], 330: [ 0.017200, 0.177975], 340: [ 0.007100, 0.188561], 350: [ 0.011200, 0.175972],\n",
    "360: [ 0.006600, 0.179566], 370: [ 0.007400, 0.180530], 380: [ 0.012900, 0.199687], 390: [ 0.005000, 0.191614], 400: [ 0.003700, 0.178989] },\n",
    "\"600a\": { 10: [ 2.853400, 2.788824], 20: [ \t2.739800, \t2.618280], 30: [ 2.518100, 2.352505], 40: [ 2.215600, 2.010691], 50: [ 1.876800, 1.643065],\n",
    "60: [ 1.569300, 1.363279], 70: [ 1.279300, 1.098443], 80: [ 1.041300, 0.855244], 90: [ 0.854200, 0.671153], 100: [ 0.616700, 0.533351],\n",
    "110: [ 0.562700, 0.431195], 120: [ 0.449400, 0.350870], 130: [ 0.334900, 0.301453], 140: [ 0.261000, 0.252131], 150: [ 0.290400, 0.220683],\n",
    "160: [ 0.190300, 0.203238], 170: [ 0.154600, 0.190547], 180: [ 0.185800, 0.167484], 190: [ 0.108400, 0.160793], 200: [ 0.101400, 0.145170],\n",
    "210: [ 0.088100, 0.133172], 220: [ 0.081000, 0.135661], 230: [ 0.068500, 0.159878], 240: [ 0.052700, 0.146953], 250: [ 0.038300, 0.169547],\n",
    "260: [ 0.037700, 0.152970], 270: [ 0.065600, 0.140576], 280: [ 0.027200, 0.173946], 290: [ 0.022900, 0.149843], 300: [ 0.023700, 0.155466] },\n",
    "\"600b\": {10: [2.97, 2.932976007461548], 20: [2.8959, 2.7588181495666504], 30: [2.6404, 2.4754464626312256], 40: [2.3043, 2.089912176132202],\n",
    "50: [1.9301, 1.6922649145126343], 60: [1.6257, 1.433002233505249], 70: [1.3531, 1.1809186935424805], 80: [1.1305, 0.9395545125007629],\n",
    "90: [0.9414, 0.735508143901825], 100: [0.673, 0.5727341771125793], 110: [0.6081, 0.4573941230773926], 120: [0.4872, 0.3719801604747772],\n",
    "130: [0.357, 0.3091298043727875], 140: [0.2731, 0.2592621445655823], 150: [0.3059, 0.23325742781162262], 160: [0.2005, 0.20716261863708496],\n",
    "170: [0.1691, 0.21426241099834442], 180: [0.1963, 0.17854894697666168], 190: [0.1302, 0.17267706990242004], 200: [0.1132, 0.1582951843738556],\n",
    "210: [0.0941, 0.1499401330947876]},\n",
    "\"600c\": {10: [3.0284, 2.9702017307281494], 20: [2.9019, 2.7836408615112305], 30: [2.6567, 2.4800596237182617], 40: [2.3059, 2.0758652687072754],\n",
    " 50: [1.9075, 1.65205717086792], 60: [1.583, 1.3605468273162842], 70: [1.285, 1.108488917350769], 80: [1.0662, 0.8703526258468628],\n",
    " 90: [0.8719, 0.6816078424453735], 100: [0.6273, 0.5537048578262329], 110: [0.5864, 0.4511786103248596], 120: [0.4745, 0.3668432831764221],\n",
    "130: [0.3503, 0.3144031763076782], 140: [0.2816, 0.264466255903244], 150: [0.3042, 0.2330472618341446], 160: [0.1985, 0.21090537309646606],\n",
    "170: [0.1735, 0.20016834139823914], 180: [0.1975, 0.17414356768131256], 190: [0.1271, 0.173195019364357], 200: [0.1204, 0.16945882141590118],\n",
    "210: [0.099, 0.1487908661365509], 220: [0.0798, 0.14753003418445587], 230: [0.0831, 0.14581818878650665], 240: [0.0679, 0.14583609998226166]},\n",
    "\"700a\": { 10:[ \t2.938300, \t2.900105], 20:[ \t2.838300, \t2.743521], 30:[ \t2.649400, \t2.491823], 40:[ \t2.343200, \t2.163887],\n",
    "50:[ \t2.032500, \t1.807033], 60:[ \t1.645100, \t1.522994], 70:[ \t1.423600, \t1.299871], 80:[ \t1.233000, \t1.048253],\n",
    "90:[ \t0.964700, \t0.846890], 100:[ \t0.770100, \t0.695937], 110:[ \t0.573500, \t0.582498], 120:[ \t0.458100, \t0.476454],\n",
    "130:[ \t0.457500, \t0.393108], 140:[ \t0.359800, \t0.332941], 150:[ \t0.248000, \t0.309698], 160:[ \t0.223600, \t0.275177],\n",
    "170:[ \t0.189700, \t0.240412], 180:[ \t0.171500, \t0.225659], 190:[ \t0.115100, \t0.214786], 200:[ \t0.094900, \t0.210268],\n",
    "210:[ \t0.094800, \t0.218391], 220:[ \t0.059200, \t0.219438], 230:[ \t0.052300, \t0.225003], 240:[ \t0.071400, \t0.240677],\n",
    "250:[ \t0.049300, \t0.207180], 260:[ \t0.040400, \t0.221386], 270:[ \t0.048800, \t0.221953], 280:[ \t0.028200, \t0.244117]},\n",
    "\"700b\": {10: [2.8077, 2.74599], 20: [2.7203, 2.598261], 30: [2.5386, 2.361184], 40: [2.2356, 2.056],\n",
    " 50: [1.9691, 1.759227], 60: [1.657, 1.536577], 70: [1.4585, 1.312596], 80: [1.2722, 1.060864],\n",
    "90: [0.9803, 0.850179], 100: [0.7778, 0.701402], 110: [0.5651, 0.578009], 120: [0.4599, 0.459326],\n",
    "130: [0.438, 0.379107], 140: [0.3474, 0.32722], 150: [0.235, 0.294267], 160: [0.2042, 0.260987],\n",
    "170: [0.1766, 0.228773], 180: [0.1504, 0.205617], 190: [0.1093, 0.202869], 200: [0.0851, 0.195545],\n",
    "210: [0.0764, 0.197944], 220: [0.0493, 0.202711], 230: [0.0456, 0.204082], 240: [0.0673, 0.206159],\n",
    "250: [0.0525, 0.193714], 260: [0.0417, 0.188306], 270: [0.042, 0.206414], 280: [0.0277, 0.206935]},\n",
    "\"700c\":{10: [3.1258, 3.077008], 20: [3.0275, 2.909514], 30: [2.8128, 2.626571], 40: [2.4663, 2.234454],\n",
    "50: [2.0665, 1.828221], 60: [1.705, 1.550596], 70: [1.4615, 1.319862], 80: [1.2743, 1.058335],\n",
    "90: [0.9715, 0.839678], 100: [0.7756, 0.682859], 110: [0.5582, 0.563459], 120: [0.4397, 0.457801],\n",
    "130: [0.4516, 0.38623], 140: [0.3451, 0.33273], 150: [0.2411, 0.30543], 160: [0.22, 0.270564],\n",
    "170: [0.1904, 0.237737], 180: [0.1667, 0.228862], 190: [0.1159, 0.207305], 200: [0.0946, 0.200976],\n",
    "210: [0.0896, 0.206707], 220: [0.0592, 0.212313], 230: [0.0463, 0.219483], 240: [0.0702, 0.222224]},\n",
    "\"700d\":{10: [2.8457, 2.812198], 20: [2.7406, 2.637834], 30: [2.5474, 2.358118], 40: [2.1981, 2.000004],\n",
    "50: [1.8766, 1.680943], 60: [1.5747, 1.441263], 70: [1.3578, 1.204207], 80: [1.15, 0.96236],\n",
    "90: [0.8909, 0.773915], 100: [0.7124, 0.644416], 110: [0.5166, 0.538043], 120: [0.4086, 0.4446],\n",
    "130: [0.4185, 0.380266], 140: [0.3525, 0.325295], 150: [0.2219, 0.323637], 160: [0.2845, 0.319027], \n",
    "170: [0.2448, 0.310449], 185: [0.1989, 0.295414], 195: [0.2073, 0.286989], 205: [0.2412, 0.270967],\n",
    "220: [0.1528, 0.262969], 230: [0.195, 0.262544], 240: [0.1563, 0.258511], 250: [0.1509, 0.252555],\n",
    "260: [0.1847, 0.247752], 270: [0.1454, 0.237777], 280: [0.1393, 0.231251], 290: [0.1407, 0.225938],\n",
    "300: [0.1172, 0.221487], 310: [0.1093, 0.221924], 320: [0.0965, 0.215767], 330: [0.0967, 0.222891],\n",
    "340: [0.0832, 0.214979], 350: [0.0694, 0.202908], 360: [0.0439, 0.203715], 370: [0.0615, 0.205893],\n",
    "380: [0.0541, 0.206606], 390: [0.0481, 0.213491], 400: [0.0343, 0.214338], 410: [0.031, 0.212545],\n",
    "420: [0.046, 0.207551], 430: [0.028, 0.203288], 440: [0.0287, 0.203225], 450: [0.0233, 0.211533],\n",
    "460: [0.0312, 0.210679], 470: [0.017, 0.211569], 480: [0.0283, 0.215215], 490: [0.0245, 0.253099],\n",
    "500: [0.037, 0.243072], 510: [0.0271, 0.238991], 520: [0.0191, 0.219453], 530: [0.0252, 0.222443],\n",
    "540: [0.0126, 0.23609], 550: [0.0129, 0.216745], 560: [0.0135, 0.20638], 570: [0.0144, 0.223499],\n",
    "580: [0.0128, 0.236927], 590: [0.0152, 0.264031], 600: [0.0171, 0.253831], 610: [0.0256, 0.24807],\n",
    "620: [0.0155, 0.25771], 630: [0.021, 0.241803],  640: [0.0074, 0.243103], 650: [0.0123, 0.240952],\n",
    "660: [0.0165, 0.245431], 670: [0.0092, 0.264513], 680: [0.0096, 0.256596], 690: [0.0171, 0.281578],\n",
    "700: [0.0181, 0.250943], 710: [0.0103, 0.250658], 720: [0.0097, 0.250658], 730: [0.014, 0.250658],\n",
    "740: [0.011, 0.250658], 750: [0.0077, 0.250658], 760: [0.0155, 0.250658], 770: [0.0089, 0.250658],\n",
    "780: [0.021, 0.250658], 790: [0.0076, 0.250658], 800: [0.0084, 0.250658], 810: [0.0144, 0.250658],\n",
    "820: [0.0101, 0.250658], 830: [0.0104, 0.250658], 840: [0.0123, 0.250658]},\n",
    "\"1000\": {10: [2.9187, 2.88821], 20: [2.7954, 2.723828], 30: [2.6157, 2.451765], 40: [2.3098, 2.097032], # a=100, b=150, c=200, d=250, e=300, f=350\n",
    "50: [2.0275, 1.743648], 60: [1.712, 1.482524], 70: [1.481, 1.231644], 80: [1.1371, 0.9992],\n",
    "90: [1.0829, 0.805159], 100: [0.8158, 0.655554], 110: [0.7605, 0.649471], 120: [0.6858, 0.632783], \n",
    "130: [0.6678, 0.608367], 140: [0.6423, 0.57365], 150: [0.623, 0.534211], 160: [0.5857, 0.492781],\n",
    "170: [0.5119, 0.447565], 180: [0.4712, 0.402347], 190: [0.4399, 0.364033], 200: [0.3586, 0.328106],\n",
    "210: [0.2874, 0.294019], 220: [0.2708, 0.269567], 230: [0.2137, 0.250032], 240: [0.2399, 0.229311],\n",
    "250: [0.2444, 0.20961], 260: [0.1408, 0.200593], 270: [0.1489, 0.189999], 280: [0.1278, 0.181852],\n",
    "290: [0.1153, 0.184566], 300: [0.112, 0.182305], 310: [0.0918, 0.187552], 320: [0.078, 0.186698],\n",
    "330: [0.0692, 0.175558], 340: [0.0801, 0.174457], 350: [0.0791, 0.171076]},\n",
    "\"1000g\": {10: [2.8584, 2.820923], 20: [2.796, 2.66589], 30: [2.6137, 2.426387], 40: [2.3372, 2.111061],\n",
    "50: [2.0059, 1.781075], 60: [1.7638, 1.522464], 70: [1.5017, 1.290353], 80: [1.3169, 1.068896],\n",
    "90: [1.0527, 0.871082], 100: [0.9006, 0.70992]},\n",
    "\"1000i\":{10: [2.9348, 2.896632], 20: [2.8531, 2.744698], 30: [2.6751, 2.499296], 40: [2.3864, 2.156715],\n",
    "50: [2.0484, 1.768833], 60: [1.7225, 1.446104], 70: [1.4219, 1.187102], 80: [1.2076, 0.958305],\n",
    "90: [0.9426, 0.79054], 100: [0.8204, 0.643298], 110: [0.6624, 0.533711], 120: [0.5249, 0.436337],\n",
    "130: [0.4497, 0.382147], 140: [0.4341, 0.315331], 150: [0.3328, 0.263771], 160: [0.2753, 0.222807],\n",
    "170: [0.2056, 0.229981], 180: [0.2139, 0.185988], 190: [0.1738, 0.180258], 200: [0.1605, 0.158712],\n",
    "210: [0.1148, 0.15716], 220: [0.1208, 0.15601], 230: [0.1249, 0.1603], 240: [0.1265, 0.153699],\n",
    "250: [0.0941, 0.154591]},\n",
    "\"1600a\": { 10: [2.8982, 2.850094], 20: [2.8202, 2.70102], 30: [2.6148, 2.455203], 40: [2.3678, 2.137227],\n",
    "50: [2.0229, 1.780287], 60: [1.7842, 1.520151], 70: [1.513, 1.276824], 80: [1.2847, 1.02483],\n",
    "90: [0.99, 0.825657], 100: [0.8327, 0.66878], 110: [0.7765, 0.542265], 120: [0.6266, 0.456178],\n",
    "130: [0.5431, 0.383968], 140: [0.4517, 0.335122], 150: [0.3463, 0.268808], 160: [0.3595, 0.232276],\n",
    "170: [0.2815, 0.231968], 180: [0.2589, 0.23097], 190: [0.221, 0.227109], 200: [0.243, 0.21801],\n",
    "210: [0.2274, 0.211847], 220: [0.2307, 0.200141], 230: [0.214, 0.192021], 240: [0.2193, 0.186254],\n",
    "250: [0.2021, 0.18411], 260: [0.1816, 0.17927], 270: [0.1451, 0.176039], 280: [0.1505, 0.17643],\n",
    "290: [0.1415, 0.173268], 300: [0.1394, 0.169241], 310: [0.1345, 0.164914], 320: [0.1503, 0.169083],\n",
    "330: [0.1527, 0.166625], 340: [0.1339, 0.160628], 350: [0.0906, 0.156777], 360: [0.0934, 0.161065],\n",
    "370: [0.0841, 0.163518], 380: [0.079, 0.162624], 390: [0.0821, 0.159327], 400: [0.1131, 0.170231],\n",
    "410: [0.1194, 0.167509], 420: [0.1024, 0.159955], 430: [0.0567, 0.15485], 440: [0.0577, 0.160201],\n",
    "450: [0.0499, 0.165606], 460: [0.041, 0.160545], 470: [0.0497, 0.163094], 480: [0.082, 0.171181],\n",
    "490: [0.0929, 0.169684], 500: [0.0776, 0.162454], 510: [0.0338, 0.156123], 520: [0.0332, 0.159722],\n",
    "530: [0.0294, 0.176274], 540: [0.02, 0.169214], 550: [0.0333, 0.170224], 560: [0.0601, 0.185153],\n",
    "570: [0.0781, 0.181815], 580: [0.0601, 0.173149], 590: [0.0204, 0.166442], 600: [0.0177, 0.16801],\n",
    "610: [0.0174, 0.178673], 620: [0.0113, 0.192567], 630: [0.0227, 0.177104], 640: [0.0436, 0.192012]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4768a-402f-4c08-a480-29d1a082748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"10 \t0.078100 \t0.181815\n",
    "20 \t0.060100 \t0.173149\n",
    "30 \t0.020400 \t0.166442\n",
    "40 \t0.017700 \t0.168010\n",
    "50 \t0.017400 \t0.178673\n",
    "60 \t0.011300 \t0.192567\n",
    "70 \t0.022700 \t0.177104\n",
    "80 \t0.043600 \t0.192012\n",
    "\"\"\"\n",
    "\n",
    "def convert_eval_scores_to_dict(string):\n",
    "    eval_dict = {}\n",
    "    token_list = []\n",
    "    for token in string.split():\n",
    "        token_list.append(token)\n",
    "        if len(token_list) >= 3:\n",
    "            eval_dict[int(token_list[0])] = [ float(token_list[1]), float(token_list[2]) ]\n",
    "            token_list = []\n",
    "    if len(token_list) > 0:\n",
    "        print(f\"there were unprocessed tokens! ({token_list})\")\n",
    "    return eval_dict\n",
    "\n",
    "convert_eval_scores_to_dict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddd835-1afe-4b47-b040-4dc8ea85d24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model(nbr_of_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a988c0-bd60-491b-88c0-d6ffd6b62ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\"1600h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd06da-3691-4c24-81c5-a57f4dbf8a47",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d72e6-12bf-4da8-aa6b-90c2e23b6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_data(trainer):\n",
    "    eval_data = {}\n",
    "    for data in trainer.state.log_history:\n",
    "        if data[\"step\"] not in eval_data:\n",
    "            eval_data[data[\"step\"]] = [0 , 0]\n",
    "        if \"loss\" in data:\n",
    "            eval_data[data[\"step\"]][0] = data[\"loss\"]\n",
    "        if \"eval_loss\" in data:\n",
    "            eval_data[data[\"step\"]][1] = data[\"eval_loss\"]\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf08f37-9349-4b86-a1ae-7b2b76aba367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_data(eval_data):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot([data_key for data_key in eval_data], [eval_data[data_key][0] for data_key in eval_data], label=\"training loss\")\n",
    "    plt.plot([data_key for data_key in eval_data], [eval_data[data_key][1] for data_key in eval_data], label=\"validation loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75d44c-f745-450c-9688-7b8b1719a6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot_eval_data(make_eval_data(trainer))\n",
    "plot_eval_data(eval_data[\"1600a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b087ff-6f39-44c5-87e1-6631aa60c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_entities(tag_id_list, token_id_list):\n",
    "    entities = []\n",
    "    token_counter = 0\n",
    "    current_tag = (\"\", -1)\n",
    "    for tag, token in zip(tag_id_list, token_id_list):\n",
    "        tag_start = tag[0]\n",
    "        tag_class = regex.sub(r\"^[BI]-\", \"\", tag)\n",
    "        current_tag_class = current_tag[0]\n",
    "        current_tag_start = current_tag[1]\n",
    "        if regex.search(r\"^##\", token):\n",
    "            token_counter -= 1\n",
    "        if current_tag_class != \"\" and not regex.search(r\"^##\", token):\n",
    "            if tag_class == \"O\" or tag_start == \"B\" or tag_class != current_tag_class:\n",
    "                entities.append([current_tag_start, token_counter, current_tag_class])\n",
    "                current_tag = (\"\", -1)\n",
    "                current_tag_class = \"\"\n",
    "                current_tag_start = -1\n",
    "        if tag_class != \"O\" and current_tag_class == \"\":\n",
    "            current_tag = (tag_class, token_counter)\n",
    "            if regex.search(r\"^##\", token) and (len(entities) == 0 or entities[-1][2] != token_counter):\n",
    "                current_tag = (tag_class, token_counter - 1)\n",
    "        token_counter += 1\n",
    "    if current_tag_class != \"\":\n",
    "        entities.append([current_tag_start, token_counter, current_tag_class])\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495cd6f2-1232-46a3-984b-87f230ac8aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_precision_and_recall(correct_count, missed_count, wrong_count):\n",
    "    for tag in [ \"total\" ] + sorted(correct_count):\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        if correct_count[tag] > 0 or wrong_count[tag] > 0:\n",
    "            precision = correct_count[tag]/(correct_count[tag] + wrong_count[tag])\n",
    "        if correct_count[tag] > 0 or missed_count[tag]:\n",
    "            recall = correct_count[tag]/(correct_count[tag] + missed_count[tag])\n",
    "        print(f\"precision: {int(100*precision):-3d}; recall: {int(100*recall):-3d}; count: {correct_count[tag] + missed_count[tag]:4d}; tag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fde91-b970-4723-9a69-04205ff675ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_ids(label_ids):\n",
    "    return [ id2tag[label_id] for label_id in label_ids if label_id != IGNORE_TAG_ID ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165aa77c-3d80-487d-9891-69dcc1bf6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_results(sentence_result):\n",
    "    return get_labels_from_ids([ int(regex.sub(\"^LABEL_\", \"\", token_result[\"entity\"])) for token_result in sentence_result ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78431472-36de-4da3-81cb-cc7d01ad9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_tokens_from_results(sentence_result):\n",
    "    return [ token_result[\"word\"] for token_result in sentence_result ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ead6a-e824-463e-aacb-8f739653bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_split_tokens(split_tokens):\n",
    "    combined_tokens = []\n",
    "    for token in split_tokens:\n",
    "        if not regex.search(r\"^##\", token):\n",
    "            combined_tokens.append(token)\n",
    "        else:\n",
    "            combined_tokens[-1] += regex.sub(r\"^##\", \"\", token)\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5077d5a-90a1-4c00-a7a9-303881e37b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize(text):\n",
    "    return regex.sub(\" ##\", \"\", \" \".join(tokenizer.tokenize(\" \".join(nltk.word_tokenize(text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f2f78-c857-443f-96c6-2e480580753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenization(texts):\n",
    "    nbr_of_mismatches = 0\n",
    "    for input_text in texts:\n",
    "        processed_text = retokenize(text)\n",
    "        if processed_text != input_text:\n",
    "            nbr_of_mismatches += 0\n",
    "    if nbr_of_mismatches > 0:\n",
    "        print(f\"tokenization mismatches: {nbr_of_mismatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99fbc9-933f-48bb-ae7b-17c50fc57e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_results(text_entities, text_tokens, error_count):\n",
    "    text = f\"({error_count})\"\n",
    "    tags = []\n",
    "    token_counter = 0\n",
    "    in_tag = False\n",
    "    for entity in text_entities:\n",
    "        entity_token_start, entity_token_end, entity_label = entity\n",
    "        for i in range(token_counter, entity_token_start):\n",
    "            text += \" \" + text_tokens[i]\n",
    "        entity_char_start = len(text) + 1\n",
    "        for i in range(entity_token_start, entity_token_end):\n",
    "            text += \" \" + text_tokens[i]\n",
    "        entity_char_end = len(text)\n",
    "        tags.append( { \"start\": entity_char_start, \"end\": entity_char_end, \"label\": entity_label } )\n",
    "        token_counter = entity_token_end\n",
    "    render_text(text, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8945a-4286-4e78-995a-72e1f1b32dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results_per_entity(results, correct_label_ids, check_labels=False):\n",
    "    correct_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    missed_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    wrong_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    errors_per_text = []\n",
    "    for sentence_result, correct_sentence_label_ids in zip(results, correct_label_ids):\n",
    "        guessed_labels = get_labels_from_results(sentence_result)\n",
    "        split_tokens = get_split_tokens_from_results(sentence_result)\n",
    "        correct_labels = get_labels_from_ids(correct_sentence_label_ids)\n",
    "        guessed_entities = results_to_entities(guessed_labels, split_tokens)\n",
    "        correct_entities = results_to_entities(correct_labels, split_tokens)\n",
    "        error_count = 0\n",
    "        for entity in correct_entities:\n",
    "            if entity in guessed_entities:\n",
    "                correct_count[entity[2]] += 1\n",
    "            else:\n",
    "                missed_count[entity[2]] += 1\n",
    "                error_count += 1\n",
    "        for entity in guessed_entities:\n",
    "            if entity not in correct_entities:\n",
    "                wrong_count[entity[2]] += 1\n",
    "                error_count += 1\n",
    "        errors_per_text.append(error_count)\n",
    "        if check_labels and error_count > 0:\n",
    "            render_results(guessed_entities, combine_split_tokens(split_tokens), error_count)\n",
    "            render_results(correct_entities, combine_split_tokens(split_tokens), 0)\n",
    "            print(\"\")\n",
    "    return correct_count, missed_count, wrong_count, errors_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad9083-2c23-495d-b075-dbdc4102ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_texts(texts, labels, model=model, tokenizer=tokenizer, check_labels=False):\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    results = [ ner_pipeline(text) for text in texts ]\n",
    "    test_tokenization(texts)\n",
    "    correct_count, missed_count, wrong_count , errors_per_text = evaluate_results_per_entity(results, labels, check_labels)\n",
    "    correct_count[\"total\"] = sum(correct_count.values())\n",
    "    wrong_count[\"total\"] = sum(wrong_count.values())\n",
    "    missed_count[\"total\"] = sum(missed_count.values())\n",
    "    compute_precision_and_recall(correct_count, missed_count, wrong_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b41cd-0442-43ed-afdc-b3537c6d8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_texts([ \" \".join(text) for text in val_texts ], val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad33794-dee3-41b2-bbdf-9457337e2b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_texts([ \" \".join(text) for text in train_texts ][:10], train_labels[:10], check_labels=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ef4c5fb-0932-4477-9c5d-a22884b7a194",
   "metadata": {},
   "source": [
    "840 precision:  87; recall:  92; count:  557; tag: total\n",
    "770 precision:  87; recall:  92; count:  557; tag: total\n",
    "720 precision:  89; recall:  91; count:  557; tag: total\n",
    "630 precision:  87; recall:  91; count:  557; tag: total\n",
    "560 precision:  88; recall:  93; count:  557; tag: total <=== chosen for 700 set\n",
    "490 precision:  86; recall:  90; count:  557; tag: total\n",
    "420 precision:  87; recall:  92; count:  557; tag: total\n",
    "350 precision:  86; recall:  91; count:  557; tag: total\n",
    "280 precision:  85; recall:  88; count:  557; tag: total\n",
    "210 precision:  76; recall:  78; count:  557; tag: total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d7a04-93e5-41ad-a4d2-2908fcecb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_tags = [ [ id2tag[list(guesses_per_token).index(max(guesses_per_token))]\n",
    "                   for guesses_per_token in guesses ] \n",
    "                   for guesses in results[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b4c61-9ae4-4023-959e-75e1e565a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_render = 1\n",
    "\n",
    "text_counter = 0\n",
    "for sentence_result, error_count in zip(results, errors_per_text):\n",
    "    guessed_labels = get_labels_from_results(sentence_result)\n",
    "    split_tokens = get_split_tokens_from_results(sentence_result)\n",
    "    guessed_entities = results_to_entities(guessed_labels, split_tokens)\n",
    "    print(guessed_entities, combine_split_tokens(split_tokens), error_count)\n",
    "    render_results(guessed_entities, combine_split_tokens(split_tokens), error_count)\n",
    "    text_counter += 1\n",
    "    if text_counter >= max_render:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6affb-b542-4a7c-8697-b07cc62dfb72",
   "metadata": {},
   "source": [
    "### 2.5 Select extra data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e6e5e-d1b7-4850-84a6-69f6c1d2d187",
   "metadata": {},
   "source": [
    "Training data selection process:\n",
    "\n",
    "1. 100 most frequent data from each half and 100 randomly selected (total 400)\n",
    "2. 50 with most of ENSLAVED|FREED|OWNER tags and 50 random with one of these tags (total 200)\n",
    "3. 50 randomly selcted data of each half with one of the tags ENSLAVED|FREED (total 100)\n",
    "4. 150 randomly selcted data of each half (total 300)\n",
    "\n",
    "Total: 1000 (3 duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab81790-7d3a-4809-a6a3-878be0352318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_id_entities_to_char_id_entities(token_id_entities, split_tokens):\n",
    "    char_id_entities = []\n",
    "    tokens = combine_split_tokens(split_tokens)\n",
    "    for token_id_entity in token_id_entities:\n",
    "        char_start = 0\n",
    "        for i in range(0, token_id_entity[0]):\n",
    "            char_start += len(tokens[i]) + 1\n",
    "        char_end = char_start\n",
    "        for i in range(token_id_entity[0], token_id_entity[1]):\n",
    "            char_end += len(tokens[i]) + 1\n",
    "        char_id_entities.append([char_start, char_end - 1, token_id_entity[2]])\n",
    "    return char_id_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa750a7-837c-4fd2-8fa2-6c0a515a9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognized_entities_to_annotation_labels(entities):\n",
    "    split_tokens = get_split_tokens_from_results(entities)\n",
    "    labels = get_labels_from_results(entities)\n",
    "    token_id_entities = results_to_entities(labels, split_tokens)\n",
    "    char_id_entities = token_id_entities_to_char_id_entities(token_id_entities, split_tokens)\n",
    "    return char_id_entities\n",
    "\n",
    "# recognized_entities_to_annotation_labels(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ba5ab-c5e5-492e-ad54-8fcd63d3caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_of_selected_extra_data(selected_extra_data, sample_size=10):\n",
    "    for i in range(0, sample_size):\n",
    "        text = selected_extra_data[i][\"data\"][\"text\"]\n",
    "        labels = [{\"start\": data[0], \"end\": data[1], \"label\": data[2]} for data in selected_extra_data[i][\"data\"][\"label\"] ]\n",
    "        render_text(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad401fbe-da55-478f-945a-a0f281eb54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load('./model/model-1000h.joblib')\n",
    "model = BertForTokenClassification.from_pretrained(\"models/1000j\", num_labels=len(unique_tags))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/1000j\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c3a5e-9fde-4733-a60f-6a2d34af7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea0a7b-cbcd-4e67-8286-c29005fd746e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = \"StartEntryInfo\"\n",
    "\n",
    "info_data_train = make_info_data_train(data_column=DATA_COLUMN)\n",
    "extra_data = make_data(info_data_train, selected_frequent=0, selected_random=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047da473-bb0f-4ccc-ac9b-cdc47b87f661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_extra_data = []\n",
    "for data in extra_data:\n",
    "    if nltk.word_tokenize(data[\"text\"]) not in annotated_texts:\n",
    "        tag_counter = 0\n",
    "        entities = ner_pipeline(data[\"text\"])\n",
    "        data[\"label\"] = recognized_entities_to_annotation_labels(entities)\n",
    "        data[\"text\"] = retokenize(data[\"text\"])\n",
    "        selected_extra_data.append({ \"tag_counter\": tag_counter, \"data\": data })\n",
    "len(selected_extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8590f9-515b-4742-97e0-f184852fa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_of_selected_extra_data(selected_extra_data, sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be37cbe-1605-451d-acf9-f1d84a4a59e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_file = open(\"outfile.json\", \"w\")\n",
    "for data in sorted(selected_extra_data, key=lambda data: data[\"tag_counter\"], reverse=True)[:400]:\n",
    "    print(json.dumps(data[\"data\"]), file=out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4413068-4100-48e4-8d4c-0db893a5abdf",
   "metadata": {},
   "source": [
    "### 2.6 Process other data with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d6018-a97f-40a6-82ce-ef7524f65728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(selected_entities):\n",
    "    for entity_list in selected_entities:\n",
    "        for entity in entity_list:\n",
    "            entity[\"label\"] = id2tag[int(regex.sub(r\"^LABEL_\", \"\", entity[\"entity\"]))]\n",
    "    return selected_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ab2f8-9169-4cef-b5a6-b15e85e492b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(selected_data[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e09ec3-06f2-49b6-b2e1-6c0e6bf55f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = make_data(info_data_train, selected_frequent=0, selected_random=10)\n",
    "selected_entities = [ ner_pipeline(data[\"text\"]) for data in selected_data ]\n",
    "selected_entities = add_labels(selected_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8f88e-1929-4d28-9ebb-5475774cbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_results(selected_entities, \n",
    "               [ tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data[\"text\"])) for data in selected_data],\n",
    "               len(entities) * [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecaf4e-07f8-48d3-8c4d-a2ca95d68398",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bcc63-b5ed-40c3-916c-a23d8ebee0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
