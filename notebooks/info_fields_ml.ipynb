{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acff869-67df-4b20-a939-86b1b609059e",
   "metadata": {},
   "source": [
    "# Info fields via machine learning\n",
    "\n",
    "Extract persons from the info fields StartEntryInfo and EndEntryInfo of the [slave registers of Suriname](https://datasets.iisg.amsterdam/dataset.xhtml?persistentId=hdl:10622/CSPBHO) via machine learning\n",
    "\n",
    "See: https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afe6d3-87bf-4c94-b48d-9cf0b81c1a4c",
   "metadata": {},
   "source": [
    "## 1. Annotating info fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39531b3-deb1-4d22-9586-de290836ee04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8ca70-3fd4-4d1b-8cff-a2815842e39c",
   "metadata": {},
   "source": [
    "### 1.1 Read data that needs to be classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa999620-9243-433e-9e92-63ffa5607c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/suriname/Dataset Suriname Slave and Emancipation Registers Version 1.1.csv\"\n",
    "DATA_COLUMN = \"EndEntryInfo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cdd66-c9bc-40f4-a145-73228319d7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_tokens(train):\n",
    "    train[\"tokens\"] = [ nltk.word_tokenize(text) for text in train[\"text\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3f692-e25b-4fca-aed5-98c2b8211153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_labels(train):\n",
    "    train[\"labels\"] = [ len(tokens) * [ \"O\" ] for tokens in train[\"tokens\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22f9b5-a3c3-46d7-b0fb-34a7b2f37581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_numeric_labels(train, numeric_labels):\n",
    "    train[\"numeric_labels\"] = [ [ numeric_labels[label] for label in labels ] for labels in train[\"labels\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17279108-eb94-46c0-87ed-655e84eeeef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_date(day, month, year):\n",
    "    return regex.search(r\"^\\d\\d\\d\\d\\b\", year) and regex.search(r\"^\\d\\d?$\", day) and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09845b-a11a-46e3-8041-ac4128800650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_date_tags_to_labels(labels, index):\n",
    "    labels[index - 2], labels[index - 1], labels[index] = \"B-DATE\", \"I-DATE\", \"I-DATE\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ca68a-b24e-4f94-b1d0-a05737cc549e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_dates(train):\n",
    "    for index, row in train.iterrows():\n",
    "        for i in range(2, len(row[\"tokens\"])):\n",
    "            if is_date(row[\"tokens\"][i-2], row[\"tokens\"][i-1], row[\"tokens\"][i]):\n",
    "                add_date_tags_to_labels(row[\"labels\"], i)\n",
    "    return train       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333bfb9-6b81-4ce5-9927-12a1e86d7d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_annotations(train):\n",
    "    for index in range(0, len(train)):\n",
    "        for i in range(0, len(train[\"labels\"][index])):\n",
    "            print(train[\"tokens\"][index][i], end=\"\")\n",
    "            if train[\"labels\"][index][i] != \"O\":\n",
    "                print(\"/\" + train[\"labels\"][index][i], end=\"\")\n",
    "            print(\" \", end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fe91b-e883-406e-b7af-eb8f7a198a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train(data, data_column=DATA_COLUMN, nbr_of_lines=100):\n",
    "    if nbr_of_lines > 0:\n",
    "        train = pd.DataFrame(data[data_column].value_counts()[:nbr_of_lines])\n",
    "    else:\n",
    "        train = pd.DataFrame(data[data_column].value_counts())\n",
    "    train = train.rename(columns={data_column: \"frequency\"})\n",
    "    train[\"text\"] = train.index\n",
    "    train[\"index\"] = range(0, len(train))\n",
    "    train = train.set_index(\"index\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aff36-ce1e-472c-963c-c23dcf63b316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_info_data_train(data_column=DATA_COLUMN):\n",
    "    data = pd.read_csv(DATA_FILE, low_memory=False)\n",
    "    info_data_train = make_train(data, data_column, nbr_of_lines=0)\n",
    "    info_data_train = add_column_tokens(info_data_train)\n",
    "    info_data_train = add_column_labels(info_data_train)\n",
    "    return info_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574100b4-9711-4f6d-bf56-40de98bdd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data_train = make_info_data_train(data_column=DATA_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d70616-ba02-4cb8-9c7e-6c0eebbebc05",
   "metadata": {},
   "source": [
    "### 1.2 Make data for initial annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ded55-aebe-4f6f-b071-7263a04d4a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_selected_data_ids(info_data_train, selected_frequent, selected_random):\n",
    "    selected_data_ids = list(range(0, selected_frequent))\n",
    "    while len(selected_data_ids) < selected_frequent + selected_random:\n",
    "        selected_data_id = random.randint(selected_frequent, len(info_data_train) - 1)\n",
    "        if selected_data_id not in selected_data_ids:\n",
    "            selected_data_ids.append(selected_data_id)\n",
    "    return selected_data_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c2de4-534d-4447-b1af-3180ea593242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_selected_data_flags(info_data_train, selected_data_ids):\n",
    "    selected_data_flags = len(info_data_train) * [ False ]\n",
    "    for id_value in selected_data_ids:\n",
    "        selected_data_flags[id_value] = True\n",
    "    return selected_data_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f518425-2956-41d4-a8fa-04a382eade0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_annotated_data(info_data_train, selected_data_flags):\n",
    "    out_file = open(\"outfile.json\", \"w\")\n",
    "    selected_data = []\n",
    "    for index, row in info_data_train[selected_data_flags].iterrows():\n",
    "        text = \" \".join(row[\"tokens\"])\n",
    "        selected_data.append({ \"eid\": DATA_COLUMN[0] + str(index), \"text\": text, \"label\": [] })\n",
    "        print(selected_data[-1], file=out_file)\n",
    "    out_file.close()\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c62af9-2653-48fe-b0ca-49bc18752081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_data(info_data_train, selected_frequent, selected_random):\n",
    "    random.seed(42)\n",
    "    selected_data_ids = make_selected_data_ids(info_data_train, selected_frequent, selected_random)\n",
    "    selected_data_flags = make_selected_data_flags(info_data_train, selected_data_ids)\n",
    "    selected_data = save_annotated_data(info_data_train, selected_data_flags)\n",
    "    return selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6687544-ff14-4a42-936b-90ff68cd0af9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SELECTED_FREQUENT = 100\n",
    "SELECTED_RANDOM = 100\n",
    "\n",
    "# selected_data = make_data(info_data_train, selected_frequent=SELECTED_FREQUENT, selected_random=SELECTED_RANDOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa63a6-fd28-4e27-bcf0-13b35794976c",
   "metadata": {},
   "source": [
    "## 2. Machine learning\n",
    "\n",
    "Based on tutorial https://huggingface.co/transformers/v3.2.0/custom_datasets.html#token-classification-with-w-nut-emerging-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed17038-ae39-4b27-9185-f14863fcbfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import regex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy import displacy\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416ec63-1bf8-4d78-8324-4e690d8b8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_text(text, entities):\n",
    "    displacy.render({ \"text\": regex.sub(\"\\\\n\", \" \", text), \n",
    "                      \"ents\": entities }, \n",
    "                      options = { \"colors\": { \"fuzzy_match\": \"yellow\"} }, style = \"ent\", manual = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b9dcc-b26a-49a2-b5ed-ea25e6cd6d1b",
   "metadata": {},
   "source": [
    "### 2.1 Read annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2394b87-dfac-4e39-afd8-9dec35cb68dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ANNOTATIONS_FILE = \"../../data/annotated/2000.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b241a2c-4d89-4bd3-99b3-2883a61737bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_offset2label_pos(text):\n",
    "    offset2label_pos = {}\n",
    "    offset = 0\n",
    "    token_counter = 0\n",
    "    for token in text.split():\n",
    "        offset2label_pos[offset] = token_counter\n",
    "        offset += len(token) + 1\n",
    "        token_counter += 1\n",
    "    return offset2label_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e4822-9585-48ba-82bf-88416b1f6201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_label_start_not_token_initial(text, label_start):\n",
    "    while regex.search(\" \", text[label_start]):\n",
    "        label_start += 1\n",
    "    while label_start > 0 and not regex.search(\" \", text[label_start - 1]):\n",
    "        label_start -= 1\n",
    "    return label_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff681a-79fc-488d-830b-11e36a17bccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_labels(data):\n",
    "    text = data[\"data\"]\n",
    "    labels = [ \"O\" for token in text.split() ]\n",
    "    offset2label_pos = make_offset2label_pos(text)\n",
    "    for label in data[\"label\"]:\n",
    "        label[0] = fix_label_start_not_token_initial(text, label[0])\n",
    "        if label[0] not in offset2label_pos:\n",
    "            raise Exception(f\"{label[0]} not found in labels {offset2label_pos} of text {text}\")\n",
    "        else:\n",
    "            labels[offset2label_pos[label[0]]] = \"B-\" + label[2]\n",
    "            for i in range(label[0] + 1, label[1] + 1):\n",
    "                if i in offset2label_pos:\n",
    "                    labels[offset2label_pos[i]] = \"I-\" + label[2]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea277fd-a04c-421e-a4e9-b43c95fcf353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_jsonl_file(file_name):\n",
    "    annotations_file = open(file_name, \"r\")\n",
    "    texts = []\n",
    "    tags = []\n",
    "    ids = []\n",
    "    for line in annotations_file:\n",
    "        data = json.loads(line)\n",
    "        texts.append(data[\"data\"].split())\n",
    "        tags.append(make_labels(data))\n",
    "        ids.append(data[\"id\"])\n",
    "    annotations_file.close()\n",
    "    return texts, tags, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c528a02-d9a9-4eda-a22b-f04ab90d606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(annotated_texts, annotated_tags):\n",
    "    seen = {}\n",
    "    items_to_delete = []\n",
    "    for i in range(0, len(annotated_texts)):\n",
    "        text = annotated_texts[i]\n",
    "        if str(text) in seen:\n",
    "            print(text)\n",
    "            items_to_delete = [i] + items_to_delete\n",
    "        seen[str(text)] = True\n",
    "    return items_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a62f3-7f6f-474f-9a8b-4d0bdb12d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(annotated_texts, annotated_tags, annotated_ids):\n",
    "    items_to_delete = find_duplicates(annotated_texts, annotated_tags)\n",
    "    for i in items_to_delete:\n",
    "        annotated_texts.pop(i)\n",
    "        annotated_tags.pop(i)\n",
    "        annotated_ids.pop(i)\n",
    "    if len(items_to_delete) == 0:\n",
    "        print(\"no duplicates found\")\n",
    "    else:\n",
    "        print(f\"removed {len(items_to_delete)} duplicate\", end=\"\")\n",
    "        if len(items_to_delete) > 1:\n",
    "            print(\"s\", end=\"\")\n",
    "        print()\n",
    "    return annotated_texts, annotated_tags, annotated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87610b03-fd82-46fc-b491-af84c13c43a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotated_texts, annotated_tags, annotated_ids = read_jsonl_file(ANNOTATIONS_FILE)\n",
    "len(annotated_texts), len(annotated_tags), len(annotated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee51689-1f0a-450f-b8b4-3261f0a46bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_texts, annotated_tags, annotated_ids = remove_duplicates(annotated_texts, annotated_tags, annotated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea805b-7804-4045-b5ad-85461741f730",
   "metadata": {},
   "source": [
    "### 2.2 Check annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ed991-46f4-4028-a7b3-4ec08309665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags_for_token(target_token, annotated_texts, annotated_tags):\n",
    "    tags_found = {}\n",
    "    for tokens, tags in zip(annotated_texts, annotated_tags):\n",
    "        for token, tag in zip(tokens, tags):\n",
    "            if token == target_token:\n",
    "                if tag in tags_found:\n",
    "                    tags_found[tag] += 1\n",
    "                else:\n",
    "                    tags_found[tag] = 1\n",
    "    print({ pair[0]: pair[1] for pair in sorted(tags_found.items(), key=lambda x: x[1], reverse=True) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17315065-32e4-4ca8-901d-f7931d5d7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_phrase(phrase, phrases_found, text_id, target_text=\"\"):\n",
    "    if phrase != \"\":\n",
    "        if phrase == target_text:\n",
    "            print(text_id)\n",
    "        if phrase in phrases_found:\n",
    "            phrases_found[phrase] += 1\n",
    "        else:\n",
    "            phrases_found[phrase] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ddfaa-daa1-426d-b0b9-f3fc9ec8e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases_for_entity(entity, annotated_texts, annotated_tags, annotated_ids, target_text=\"\"):\n",
    "    phrases_found = {}\n",
    "    for tokens, tags, text_id in zip(annotated_texts, annotated_tags, annotated_ids):\n",
    "        phrase = \"\"\n",
    "        for token, tag in zip(tokens, tags):\n",
    "            if tag == \"B-\" + entity:\n",
    "                check_phrase(phrase, phrases_found, text_id, target_text)\n",
    "                phrase = token\n",
    "            elif tag == \"I-\" + entity:\n",
    "                phrase += \" \" + token\n",
    "            else:\n",
    "                check_phrase(phrase, phrases_found, text_id, target_text)\n",
    "                phrase = \"\"\n",
    "    print({ pair[0]: pair[1] for pair in sorted(phrases_found.items(), key=lambda x: x[1], reverse=True) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b9b8e-c4a7-4c2d-ae2c-2b05280b5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tags_for_token(\"van\", annotated_texts, annotated_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28ce8f-2a62-4620-ae81-13e181181c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_phrases_for_entity(\"TOPIC\", annotated_texts, annotated_tags, annotated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593d8cf-b441-4ee9-aabe-c4c25caa6ee5",
   "metadata": {},
   "source": [
    "### 2.3 Convert data to train set and validation set\n",
    "\n",
    "A tokenizer needs to be defined for breaking up the texts in known tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974af327-db79-4dd6-b93a-ca1d707bdd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(num_labels, model_name=\"GroNLP/bert-base-dutch-cased\"):\n",
    "    model = BertForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aea3f-482a-4421-ab09-6abb206d8c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_missing_I_tags(tags):\n",
    "    missing_tags = []\n",
    "    for tag in tags:\n",
    "        i_tag = regex.sub(r\"^B-\", \"I-\", tag)\n",
    "        if i_tag not in tags:\n",
    "            missing_tags.append(i_tag)\n",
    "    return list(tags) + missing_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e4673-8459-42ee-a5aa-221306002e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_B_to_I_tag(tag):\n",
    "    return regex.sub(r\"^B\", \"I\", tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f431ba-9046-47cd-a82a-d0441193e59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_tags(tags_in, encodings):\n",
    "    tags_out = [ [] for _ in range(len(encodings.offset_mapping,)) ]\n",
    "    for encodings_doc, tags_in_doc, tags_out_doc in zip(encodings.offset_mapping, tags_in, tags_out):\n",
    "        CLS_seen = False\n",
    "        SEP_seen = False\n",
    "        tags_counter = 0\n",
    "        for encoding in encodings_doc:\n",
    "            if encoding[1] == 0:\n",
    "                if not CLS_seen:\n",
    "                    tags_out_doc.append(\"CLS\")\n",
    "                    CLS_seen = True\n",
    "                elif not SEP_seen:\n",
    "                    tags_out_doc.append(\"SEP\")\n",
    "                    SEP_seen = True\n",
    "                else:\n",
    "                    tags_out_doc.append(\"PAD\")\n",
    "            elif encoding[0] == 0:\n",
    "                tags_out_doc.append(tags_in_doc[tags_counter])\n",
    "                tags_counter += 1\n",
    "            else:\n",
    "                tags_out_doc.append(convert_B_to_I_tag(tags_in_doc[tags_counter - 1]))\n",
    "    return tags_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cee79-11ae-4a73-af85-82bba30cb4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tags_to_numbers(tags, tag2id):\n",
    "    return [ [ tag2id[tag] for tag in doc ] for doc in tags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba62d6-0caa-49f7-a63f-bfcc242e745c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cbe0e-f7ea-4bdc-9854-60cb9838fa55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_annotated_data(annotated_texts, annotated_tags):\n",
    "    train_texts, val_texts, train_tags, val_tags = train_test_split(annotated_texts, \n",
    "                                                                    annotated_tags, \n",
    "                                                                    test_size=.2, \n",
    "                                                                    random_state=42)\n",
    "    return train_texts, val_texts, train_tags, val_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2ee85-783f-4af8-bac2-b5a1f012d16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_annotated_tags(annotated_tags):\n",
    "    unique_tags = set(tag for doc in annotated_tags for tag in doc )\n",
    "    unique_tags = sorted(add_missing_I_tags(unique_tags))\n",
    "    unique_types = list(set([ regex.sub(r\"^[BI]-\", \"\", tag) for tag in unique_tags ]))\n",
    "    tag2id = { tag: id for id, tag in enumerate(unique_tags) }\n",
    "    id2tag = { id: tag for tag, id in tag2id.items() }\n",
    "    return unique_tags, unique_types, tag2id, id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671b280-dcf8-4bce-9c1e-4a73b64305ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_labels(train_tags, val_tags, extra_tags):\n",
    "    train_labels = tags_to_numbers( split_tags(train_tags, train_encodings),\n",
    "                                    { **tag2id, **extra_tags})\n",
    "    val_labels =   tags_to_numbers( split_tags(val_tags, val_encodings),\n",
    "                                    { **tag2id, **extra_tags})\n",
    "    return train_labels, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ec5ba-1445-4714-b326-66c46eea3719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IGNORE_TAG_ID = -100\n",
    "\n",
    "extra_tags = { 'CLS': IGNORE_TAG_ID, 'SEP': IGNORE_TAG_ID, 'PAD': IGNORE_TAG_ID }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432345cf-ad2d-4026-8998-aa873a2da271",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_tags, val_tags = split_annotated_data(annotated_texts, annotated_tags)\n",
    "unique_tags, unique_types, tag2id, id2tag = analyze_annotated_tags(annotated_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87990f2e-0ce8-42bd-bdc4-75df0872515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    print(\"initializing model and tokenizer...\")\n",
    "    model, tokenizer = load_model(num_labels=len(unique_tags), model_name=\"GroNLP/bert-base-dutch-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96866fb1-e997-46e9-9148-04e45c534e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)\n",
    "val_encodings =   tokenizer(val_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd21f3-2f02-44b8-8d9a-8e5f125aef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, val_labels = make_labels(train_tags, val_tags, extra_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346470a9-6199-45e6-8948-23b45c1f36d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aa58a-c202-4004-b27d-589a995c97f7",
   "metadata": {},
   "source": [
    "### 2.4 Fine-tune model with data\n",
    "\n",
    "Using Bertje as base model: https://huggingface.co/GroNLP/bert-base-dutch-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e480aab-c670-4c84-a9ca-70db30ef4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, nbr_of_epochs=1):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=nbr_of_epochs,  # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                     # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,              # training arguments, defined above\n",
    "        train_dataset=train_dataset,     # training dataset\n",
    "        eval_dataset=val_dataset         # evaluation dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c520d-1ea5-44e9-8667-03b9de4eef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name):\n",
    "    print(f\"saving model {model_name}...\")\n",
    "    model.save_pretrained(f\"./models/{model_name}\")\n",
    "    tokenizer.save_pretrained(f\"./models/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ff458-e1b9-4f9d-a906-a4c764fd34bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_data = { \n",
    "\"1600k\": { 10: [8.605, 9.388613],   20: [8.1468, 8.581432],  30: [7.2292, 6.912892],  40: [5.2916, 4.459104],\n",
    "           50: [3.0505, 2.291136],  60: [1.9392, 1.396627],  70: [1.3153, 0.974922],  80: [0.9103, 0.611082]},\n",
    "\"1600l\": { 10: [2.7978, 2.72777],   20: [2.7011, 2.594252],  30: [2.5368, 2.377436],  40: [2.3084, 2.102149],\n",
    "           50: [1.9682, 1.813048],  60: [1.8059, 1.541138],  70: [1.5286, 1.295988],  80: [1.3218, 1.071442],\n",
    "           90: [1.1927, 1.059575], 100: [1.1952, 1.02582],  110: [1.0601, 0.972512], 120: [1.0243, 0.90479],\n",
    "          130: [0.9487, 0.831679], 140: [0.9014, 0.74661],  150: [0.7793, 0.673899], 160: [0.7085, 0.587361],\n",
    "          170: [0.6515, 0.582702], 180: [0.6932, 0.571471], 190: [0.5952, 0.55383],  200: [0.5968, 0.525524],\n",
    "          210: [0.5394, 0.497014], 220: [0.5214, 0.456926], 230: [0.4567, 0.422593], 240: [0.431, 0.380497],\n",
    "          250: [0.3958, 0.377924], 260: [0.4473, 0.372003], 270: [0.3626, 0.362811], 280: [0.3715, 0.345728],\n",
    "          290: [0.3268, 0.331447], 300: [0.3099, 0.309965], 310: [0.2709, 0.291733], 320: [0.2764, 0.274869],\n",
    "          330: [0.2474, 0.272652], 340: [0.2974, 0.268813], 350: [0.2275, 0.263912], 360: [0.2351, 0.251166],\n",
    "          370: [0.2027, 0.244896], 380: [0.1863, 0.234641], 390: [0.1571, 0.228038], 400: [0.1837, 0.229123],\n",
    "          410: [0.1653, 0.224482], 420: [0.2015, 0.21968],  430: [0.1479, 0.216111], 440: [0.1501, 0.211968],\n",
    "          450: [0.1255, 0.206149], 460: [0.1102, 0.202833], 470: [0.0887, 0.202619], 480: [0.1239, 0.213482]},\n",
    "\"1600m\": { 10: [3.0414, 3.001803],  20: [2.9454, 2.856211],  30: [2.7619, 2.614135],  40: [2.4844, 2.284899],\n",
    "           50: [2.1267, 1.891226],  60: [1.8582, 1.584162],  70: [1.5569, 1.312878],  80: [1.3307, 1.080446],\n",
    "           90: [1.2072, 1.06774],  100: [1.1987, 1.032933], 110: [1.067, 0.978802],  120: [1.0432, 0.911087],\n",
    "          130: [0.9554, 0.834676], 140: [0.8992, 0.745508], 150: [0.7885, 0.660629], 160: [0.7031, 0.568473],\n",
    "          170: [0.6488, 0.563569], 180: [0.6766, 0.55112],  190: [0.5832, 0.530326], 200: [0.5905, 0.500823],\n",
    "          210: [0.526, 0.470368],  220: [0.4952, 0.431339], 230: [0.4296, 0.391956], 240: [0.4074, 0.349349],\n",
    "          250: [0.3719, 0.347294], 260: [0.4158, 0.342064], 270: [0.3355, 0.333896], 280: [0.3472, 0.319556], \n",
    "          290: [0.3024, 0.306712], 300: [0.2787, 0.288847], 310: [0.2306, 0.271399], 320: [0.2517, 0.257995],\n",
    "          330: [0.2279, 0.256285], 340: [0.2737, 0.252748], 350: [0.2027, 0.248475], 360: [0.2106, 0.239845],\n",
    "          370: [0.1792, 0.23603],  380: [0.1602, 0.22973],  390: [0.1281, 0.222972], 400: [0.1683, 0.227971],\n",
    "          410: [0.1554, 0.222322], 420: [0.1879, 0.216514], 430: [0.1279, 0.212594], 440: [0.1291, 0.209587],\n",
    "          450: [0.1072, 0.206074], 460: [0.0913, 0.206748], 470: [0.0697, 0.205872], 480: [0.1197, 0.22059],\n",
    "          490: [0.1455, 0.210196], 500: [0.1858, 0.202405], 510: [0.1059, 0.192074], 520: [0.135, 0.189939],\n",
    "          530: [0.1383, 0.187261], 540: [0.1395, 0.179227], 550: [0.1186, 0.17271],  560: [0.1097, 0.162364],\n",
    "          570: [0.0839, 0.169346], 580: [0.1037, 0.17176],  590: [0.0733, 0.167507], 600: [0.0823, 0.160418],\n",
    "          610: [0.1072, 0.156252], 620: [0.0786, 0.155454], 630: [0.0787, 0.153239], 640: [0.0995, 0.157059]\n",
    "         },\n",
    "\"2000a\": { 10: [3.1072, 3.052986],  20: [3.0062, 2.904115],  30: [2.8227, 2.666382],  40: [2.5515, 2.343706],\n",
    "           50: [2.2182, 1.966769],  60: [1.8726, 1.637894],  70: [1.5951, 1.37603],   80: [1.3811, 1.116854],\n",
    "           90: [1.1845, 0.876394], 100: [0.9552, 0.707807], 110: [0.9001, 0.701295], 120: [0.8582, 0.680713],\n",
    "          130: [0.766, 0.64848],   140: [0.7666, 0.610998], 150: [0.739, 0.567632],  160: [0.5975, 0.515508],\n",
    "          170: [0.6255, 0.46969],  180: [0.527, 0.415647],  190: [0.4658, 0.365828], 200: [0.4232, 0.325514],\n",
    "          210: [0.4591, 0.323955], 220: [0.4186, 0.315625], 230: [0.3616, 0.301866], 240: [0.3674, 0.291112],\n",
    "          250: [0.3563, 0.276382], 260: [0.2481, 0.255775], 270: [0.2955, 0.243],    280: [0.2497, 0.228334],\n",
    "          290: [0.1997, 0.207621], 300: [0.2092, 0.199684], 310: [0.2703, 0.199463], 320: [0.2252, 0.194289], \n",
    "          330: [0.1971, 0.185111], 340: [0.1894, 0.180386], 350: [0.187, 0.175801],  360: [0.1026, 0.167931],\n",
    "          370: [0.1451, 0.168511], 380: [0.1144, 0.17272],  390: [0.0827, 0.158728], 400: [0.1173, 0.171397]\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4768a-402f-4c08-a480-29d1a082748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "def convert_eval_scores_to_dict(string):\n",
    "    eval_dict = {}\n",
    "    token_list = []\n",
    "    for token in string.split():\n",
    "        token_list.append(token)\n",
    "        if len(token_list) >= 3:\n",
    "            eval_dict[int(token_list[0])] = [ float(token_list[1]), float(token_list[2]) ]\n",
    "            token_list = []\n",
    "    if len(token_list) > 0:\n",
    "        print(f\"there were unprocessed tokens! ({token_list})\")\n",
    "    return eval_dict\n",
    "\n",
    "convert_eval_scores_to_dict(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddd835-1afe-4b47-b040-4dc8ea85d24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = train_model(model, nbr_of_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab73ca-663e-4a94-9cd0-4d13b12e2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_texts([ \" \".join(text) for text in val_texts ], val_labels, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a988c0-bd60-491b-88c0-d6ffd6b62ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\"2000a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd06da-3691-4c24-81c5-a57f4dbf8a47",
   "metadata": {},
   "source": [
    "### 2.5 Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d72e6-12bf-4da8-aa6b-90c2e23b6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_eval_data(trainer):\n",
    "    eval_data = {}\n",
    "    for data in trainer.state.log_history:\n",
    "        if data[\"step\"] not in eval_data:\n",
    "            eval_data[data[\"step\"]] = [0 , 0]\n",
    "        if \"loss\" in data:\n",
    "            eval_data[data[\"step\"]][0] = data[\"loss\"]\n",
    "        if \"eval_loss\" in data:\n",
    "            eval_data[data[\"step\"]][1] = data[\"eval_loss\"]\n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf08f37-9349-4b86-a1ae-7b2b76aba367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval_data(eval_data):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot([data_key for data_key in eval_data], [eval_data[data_key][0] for data_key in eval_data], label=\"training loss\")\n",
    "    plt.plot([data_key for data_key in eval_data], [eval_data[data_key][1] for data_key in eval_data], label=\"validation loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75d44c-f745-450c-9688-7b8b1719a6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_eval_data(make_eval_data(trainer))\n",
    "#plot_eval_data(eval_data[\"1600m\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b087ff-6f39-44c5-87e1-6631aa60c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_entities(tag_id_list, token_id_list):\n",
    "    entities = []\n",
    "    token_counter = 0\n",
    "    current_tag = (\"\", -1)\n",
    "    for tag, token in zip(tag_id_list, token_id_list):\n",
    "        tag_start = tag[0]\n",
    "        tag_class = regex.sub(r\"^[BI]-\", \"\", tag)\n",
    "        current_tag_class = current_tag[0]\n",
    "        current_tag_start = current_tag[1]\n",
    "        if regex.search(r\"^##\", token):\n",
    "            token_counter -= 1\n",
    "        if current_tag_class != \"\" and not regex.search(r\"^##\", token):\n",
    "            if tag_class == \"O\" or tag_start == \"B\" or tag_class != current_tag_class:\n",
    "                entities.append([current_tag_start, token_counter, current_tag_class])\n",
    "                current_tag = (\"\", -1)\n",
    "                current_tag_class = \"\"\n",
    "                current_tag_start = -1\n",
    "        if tag_class != \"O\" and current_tag_class == \"\":\n",
    "            current_tag = (tag_class, token_counter)\n",
    "            if regex.search(r\"^##\", token) and (len(entities) == 0 or entities[-1][2] != token_counter):\n",
    "                current_tag = (tag_class, token_counter - 1)\n",
    "        token_counter += 1\n",
    "    if current_tag_class != \"\":\n",
    "        entities.append([current_tag_start, token_counter, current_tag_class])\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495cd6f2-1232-46a3-984b-87f230ac8aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_precision_and_recall(correct_count, missed_count, wrong_count):\n",
    "    for tag in [ \"total\" ] + sorted(correct_count):\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        if correct_count[tag] > 0 or wrong_count[tag] > 0:\n",
    "            precision = correct_count[tag]/(correct_count[tag] + wrong_count[tag])\n",
    "        if correct_count[tag] > 0 or missed_count[tag]:\n",
    "            recall = correct_count[tag]/(correct_count[tag] + missed_count[tag])\n",
    "        print(f\"precision: {int(100*precision):-3d}; recall: {int(100*recall):-3d}; count: {correct_count[tag] + missed_count[tag]:4d}; tag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fde91-b970-4723-9a69-04205ff675ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_ids(label_ids):\n",
    "    return [ id2tag[label_id] for label_id in label_ids if label_id != IGNORE_TAG_ID ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165aa77c-3d80-487d-9891-69dcc1bf6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_results(sentence_result):\n",
    "    return get_labels_from_ids([ int(regex.sub(\"^LABEL_\", \"\", token_result[\"entity\"])) for token_result in sentence_result ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78431472-36de-4da3-81cb-cc7d01ad9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_tokens_from_results(sentence_result):\n",
    "    return [ token_result[\"word\"] for token_result in sentence_result ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ead6a-e824-463e-aacb-8f739653bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_split_tokens(split_tokens):\n",
    "    combined_tokens = []\n",
    "    for token in split_tokens:\n",
    "        if not regex.search(r\"^##\", token):\n",
    "            combined_tokens.append(token)\n",
    "        else:\n",
    "            combined_tokens[-1] += regex.sub(r\"^##\", \"\", token)\n",
    "    return combined_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5077d5a-90a1-4c00-a7a9-303881e37b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize(text):\n",
    "    return regex.sub(\" ##\", \"\", \" \".join(tokenizer.tokenize(\" \".join(nltk.word_tokenize(text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f2f78-c857-443f-96c6-2e480580753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenization(texts):\n",
    "    nbr_of_mismatches = 0\n",
    "    for input_text in texts:\n",
    "        processed_text = retokenize(input_text)\n",
    "        if processed_text != input_text:\n",
    "            nbr_of_mismatches += 0\n",
    "    if nbr_of_mismatches > 0:\n",
    "        print(f\"tokenization mismatches: {nbr_of_mismatches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99fbc9-933f-48bb-ae7b-17c50fc57e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_results(text_entities, text_tokens, error_count):\n",
    "    text = f\"({error_count})\"\n",
    "    tags = []\n",
    "    token_counter = 0\n",
    "    in_tag = False\n",
    "    for entity in text_entities:\n",
    "        entity_token_start, entity_token_end, entity_label = entity\n",
    "        for i in range(token_counter, entity_token_start):\n",
    "            text += \" \" + text_tokens[i]\n",
    "        entity_char_start = len(text) + 1\n",
    "        for i in range(entity_token_start, entity_token_end):\n",
    "            text += \" \" + text_tokens[i]\n",
    "        entity_char_end = len(text)\n",
    "        tags.append( { \"start\": entity_char_start, \"end\": entity_char_end, \"label\": entity_label } )\n",
    "        token_counter = entity_token_end\n",
    "    render_text(text, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8945a-4286-4e78-995a-72e1f1b32dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results_per_entity(results, correct_label_ids, check_labels=False):\n",
    "    correct_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    missed_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    wrong_count = { tag: 0 for tag in unique_types if tag != \"O\" }\n",
    "    errors_per_text = []\n",
    "    for sentence_result, correct_sentence_label_ids in zip(results, correct_label_ids):\n",
    "        guessed_labels = get_labels_from_results(sentence_result)\n",
    "        split_tokens = get_split_tokens_from_results(sentence_result)\n",
    "        correct_labels = get_labels_from_ids(correct_sentence_label_ids)\n",
    "        guessed_entities = results_to_entities(guessed_labels, split_tokens)\n",
    "        correct_entities = results_to_entities(correct_labels, split_tokens)\n",
    "        error_count = 0\n",
    "        for entity in correct_entities:\n",
    "            if entity in guessed_entities:\n",
    "                correct_count[entity[2]] += 1\n",
    "            else:\n",
    "                missed_count[entity[2]] += 1\n",
    "                error_count += 1\n",
    "        for entity in guessed_entities:\n",
    "            if entity not in correct_entities:\n",
    "                wrong_count[entity[2]] += 1\n",
    "                error_count += 1\n",
    "        errors_per_text.append(error_count)\n",
    "        if check_labels and error_count > 0:\n",
    "            render_results(guessed_entities, combine_split_tokens(split_tokens), error_count)\n",
    "            render_results(correct_entities, combine_split_tokens(split_tokens), 0)\n",
    "            print(\"\")\n",
    "    return correct_count, missed_count, wrong_count, errors_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad9083-2c23-495d-b075-dbdc4102ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_texts(texts, labels, model, tokenizer, check_labels=False):\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "    results = [ ner_pipeline(text) for text in texts ]\n",
    "    test_tokenization(texts)\n",
    "    correct_count, missed_count, wrong_count , errors_per_text = evaluate_results_per_entity(results, labels, check_labels)\n",
    "    correct_count[\"total\"] = sum(correct_count.values())\n",
    "    wrong_count[\"total\"] = sum(wrong_count.values())\n",
    "    missed_count[\"total\"] = sum(missed_count.values())\n",
    "    compute_precision_and_recall(correct_count, missed_count, wrong_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9f516-1a91-4cce-a1bf-b5974196cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(num_labels=len(unique_tags), model_name=\"models/1600m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63ac76-1203-471a-8a17-ccddc717de4b",
   "metadata": {},
   "source": [
    "Now regenerate train and val data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b41cd-0442-43ed-afdc-b3537c6d8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_texts([ \" \".join(text) for text in val_texts ], val_labels, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d7a04-93e5-41ad-a4d2-2908fcecb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_tags = [ [ id2tag[list(guesses_per_token).index(max(guesses_per_token))]\n",
    "                   for guesses_per_token in guesses ] \n",
    "                   for guesses in results[0] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b4c61-9ae4-4023-959e-75e1e565a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_render = 1\n",
    "\n",
    "text_counter = 0\n",
    "for sentence_result, error_count in zip(results, errors_per_text):\n",
    "    guessed_labels = get_labels_from_results(sentence_result)\n",
    "    split_tokens = get_split_tokens_from_results(sentence_result)\n",
    "    guessed_entities = results_to_entities(guessed_labels, split_tokens)\n",
    "    print(guessed_entities, combine_split_tokens(split_tokens), error_count)\n",
    "    render_results(guessed_entities, combine_split_tokens(split_tokens), error_count)\n",
    "    text_counter += 1\n",
    "    if text_counter >= max_render:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6affb-b542-4a7c-8697-b07cc62dfb72",
   "metadata": {},
   "source": [
    "### 2.6 Select extra data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e6e5e-d1b7-4850-84a6-69f6c1d2d187",
   "metadata": {},
   "source": [
    "Training data selection process:\n",
    "\n",
    "1. 100 most frequent data from each half and 100 randomly selected (total 400)\n",
    "2. 50 with most of ENSLAVED|FREED|OWNER tags and 50 random with one of these tags (total 200)\n",
    "3. 50 randomly selcted data of each half with one of the tags ENSLAVED|FREED (total 100)\n",
    "4. 150 randomly selcted data of each half (total 300)\n",
    "\n",
    "Total: 1000 (3 duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab81790-7d3a-4809-a6a3-878be0352318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_id_entities_to_char_id_entities(token_id_entities, split_tokens):\n",
    "    char_id_entities = []\n",
    "    tokens = combine_split_tokens(split_tokens)\n",
    "    for token_id_entity in token_id_entities:\n",
    "        char_start = 0\n",
    "        for i in range(0, token_id_entity[0]):\n",
    "            char_start += len(tokens[i]) + 1\n",
    "        char_end = char_start\n",
    "        for i in range(token_id_entity[0], token_id_entity[1]):\n",
    "            char_end += len(tokens[i]) + 1\n",
    "        char_id_entities.append([char_start, char_end - 1, token_id_entity[2]])\n",
    "    return char_id_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa750a7-837c-4fd2-8fa2-6c0a515a9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognized_entities_to_annotation_labels(entities):\n",
    "    split_tokens = get_split_tokens_from_results(entities)\n",
    "    labels = get_labels_from_results(entities)\n",
    "    token_id_entities = results_to_entities(labels, split_tokens)\n",
    "    char_id_entities = token_id_entities_to_char_id_entities(token_id_entities, split_tokens)\n",
    "    return char_id_entities\n",
    "\n",
    "# recognized_entities_to_annotation_labels(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ba5ab-c5e5-492e-ad54-8fcd63d3caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_of_selected_extra_data(selected_extra_data, sample_size=10):\n",
    "    for i in range(0, sample_size):\n",
    "        text = selected_extra_data[i][\"data\"][\"text\"]\n",
    "        labels = [{\"start\": data[0], \"end\": data[1], \"label\": data[2]} for data in selected_extra_data[i][\"data\"][\"label\"] ]\n",
    "        render_text(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad401fbe-da55-478f-945a-a0f281eb54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = joblib.load('./model/model-1000h.joblib')\n",
    "model = BertForTokenClassification.from_pretrained(\"models/1600m\", num_labels=len(unique_tags))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/1600m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c3a5e-9fde-4733-a60f-6a2d34af7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ea0a7b-cbcd-4e67-8286-c29005fd746e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = \"EndEntryInfo\"\n",
    "\n",
    "info_data_train = make_info_data_train(data_column=DATA_COLUMN)\n",
    "extra_data = make_data(info_data_train, selected_frequent=0, selected_random=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047da473-bb0f-4ccc-ac9b-cdc47b87f661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_extra_data = []\n",
    "for data in extra_data:\n",
    "    if nltk.word_tokenize(data[\"text\"]) not in annotated_texts:\n",
    "        tag_counter = 0\n",
    "        entities = ner_pipeline(data[\"text\"])\n",
    "        data[\"label\"] = recognized_entities_to_annotation_labels(entities)\n",
    "        data[\"text\"] = retokenize(data[\"text\"])\n",
    "        selected_extra_data.append({ \"tag_counter\": tag_counter, \"data\": data })\n",
    "len(selected_extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8590f9-515b-4742-97e0-f184852fa7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_of_selected_extra_data(selected_extra_data, sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be37cbe-1605-451d-acf9-f1d84a4a59e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_file = open(\"outfile.json\", \"w\")\n",
    "for data in sorted(selected_extra_data, key=lambda data: data[\"tag_counter\"], reverse=True)[:400]:\n",
    "    print(json.dumps(data[\"data\"]), file=out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4413068-4100-48e4-8d4c-0db893a5abdf",
   "metadata": {},
   "source": [
    "### 2.7 Process other data with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d6018-a97f-40a6-82ce-ef7524f65728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(selected_entities):\n",
    "    for entity_list in selected_entities:\n",
    "        for entity in entity_list:\n",
    "            entity[\"label\"] = id2tag[int(regex.sub(r\"^LABEL_\", \"\", entity[\"entity\"]))]\n",
    "    return selected_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ab2f8-9169-4cef-b5a6-b15e85e492b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(selected_data[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e09ec3-06f2-49b6-b2e1-6c0e6bf55f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = make_data(info_data_train, selected_frequent=0, selected_random=10)\n",
    "selected_entities = [ ner_pipeline(data[\"text\"]) for data in selected_data ]\n",
    "selected_entities = add_labels(selected_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8f88e-1929-4d28-9ebb-5475774cbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_results(selected_entities, \n",
    "               [ tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data[\"text\"])) for data in selected_data],\n",
    "               len(entities) * [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ecaf4e-07f8-48d3-8c4d-a2ca95d68398",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_entities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028a02f-b921-471b-b929-daa5816fc0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
