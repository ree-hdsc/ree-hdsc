{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acff869-67df-4b20-a939-86b1b609059e",
   "metadata": {},
   "source": [
    "# Info fields via machine learning\n",
    "\n",
    "Extract persons from the info fields StartEntryInfo and EndEntryInfo of the [slave registers of Suriname](https://datasets.iisg.amsterdam/dataset.xhtml?persistentId=hdl:10622/CSPBHO) via machine learning\n",
    "\n",
    "See: https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afe6d3-87bf-4c94-b48d-9cf0b81c1a4c",
   "metadata": {},
   "source": [
    "## 1. Annotating info fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39531b3-deb1-4d22-9586-de290836ee04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import regex\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa999620-9243-433e-9e92-63ffa5607c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/suriname/Dataset Suriname Slave and Emancipation Registers Version 1.1.csv\"\n",
    "\n",
    "data = pd.read_csv(DATA_FILE, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cdd66-c9bc-40f4-a145-73228319d7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_tokens(train):\n",
    "    train[\"tokens\"] = [ nltk.word_tokenize(text) for text in train[\"text\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3f692-e25b-4fca-aed5-98c2b8211153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_labels(train):\n",
    "    train[\"labels\"] = [ len(tokens) * [ \"O\" ] for tokens in train[\"tokens\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22f9b5-a3c3-46d7-b0fb-34a7b2f37581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_column_numeric_labels(train, numeric_labels):\n",
    "    train[\"numeric_labels\"] = [ [ numeric_labels[label] for label in labels ] for labels in train[\"labels\"] ]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17279108-eb94-46c0-87ed-655e84eeeef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_date(day, month, year):\n",
    "    return regex.search(r\"^\\d\\d\\d\\d\\b\", year) and regex.search(r\"^\\d\\d?$\", day) and True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09845b-a11a-46e3-8041-ac4128800650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_date_tags_to_labels(labels, index):\n",
    "    labels[index - 2], labels[index - 1], labels[index] = \"B-DATE\", \"I-DATE\", \"I-DATE\"\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ca68a-b24e-4f94-b1d0-a05737cc549e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def label_dates(train):\n",
    "    for index, row in train.iterrows():\n",
    "        for i in range(2, len(row[\"tokens\"])):\n",
    "            if is_date(row[\"tokens\"][i-2], row[\"tokens\"][i-1], row[\"tokens\"][i]):\n",
    "                add_date_tags_to_labels(row[\"labels\"], i)\n",
    "    return train       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333bfb9-6b81-4ce5-9927-12a1e86d7d75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_annotations(train):\n",
    "    for index in range(0, len(train)):\n",
    "        for i in range(0, len(train[\"labels\"][index])):\n",
    "            print(train[\"tokens\"][index][i], end=\"\")\n",
    "            if train[\"labels\"][index][i] != \"O\":\n",
    "                print(\"/\" + train[\"labels\"][index][i], end=\"\")\n",
    "            print(\" \", end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fe91b-e883-406e-b7af-eb8f7a198a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train(data, nbr_of_lines=50):\n",
    "    train = pd.DataFrame(data[\"EndEntryInfo\"].value_counts()[:nbr_of_lines])\n",
    "    train = train.rename(columns={\"EndEntryInfo\": \"frequency\"})\n",
    "    train[\"text\"] = train.index\n",
    "    train[\"index\"] = range(0, len(train))\n",
    "    train = train.set_index(\"index\")\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27aff36-ce1e-472c-963c-c23dcf63b316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = make_train(data)\n",
    "train = add_column_tokens(train)\n",
    "train = add_column_labels(train)\n",
    "train = label_dates(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa421d11-bdd2-47ef-aa24-a797afc94609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_labels = { \"O\": 0, \"B-DATE\": 1, \"I-DATE\": 2 }\n",
    "\n",
    "train = add_column_numeric_labels(train, numeric_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a481b2-7ae8-4e0b-af77-cc0390af894e",
   "metadata": {},
   "source": [
    "### 1.1 Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e54e5e-7129-4eb2-9902-a56d016b0ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"wietsedv/bert-base-dutch-cased-finetuned-udlassy-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ea959-f4f5-4044-aeb7-9b22c3d6fc71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(train[\"tokens\"][0], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6a4dd-5d93-4512-bbf6-967c60fe89a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361e66b-1c00-4eb2-a874-0e1d3926c310",
   "metadata": {},
   "source": [
    "## 2. Tutorial for token classification\n",
    "\n",
    "https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "\n",
    "Required modules to install: `pip install transformers datasets evaluate seqeval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab50c05-4168-4eeb-842b-3a138fbfeb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b5c93-e02a-46b1-9ad9-ba62c01b475f",
   "metadata": {},
   "source": [
    "### 2.1 Setting up tutodial data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff921774-b61f-4f08-bad3-9af4b68f38e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wnut = datasets.load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca475d-bdce-4cbf-a6bc-196220a43ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key in wnut:\n",
    "    print(key, len(wnut[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61918306-acd9-4ca1-b005-86a3db3a8991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wnut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec435af8-3f16-423f-9d19-f1486115e9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wnut_label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "wnut_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40774ed-c38c-4db9-8ba8-8dd9dee318cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(wnut), type(wnut[\"train\"]), type(wnut[\"train\"][\"tokens\"]), type(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427264eb-8aa4-426c-b29d-d10a2900d995",
   "metadata": {},
   "source": [
    "### 2.2 Setting up info fields data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000904e-f330-4a40-b81c-7de5963fcf27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_info_field_data(train):\n",
    "    info_fields_data = {}\n",
    "    info_fields_data[\"train\"] = datasets.arrow_dataset.Dataset.from_list([ { \"id\": i, \"tokens\": train[\"tokens\"][i], \"ner_tags\": train[\"labels\"][i] } \n",
    "                                for i in range(0, int(0.5 + 0.6 * len(train))) ])\n",
    "    info_fields_data[\"test\"] = datasets.arrow_dataset.Dataset.from_list([ { \"id\": i, \"tokens\": train[\"tokens\"][i], \"ner_tags\": train[\"labels\"][i] } \n",
    "                               for i in range(int(0.5 + 0.6 * len(train)), len(train)) ])\n",
    "    info_fields_data[\"validation\"] = info_fields_data[\"test\"]\n",
    "    return info_fields_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d51e8-6e93-4ffe-a3c6-439031176767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_fields_data = make_info_field_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4530373-536c-437f-acbd-24954d16ea39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_fields_label_list = list(numeric_labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c62054-ddc9-4eae-a3a7-0bd795f25760",
   "metadata": {},
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c920eb-76e3-428a-9c1f-3620f2fbba96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e28c1a-f5ce-4138-a68f-155009b946be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training_data = wnut\n",
    "#label_list = wnut_label_list\n",
    "training_data = info_fields_data\n",
    "label_list = info_fields_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a57f6-c908-4b34-9eae-05dcbb47f97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = training_data[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab744e1e-93fd-4e2e-92fb-a7ce7938fd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca32c24-797d-4ed9-aa1b-46f77db3943e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46bf1c1-dd30-4999-babb-06d2e34e03b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_training_data = training_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c51a83-9c77-44a9-8a38-2c58426f7edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfcc4c-3198-4a56-be29-a7038e973b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f053e4-3380-40d5-a278-64b095aa5a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adb284e-6635-4d76-89a5-9a5af520bd98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1d0d6-61fd-42c6-8fe3-d8a877924a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62f80c-7fef-417d-b6d9-7ac3f903796e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False, ### <--- changed\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89c878c1-81de-4539-814e-ab0139bc9fcd",
   "metadata": {},
   "source": [
    "Epoch \tTraining Loss \tValidation \tPrecision \tRecall \tF1 \tAccuracy\n",
    "        1 \tNo log \t0.282456 \t0.528226 \t0.242817 \t0.332698 \t0.937711\n",
    "        2 \tNo log \t0.274149 \t0.559853 \t0.281742 \t0.374846 \t0.940832\n",
    "\n",
    "Epoch \tTraining Loss \tValidation \tPrecision \tRecall \tF1 \tAccuracy\n",
    "        1 \tNo log \t0.255242 \t0.501940 \t0.359592 \t0.419006 \t0.943226\n",
    "        2 \tNo log \t0.282641 \t0.562500 \t0.358665 \t0.438031 \t0.945791"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dffad-756c-43d8-a644-e6aa2422358a",
   "metadata": {},
   "source": [
    "### 2.4 Post-training tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ed2f7c-78e0-45aa-862c-f0fc7b427c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8d6d8-6b2a-4584-973a-1c19caae5dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SKIP\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\", \"stevhliu/my_awesome_wnut_model\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33018a-db43-408c-8c6a-4ae061348c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SKIP\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37db521-d00a-4e25-a907-b2049c143bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SKIP\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae240a94-eb9c-4934-8d2b-55ad31866d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd1a4c-64a3-4bed-ae67-fabe8e259220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bea111-be87-48c7-883e-8c16d00b9624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ab58f-4bbc-4a11-9764-223c8ddb4d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "predicted_token_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ff27a-fe1e-4e37-bcda-f398f5ef9700",
   "metadata": {},
   "source": [
    "## 3 Tutorial for IMDB data\n",
    "\n",
    "https://huggingface.co/transformers/v3.2.0/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da7237-d559-4bfd-82b0-8f49806dd831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632546c2-880d-4494-9e04-d576c4e36229",
   "metadata": {},
   "source": [
    "### 3.1 Read data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c1f50-8a0d-4d63-8f17-e742ee919b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = [ text_file.read_text() for label_dir in [\"pos\", \"neg\"] \n",
    "                                    for text_file in (split_dir/label_dir).iterdir() ]\n",
    "    labels = ( len(list((split_dir/\"pos\").iterdir())) * [1] + \n",
    "               len(list((split_dir/\"neg\").iterdir())) * [0] )\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed8928-143e-429a-b53d-2ed64fd2bfad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_imdb_split('data/aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('data/aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8445af-f48e-4b7c-a066-029bd3bdc877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f2b59-6c73-4994-943b-1e09485b5257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts = train_texts[:1000]\n",
    "train_labels = train_labels[:1000]\n",
    "test_texts = test_texts[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "val_texts = val_texts[:1000]\n",
    "val_labels = val_labels[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59480a4d-09a7-4a22-be77-0948076fac2c",
   "metadata": {},
   "source": [
    "### 3.2 Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ddf261-b726-4595-9379-783169fbeaad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0b5b0-992a-458a-b18f-3c60ebbdf584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ee367-222f-492d-b493-aff187676330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee71c00-b561-4bba-bcd7-f9b02d81b44c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10fcf8b-4815-475c-af47-eac8a592b87a",
   "metadata": {},
   "source": [
    "### 3.3 Fine-tune model with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e337194-bedd-41c5-9906-85d9090f6d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b908e-8fcc-4733-95da-c3e7a9ad9710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408a749-0f06-4da0-95dc-280e40ad800e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82deca-f841-4593-9b3d-211c4c9cd2c6",
   "metadata": {},
   "source": [
    "Error: Kernel Restarting || The kernel for info_fields_ml.ipynb appears to have died. It will restart automatically.\n",
    "\n",
    "Solutions: \n",
    "1. run with smaller datasets (definitely necessary)\n",
    "2. re-install torchvision: (might not make a difference)\n",
    "* !pip3 uninstall -y torch torchvision\n",
    "* !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f0664-a7c0-4fa1-9376-1ddd77d76739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738bb75-4ae9-4cdc-9b47-ba977f266454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5c596-d666-4f6a-8037-07bb1e82097e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29434f25-e6bc-4b70-8de7-7d988be8e0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct_counter = 0\n",
    "for i in range(0, len(results[0])):\n",
    "    result = 1\n",
    "    if results[0][i][0] > results[0][i][1]:\n",
    "        result = 0\n",
    "    if result == val_labels[i]:\n",
    "        correct_counter += 1\n",
    "correct_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac731f1-c01c-492e-b25a-0050b82fd880",
   "metadata": {},
   "source": [
    "The tutorial contains no example code for testing but see: https://huggingface.co/docs/transformers/main_classes/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa63a6-fd28-4e27-bcf0-13b35794976c",
   "metadata": {},
   "source": [
    "## 4. Tutorial for wnut data\n",
    "\n",
    "https://huggingface.co/transformers/v3.2.0/custom_datasets.html#token-classification-with-w-nut-emerging-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed17038-ae39-4b27-9185-f14863fcbfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import regex\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56713256-b77c-4e5a-884f-f232faa754d8",
   "metadata": {},
   "source": [
    "### 4.1 Read data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae170ee3-624b-47bc-b199-6e3a876aca3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = regex.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = [ [ line.split('\\t')[0] for line in doc.split('\\n') ] for doc in raw_docs ]\n",
    "    tag_docs = [ [ line.split('\\t')[1] for line in doc.split('\\n') ] for doc in raw_docs ]\n",
    "    return token_docs, tag_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9a9c3-516b-4b4f-83e3-17b8c0070a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts, tags = read_wnut('data/wnut17train.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cbe0e-f7ea-4bdc-9854-60cb9838fa55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593d8cf-b441-4ee9-aabe-c4c25caa6ee5",
   "metadata": {},
   "source": [
    "### 4.2 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2ee85-783f-4af8-bac2-b5a1f012d16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc )\n",
    "tag2id = { tag: id for id, tag in enumerate(unique_tags) }\n",
    "id2tag = { id: tag for tag, id in tag2id.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961de56-1194-44f7-b20f-4d36879459c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96866fb1-e997-46e9-9148-04e45c534e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)\n",
    "val_encodings =   tokenizer(val_texts, \n",
    "                            is_split_into_words=True, \n",
    "                            return_offsets_mapping=True, \n",
    "                            padding=True, \n",
    "                            truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e4673-8459-42ee-a5aa-221306002e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_B_to_I_tag(tag):\n",
    "    return regex.sub(r\"^B\", \"I\", tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f431ba-9046-47cd-a82a-d0441193e59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_tags(tags_in, encodings):\n",
    "    tags_out = [ [] for _ in range(len(encodings.offset_mapping,)) ]\n",
    "    for encodings_doc, tags_in_doc, tags_out_doc in zip(encodings.offset_mapping, tags_in, tags_out):\n",
    "        CLS_seen = False\n",
    "        SEP_seen = False\n",
    "        tags_counter = 0\n",
    "        for encoding in encodings_doc:\n",
    "            if encoding[1] == 0:\n",
    "                if not CLS_seen:\n",
    "                    tags_out_doc.append(\"CLS\")\n",
    "                    CLS_seen = True\n",
    "                elif not SEP_seen:\n",
    "                    tags_out_doc.append(\"SEP\")\n",
    "                    SEP_seen = True\n",
    "                else:\n",
    "                    tags_out_doc.append(\"PAD\")\n",
    "            elif encoding[0] == 0:\n",
    "                tags_out_doc.append(tags_in_doc[tags_counter])\n",
    "                tags_counter += 1\n",
    "            else:\n",
    "                tags_out_doc.append(convert_B_to_I_tag(tags_in_doc[tags_counter - 1]))\n",
    "    return tags_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cee79-11ae-4a73-af85-82bba30cb4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tags_to_numbers(tags, tag2id):\n",
    "    return [ [ tag2id[tag] for tag in doc ] for doc in tags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ec5ba-1445-4714-b326-66c46eea3719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extra_tags = { 'CLS': -100, 'SEP': -100, 'PAD': -100 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671b280-dcf8-4bce-9c1e-4a73b64305ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = tags_to_numbers( split_tags(train_tags, train_encodings),\n",
    "                                { **tag2id, **extra_tags})\n",
    "val_labels =   tags_to_numbers( split_tags(val_tags, val_encodings),\n",
    "                                { **tag2id, **extra_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba62d6-0caa-49f7-a63f-bfcc242e745c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346470a9-6199-45e6-8948-23b45c1f36d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1aa58a-c202-4004-b27d-589a995c97f7",
   "metadata": {},
   "source": [
    "### 4.3 Fine-tune model with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb74416-007d-4d52-9f3b-0ea13646fe69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7304d-d4fb-4314-bdd5-8451b149f87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4f40f-ef07-49f2-81a5-1c313d9d24d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eddd835-1afe-4b47-b040-4dc8ea85d24c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f621a4-b656-4e2f-befb-4a7aa7ddafe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3713cb-1c14-4cc4-9f10-577a5acf8406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab46522-649d-4ea7-8250-49d0a0d2eebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_results(results):\n",
    "    correct_count = 0\n",
    "    missed_count = 0\n",
    "    wrong_count = 0\n",
    "    for guesses, corrects in zip(results[0], results[1]):\n",
    "        for guess_values, correct in zip(guesses, corrects):\n",
    "            guess = list(guess_values).index(max(guess_values))\n",
    "            if correct != -100:\n",
    "                if correct != tag2id['O'] and guess == correct:\n",
    "                    correct_count += 1\n",
    "                else:\n",
    "                    if correct != tag2id['O']:\n",
    "                        missed_count += 1\n",
    "                    if guess != tag2id['O']:\n",
    "                        wrong_count += 1           \n",
    "    precision = correct_count/(correct_count + wrong_count)\n",
    "    recall = correct_count/(correct_count + missed_count)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35201e78-1541-42f4-93b8-c0a16a87c368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall = evaluate_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4987fa5-71a8-4226-9030-7b9e80dafe09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1282e-e1d6-463e-a2f1-8359d405ba76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inspect_results(results, encodings):\n",
    "    for guess_data, correct_data, token_data in zip(results[0], results[1], encodings):\n",
    "        for guess_values, correct, token in zip(guess_data, correct_data, tokenizer.convert_ids_to_tokens(token_data)):\n",
    "            guess = list(guess_values).index(max(guess_values))\n",
    "            if correct != -100:\n",
    "                print(token, end=\"\")\n",
    "                if guess != -100 and guess != tag2id['O']:\n",
    "                    print(\"/\" + id2tag[guess], end=\" \")\n",
    "                print(\" \", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0435b6-cd12-46a9-93ad-df3be66048cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inspect_results(results, val_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a180e03-1656-4a2e-bdec-12936d86f808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([101, 137])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be5bf7-17a3-4feb-9835-99146bce5b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
