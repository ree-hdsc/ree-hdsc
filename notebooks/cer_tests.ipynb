{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c777b58-5543-4526-9e58-b03ffc0f0d26",
   "metadata": {},
   "source": [
    "# CER tests\n",
    "\n",
    "Tests with character error rate (CER) computation with the goal of evaluating parts of documents that were generated with hand-written text recognition (HTR). For this purpose we use the [fastwer Python package](https://pypi.org/project/fastwer/) which can compute word error reates and character error rates for similar texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c1e4c-3860-41e4-bbc9-f8676175784d",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2bd8c-0e8f-4f6d-93b2-c6e8c3b8cb09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import fastwer\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "from scripts import read_transkribus_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da4fe3b-1818-47d6-be3c-1be6ddd862c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data for HTR bestModel Curarao\n",
    "data_dir_gold = \"tmp/1586854/Validation_set/page\"\n",
    "data_dir_htr = \"tmp/1749649/Validation_set_HTR_Curacao_bestModel/page\"\n",
    "# single page test of Loghi with the above validation data\n",
    "data_dir_htr = \"../../loghi/images.20240126.01/page.republicprint.baseline\"\n",
    "# sample test 1: 100 newly annotated files from January 2024\n",
    "###data_dir_gold= \"tmp/1616639/Sample_test_1/page\"\n",
    "###data_dir_htr = \"tmp/1764847/Sample_test_1/page\"\n",
    "# ten-page test with loghi; documents from sample test 1\n",
    "###data_dir_htr = \"../../loghi/images/page\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99456a-1cda-426d-9d5c-a20b93be64ce",
   "metadata": {},
   "source": [
    "Training data for model Dutchess Curacao (276):\n",
    "\n",
    "* Training_extra_0001 - Training_extra_0011 (11)\n",
    "* p001 - p100 (skipped p010, p020, *p021*, p030, *p036*, p040, *p044*, p050, p060, p070, p080, p090, *p098*, p100) (86)\n",
    "* Sample_regex-0001 - Sample_regex-0100 (skipped: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100) (90)\n",
    "* p055 - p100 (skipped 55, 62, 71, 85, 90) \n",
    "* p001 - p054 (skipped 5, 12, 18, 31, 43, 51) - 54: 25-08-1880 577 (89)\n",
    "\n",
    "Validation data (35):\n",
    "\n",
    "- Validation_extra-0001 - Validation_extra-0005 (skipped 3) (4)\n",
    "- p010 - p100 (10)\n",
    "- Sample_regex-0010 - Sample_regex-0100 (10)\n",
    "- p055, 62, 71, 85, 90\n",
    "- p005, 12, 18, 31, 43, 51 (11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd503987-9d2c-4a30-8174-b455dcbc9f6b",
   "metadata": {},
   "source": [
    "## 2. Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9807d-8b6f-4a2b-bc6f-18ed0daf66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_gold, metadata_gold, textregions_gold  = read_transkribus_files.read_files(data_dir_gold)\n",
    "texts_htr, metadata_htr, textregions_htr  = read_transkribus_files.read_files(data_dir_htr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3beee-bad3-41df-831f-29766292627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not regex.search(\"loghi\", data_dir_htr):\n",
    "    for file_name in texts_htr:\n",
    "        print(f\"{file_name}: combining {len(texts_htr[file_name])} text regions\")\n",
    "        texts_htr[file_name] = \"\".join(texts_htr[file_name])\n",
    "else:\n",
    "    if len(texts_htr) != 1 or list(texts_htr.keys())[0] != \"p010.xml\":\n",
    "        print(\"unexpected loghi data: {texts_htr.keys()}\")\n",
    "    elif regex.search(\"cBAD\", data_dir_htr):\n",
    "        texts_htr[\"p010.xml\"] = (texts_htr[\"p010.xml\"][0] +\n",
    "                                 texts_htr[\"p010.xml\"][1] +\n",
    "                                 texts_htr[\"p010.xml\"][2] +\n",
    "                                 texts_htr[\"p010.xml\"][3] +\n",
    "                                 texts_htr[\"p010.xml\"][4] +\n",
    "                                 texts_htr[\"p010.xml\"][7] +\n",
    "                                 texts_htr[\"p010.xml\"][5] +\n",
    "                                 texts_htr[\"p010.xml\"][4])\n",
    "    elif regex.search(\"general\", data_dir_htr):\n",
    "        texts_htr[\"p010.xml\"] = (texts_htr[\"p010.xml\"][0] +\n",
    "                                 texts_htr[\"p010.xml\"][6] +\n",
    "                                 texts_htr[\"p010.xml\"][1] +\n",
    "                                 texts_htr[\"p010.xml\"][2] +\n",
    "                                 texts_htr[\"p010.xml\"][3] +\n",
    "                                 texts_htr[\"p010.xml\"][5] +\n",
    "                                 texts_htr[\"p010.xml\"][4])\n",
    "    elif regex.search(\"republic\\.\", data_dir_htr):\n",
    "        texts_htr[\"p010.xml\"] = (texts_htr[\"p010.xml\"][0] +\n",
    "                                 texts_htr[\"p010.xml\"][10] +\n",
    "                                 texts_htr[\"p010.xml\"][1] +\n",
    "                                 texts_htr[\"p010.xml\"][2] +\n",
    "                                 texts_htr[\"p010.xml\"][9] +\n",
    "                                 texts_htr[\"p010.xml\"][3] +\n",
    "                                 texts_htr[\"p010.xml\"][4] +\n",
    "                                 texts_htr[\"p010.xml\"][5] +\n",
    "                                 texts_htr[\"p010.xml\"][8] +\n",
    "                                 texts_htr[\"p010.xml\"][6] +\n",
    "                                 texts_htr[\"p010.xml\"][7])\n",
    "    elif regex.search(\"republicprint\", data_dir_htr):\n",
    "        texts_htr[\"p010.xml\"] = (texts_htr[\"p010.xml\"][0] +\n",
    "                                 texts_htr[\"p010.xml\"][32] +\n",
    "                                 texts_htr[\"p010.xml\"][27] +\n",
    "                                 texts_htr[\"p010.xml\"][1] +\n",
    "                                 texts_htr[\"p010.xml\"][23] +    \n",
    "                                 texts_htr[\"p010.xml\"][12] +\n",
    "                                 texts_htr[\"p010.xml\"][7] +\n",
    "                                 texts_htr[\"p010.xml\"][16] +\n",
    "                                 texts_htr[\"p010.xml\"][10] +\n",
    "                                 texts_htr[\"p010.xml\"][2] +\n",
    "                                 texts_htr[\"p010.xml\"][3] +\n",
    "                                 texts_htr[\"p010.xml\"][4] +\n",
    "                                 texts_htr[\"p010.xml\"][5] +\n",
    "                                 texts_htr[\"p010.xml\"][6] +\n",
    "                                 texts_htr[\"p010.xml\"][8] +\n",
    "                                 texts_htr[\"p010.xml\"][9] +\n",
    "                                 texts_htr[\"p010.xml\"][11] +\n",
    "                                 texts_htr[\"p010.xml\"][13] +\n",
    "                                 texts_htr[\"p010.xml\"][14] +\n",
    "                                 texts_htr[\"p010.xml\"][15] +\n",
    "                                 texts_htr[\"p010.xml\"][17] +\n",
    "                                 texts_htr[\"p010.xml\"][18] +\n",
    "                                 texts_htr[\"p010.xml\"][19] +\n",
    "                                 texts_htr[\"p010.xml\"][20] +\n",
    "                                 texts_htr[\"p010.xml\"][21] +\n",
    "                                 texts_htr[\"p010.xml\"][22] +\n",
    "                                 texts_htr[\"p010.xml\"][24] +\n",
    "                                 texts_htr[\"p010.xml\"][25] +\n",
    "                                 texts_htr[\"p010.xml\"][26] +\n",
    "                                 texts_htr[\"p010.xml\"][28] +\n",
    "                                 texts_htr[\"p010.xml\"][29] +\n",
    "                                 texts_htr[\"p010.xml\"][30] +\n",
    "                                 texts_htr[\"p010.xml\"][31] +\n",
    "                                 texts_htr[\"p010.xml\"][33] +\n",
    "                                 texts_htr[\"p010.xml\"][34] +\n",
    "                                 texts_htr[\"p010.xml\"][35] +\n",
    "                                 texts_htr[\"p010.xml\"][36] +\n",
    "                                 texts_htr[\"p010.xml\"][37] +\n",
    "                                 texts_htr[\"p010.xml\"][38] +\n",
    "                                 texts_htr[\"p010.xml\"][39] +\n",
    "                                 texts_htr[\"p010.xml\"][40] +\n",
    "                                 texts_htr[\"p010.xml\"][41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18323029-9a4d-4e7a-a985-c1a485faa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in texts_gold:\n",
    "    print(f\"{file_name}: combining {len(texts_gold[file_name])} text regions\")\n",
    "    texts_gold[file_name] = \"\".join(texts_gold[file_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2004c-803d-4e73-8113-f8d8f2837e39",
   "metadata": {},
   "source": [
    "## 2. Compute CER per text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105c443-d99a-4a2c-8eef-0a3d1b02670a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_cer_per_text(texts_htr, texts_gold):\n",
    "    total_cer = 0\n",
    "    total_chars = 0\n",
    "    for file_name in sorted(texts_htr.keys()):\n",
    "        cer = fastwer.score_sent(texts_htr[file_name], texts_gold[file_name], char_level=True)\n",
    "        max_text_length = max(len(texts_htr[file_name]), len(texts_gold[file_name]))\n",
    "        total_chars += max_text_length\n",
    "        total_cer += max_text_length * cer\n",
    "        print(f\"{cer:4.1f}  {file_name}\")\n",
    "    print(f\"average cer: {total_cer/total_chars:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef57fdc-2b6d-4ee8-b904-b4e56911a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cer_per_text(texts_htr, texts_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab0a3a-9d30-488f-a23b-62db0b3286a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{\"dataset\": \"CBAD\", \"order\": [0,1,2,3,6,7,5,4], \"#order\": 8, \"cer\": 59},\n",
    "              {\"dataset\": \"general\", \"order\": [0,6,1,2,3,5,4], \"#order\": 7, \"cer\": 22},\n",
    "              {\"dataset\": \"republic\", \"order\": [0,10,1,2,9,3,4,5,8,6,7], \"#order\": 11, \"cer\": 38},\n",
    "              {\"dataset\": \"republicprint\", \n",
    "               \"order\": [0,32,27,1,23,12,7,16,10,2,3,4,5,6,8,9,11,13,14,15,17,18,19,20,21,24,25,26,28,29,30,31,32,33,34,35,36,37,38,39,40,41], \n",
    "               \"#order\": 42, \"cer\": 62}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b04f7-3159-4f37-a294-5e2fe2c0a1c2",
   "metadata": {},
   "source": [
    "## 3. Correct printed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ee898-872e-42b0-b09b-b08fc81d2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_year(text_htr):\n",
    "    years = []\n",
    "    numbers = { \"een\": 1, \"twee\": 2, \"drie\": 3, \"vier\": 4, \"vijf\": 5, \"zes\": 6, \"zeven\": 7, \"acht\": 8, \"negen\": 9, \"tien\": 10, \n",
    "                \"elf\": 11, \"twaalf\": 12, \"dertien\": 13, \"veertien\": 14, \"vijftien\": 15, \"zestien\": 16, \"zeventien\": 17, \"achttien\": 18, \"negentien\": 19,\n",
    "                \"twintig\": 20, \"dertig\": 30, \"veertig\": 40, \"vijftig\": 50, \"zestig\": 60, \"zeventig\": 70, \"tachtig\": 80, \"negentig\": 90 }\n",
    "    for line in text_htr.split(\"\\n\"):\n",
    "        tokens = line.lower().split()\n",
    "        for tokens_index in range(0, len(tokens)):\n",
    "            if tokens[tokens_index] == \"honderd\" and tokens_index > 0:\n",
    "                if tokens[tokens_index - 1] in [ \"zeven\", \"zeventien\" ]:\n",
    "                    year = 1700\n",
    "                elif tokens[tokens_index - 1] in [ \"acht\", \"achttien\" ]:\n",
    "                    year = 1800\n",
    "                elif tokens[tokens_index - 1] in [ \"negen\", \"negentien\" ]:\n",
    "                    year = 1900\n",
    "                else:\n",
    "                    year = 0\n",
    "                    print(f\"unexpected token before \\\"honderd\\\": {tokens[tokens_index - 1]}\")\n",
    "                if tokens_index < len(tokens) - 1 and tokens[tokens_index +  1] in numbers:\n",
    "                    year += numbers[tokens[tokens_index + 1]]\n",
    "                if tokens_index < len(tokens) - 2 and tokens[tokens_index +  1] == \"en\" and tokens[tokens_index +  2] in numbers:\n",
    "                    year += numbers[tokens[tokens_index + 2]]\n",
    "                if tokens_index < len(tokens) - 3 and tokens[tokens_index +  2] == \"en\" and tokens[tokens_index +  3] in numbers:\n",
    "                    year += numbers[tokens[tokens_index + 3]]                \n",
    "                years.append(year)\n",
    "    return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e884c98-87dd-400b-b531-f4597a5e6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in texts_htr:\n",
    "    print(guess_year(texts_htr[file_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124ba4b-a311-4eb8-a0ff-71f2017d266e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713586f3-edf3-45ec-a10c-70c29e58db7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00d6c13-152d-4fab-a6e7-5ebfb8b154b8",
   "metadata": {},
   "source": [
    "## 4. Find incorrect words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d071b58-cedb-417d-94e2-deff3bb11856",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_best_line_match(lines_htr, lines_gold, index_htr, gold_index_used, alignments):\n",
    "    best_index_gold = -1\n",
    "    best_cer = 100\n",
    "    line_htr = lines_htr[index_htr]\n",
    "    index_htr_old = -1\n",
    "    for index_gold in range(0, len(lines_gold)):\n",
    "        cer = fastwer.score_sent(line_htr, lines_gold[index_gold], char_level=True)\n",
    "        if cer < best_cer and ( index_gold not in gold_index_used.keys() or cer < gold_index_used[index_gold][0]):\n",
    "            best_cer = cer\n",
    "            best_index_gold = index_gold\n",
    "            if index_gold in gold_index_used:\n",
    "                index_htr_old = gold_index_used[index_gold][1]\n",
    "            gold_index_used[index_gold] = [ best_cer, index_htr ]\n",
    "            if index_htr_old >= 0:\n",
    "                best_index_gold_old, best_cer_old, gold_index_used, alignments = find_best_line_match(lines_htr, \n",
    "                                                                                                      lines_gold, \n",
    "                                                                                                      index_htr_old,\n",
    "                                                                                                      gold_index_used,\n",
    "                                                                                                      alignments)\n",
    "                alignments[index_htr_old] = [ best_index_gold_old, best_cer_old ]\n",
    "    alignments[index_htr] = [ best_index_gold, best_cer ]\n",
    "    return best_index_gold, best_cer, gold_index_used, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b2428-cc50-46d3-809f-a21043eaec60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def align_lines(lines_htr, lines_gold):\n",
    "    alignments = {}\n",
    "    gold_index_used = {}\n",
    "    for index_htr in range(0, len(lines_htr)):\n",
    "        best_index_gold, best_cer, gold_index_used, alignments = find_best_line_match(lines_htr, lines_gold, index_htr, gold_index_used, alignments)\n",
    "    alignments = convert_alignments(alignments)\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b7eda-9504-481b-a4fa-17603d4e311a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_line(line):\n",
    "    line = line.lower()\n",
    "    line = regex.sub(\"y\", \"ij\", line)\n",
    "    line = line.translate(str.maketrans(\"çéëóü\", \"ceeou\"))\n",
    "    line = regex.sub('^[.,!?:;\")(-]+', \"\", line)\n",
    "    line = regex.sub('\\s[.,!?:;\")(-]+', \" \", line)\n",
    "    line = regex.sub('[.,!?:;\")(-]+\\s', \" \", line)\n",
    "    line = regex.sub('[.,!?:;\")(-]+$', \"\", line)\n",
    "    return(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babcae18-bc32-4302-9075-500fde5023fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text):\n",
    "    return \"\\n\".join([cleanup_line(line) for line in  text.split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ab03a-6c23-410f-8fa2-0ce7310e531e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def count_distances(alignments):\n",
    "    return Counter([ alignment[1] - alignment[0] for alignment in alignments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58adad-8589-4fa2-a5d3-25966283bf49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_alignments_order(alignments):\n",
    "    last_gold_index = -1\n",
    "    to_be_deleted = []\n",
    "    distances = count_distances(alignments)\n",
    "    for alignment_index in range(0, len(alignments)):\n",
    "        if last_gold_index < alignments[alignment_index][1]:\n",
    "            last_gold_index = alignments[alignment_index][1]\n",
    "        elif alignments[alignment_index][2] > alignments[alignment_index - 1][2]:\n",
    "            to_be_deleted.append(alignment_index)\n",
    "        elif alignments[alignment_index][2] < alignments[alignment_index - 1][2]:\n",
    "            to_be_deleted.append(alignment_index - 1)\n",
    "            last_gold_index = alignments[alignment_index][1]\n",
    "        elif (distances[alignments[alignment_index][1] - alignments[alignment_index][0]] <\n",
    "              distances[alignments[alignment_index - 1][1] - alignments[alignment_index - 1][0]]):\n",
    "            to_be_deleted.append(alignment_index)\n",
    "        else:\n",
    "            to_be_deleted.append(alignment_index - 1)\n",
    "            last_gold_index = alignments[alignment_index][1]\n",
    "    for to_be_deleted_value in list(reversed(to_be_deleted)):\n",
    "        alignments.pop(to_be_deleted_value)\n",
    "    to_be_added = []\n",
    "    for alignment_index in range(1, len(alignments)):\n",
    "        if ((alignments[alignment_index][0] - alignments[alignment_index - 1][0] == \n",
    "             alignments[alignment_index][1] - alignments[alignment_index - 1][1]) and\n",
    "            alignments[alignment_index][0] - alignments[alignment_index - 1][0] != 1):\n",
    "            for alignment_index_delta in range(1, alignments[alignment_index][0] - alignments[alignment_index - 1][0]):\n",
    "                to_be_added.append((alignment_index, \n",
    "                                    alignments[alignment_index - 1][0] + alignment_index_delta,\n",
    "                                    alignments[alignment_index - 1][1] + alignment_index_delta))\n",
    "    for to_be_added_element in list(reversed(to_be_added)):\n",
    "        alignments.insert(to_be_added_element[0], (to_be_added_element[1],\n",
    "                                                   to_be_added_element[2],\n",
    "                                                   fastwer.score_sent(lines_htr[to_be_added_element[1]], \n",
    "                                                                      lines_gold[to_be_added_element[2]], \n",
    "                                                                      char_level=True)))\n",
    "    return len(to_be_deleted) > 0 or len(to_be_added) > 0, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14f106-3e1f-4a3e-b01d-bea0287a58ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_alignments_order_wrapper(alignments):\n",
    "    alignments_changed = True\n",
    "    while alignments_changed:\n",
    "        alignments_changed, alignments = check_alignments_order(alignments)\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc73f3-9392-4cdc-8824-df1f73eed356",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fix_split_words(words_htr, words_gold, wrong_words, missed_words):\n",
    "    to_be_deleted = []\n",
    "    for index_wrong in range(1, len(wrong_words)):\n",
    "        if wrong_words[index_wrong] == wrong_words[index_wrong - 1] + 1:\n",
    "            combined_word = (words_htr[wrong_words[index_wrong - 1]] + \n",
    "                             words_htr[wrong_words[index_wrong]])\n",
    "            for index_missed in range(0, len(missed_words)):\n",
    "                if words_gold[missed_words[index_missed]] == combined_word:\n",
    "                    to_be_deleted.append((index_wrong - 1, index_wrong, index_missed))\n",
    "                    break\n",
    "    for to_be_deleted_item in list(reversed(to_be_deleted)):\n",
    "        for to_be_deleted_wrong_index in range(to_be_deleted_item[1], to_be_deleted_item[0] - 1, -1):\n",
    "            wrong_words.pop(to_be_deleted_wrong_index)\n",
    "        missed_words.pop(to_be_deleted_item[2])\n",
    "    return wrong_words, missed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ac023-2a9f-4396-9336-6b5235ffca59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_words(line_htr, line_gold):\n",
    "    missed_words = []\n",
    "    wrong_words = []\n",
    "    if line_htr != line_gold:\n",
    "        words_htr = line_htr.split()\n",
    "        words_gold = line_gold.split()\n",
    "        alignments = align_lines(words_htr, words_gold)\n",
    "        alignments = check_alignments_order_wrapper(alignments)\n",
    "        index_htr = 0\n",
    "        index_gold = 0\n",
    "        for index_alignment in range(0, len(alignments)):\n",
    "            target_index_htr = alignments[index_alignment][0]\n",
    "            target_index_gold = alignments[index_alignment][1]\n",
    "            while index_htr < len(words_htr) and index_htr < target_index_htr:\n",
    "                wrong_words.append(index_htr)\n",
    "                index_htr += 1\n",
    "            while index_gold < len(words_gold) and index_gold < target_index_gold:\n",
    "                missed_words.append(index_gold)\n",
    "                index_gold += 1\n",
    "            if words_htr[target_index_htr] != words_gold[target_index_gold]:\n",
    "                missed_words.append(target_index_gold)\n",
    "                wrong_words.append(index_htr)\n",
    "            index_htr += 1\n",
    "            index_gold += 1\n",
    "        for index_htr_extra in range(index_htr, len(words_htr)):\n",
    "            wrong_words.append(index_htr)\n",
    "        for index_gold_extra in range(index_gold, len(words_gold)):\n",
    "            missed_words.append(index_gold_extra)\n",
    "        wrong_words, missed_words = fix_split_words(words_htr, words_gold, wrong_words, missed_words)\n",
    "    return wrong_words, missed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135a760-fc74-4940-901c-4a6f7c7fd376",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_lines(lines_htr, lines_gold, alignments):\n",
    "    index_htr = 0\n",
    "    line_analysis = []\n",
    "    for alignment in alignments:\n",
    "        for index_htr_delta in range(1, alignment[0]-index_htr):\n",
    "             line_analysis.append(analyze_words(lines_htr[index_htr + index_htr_delta], \"\"))\n",
    "        line_analysis.append(analyze_words(lines_htr[alignment[0]], lines_gold[alignment[1]]))\n",
    "        index_htr = alignment[0] + 1\n",
    "    for index_htr_delta in range(1, len(lines_htr)-index_htr):\n",
    "        line_analysis.append(analyze_words(lines_htr[index_htr + index_htr_delta], \"\"))\n",
    "    return line_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5d291-a70d-4a73-860d-d26902c46666",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_word_analysis(line_htr, line_gold, line_analysis_line):\n",
    "    words_htr = line_htr.split()\n",
    "    words_gold = line_gold.split()\n",
    "    for index_htr in range(0, len(words_htr)):\n",
    "        if index_htr in line_analysis_line[0]:\n",
    "            read_transkribus_files.print_with_color(words_htr[index_htr], color_code=1, end=\" \")\n",
    "        else:\n",
    "            read_transkribus_files.print_with_color(words_htr[index_htr], color_code=0, end=\" \")\n",
    "    if len(line_analysis_line[1]) > 0:\n",
    "        read_transkribus_files.print_with_color([ words_gold[index_gold] \n",
    "                                                  for index_gold in line_analysis_line[1] ], \n",
    "                                                color_code=4, \n",
    "                                                end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2112768-e7ac-4e22-8198-d788214da11a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_line_analysis(lines_htr, lines_gold, alignments, line_analysis):\n",
    "    index_htr = 0\n",
    "    for alignment in alignments:\n",
    "        for index_htr_extra in range(index_htr, alignment[0]):\n",
    "            show_word_analysis(lines_htr[index_htr_extra], \"\", [[], []])\n",
    "        show_word_analysis(lines_htr[alignment[0]], \n",
    "                           lines_gold[alignment[1]], \n",
    "                           line_analysis[alignment[0]])\n",
    "        index_htr = alignment[0] + 1\n",
    "    for index_htr_extra in range(index_htr, len(lines_htr)):\n",
    "        show_word_analysis(lines_htr[index_htr_extra], \"\", [[], []])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61919a1b-925f-4ef7-9653-c5e6bec42725",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_alignments(alignments):\n",
    "    alignments_converted = []\n",
    "    for alignments_key in alignments.keys():\n",
    "        alignments_converted.append((alignments_key, alignments[alignments_key][0], alignments[alignments_key][1]))\n",
    "    return alignments_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69504699-89eb-4108-995b-b008ad6b228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in sorted(texts_htr.keys()):\n",
    "    lines_htr = cleanup_text(texts_htr[file_name]).split(\"\\n\")\n",
    "    lines_gold = cleanup_text(texts_gold[file_name]).split(\"\\n\")\n",
    "    alignments = align_lines(lines_htr, lines_gold)\n",
    "    cer = fastwer.score_sent(texts_htr[file_name], texts_gold[file_name], char_level=True)\n",
    "    read_transkribus_files.print_with_color(f\"{file_name} (cer={cer:.1f}):\\n\", color_code=4)\n",
    "    line_analysis = analyze_lines(lines_htr, lines_gold, alignments)\n",
    "    show_line_analysis(lines_htr, lines_gold, alignments, line_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5a229-a6ac-43ac-918e-3afa20ca35e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
