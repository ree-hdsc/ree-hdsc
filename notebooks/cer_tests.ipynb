{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c777b58-5543-4526-9e58-b03ffc0f0d26",
   "metadata": {},
   "source": [
    "# CER tests\n",
    "\n",
    "Tests with character error rate (CER) computation with the goal of evaluating parts of documents that were generated with hand-written text recognition (HTR). For this purpose we use the [fastwer Python package](https://pypi.org/project/fastwer/) which can compute word error reates and character error rates for similar texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c1e4c-3860-41e4-bbc9-f8676175784d",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df403e9-53f0-4c25-ab85-acc41b2a6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2bd8c-0e8f-4f6d-93b2-c6e8c3b8cb09",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "import fastwer\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex\n",
    "from spacy import displacy\n",
    "import sys\n",
    "from termcolor import colored\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "from scripts import read_transkribus_files, printed_text, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b11ac-eb88-4d1c-8ba4-8288fd1ecac9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def render_text(text, entities):\n",
    "    \"\"\"Display text with colored entities.\n",
    "\n",
    "    Keyword arguments:\n",
    "    text -- string\n",
    "    entities -- list of dicts like: [{\"start\": 0, \"end\": 6, \"label\": \"PERSON\"}]\n",
    "    \"\"\"\n",
    "    displacy.render({\"text\": regex.sub(\"\\\\n\", \" \", text),\n",
    "                    \"ents\": entities},\n",
    "                    options={\"colors\": {\"fuzzy_match\": \"yellow\"}},\n",
    "                    style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1fdca-4449-4886-ba7f-2ca9d8023a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20240326 new htr model tests\n",
    "# data_dir_gold = \"tmp/1900167/Test_oldDutchess_(corrected)/page\"\n",
    "# data_dir_htr = {\"bestmodel\": \"tmp/1900168/Test_oldDutchess_(corrected)/page\",\n",
    "#                \"dutchess\": \"tmp/1893490/Test_oldDutchess/page\",\n",
    "#                \"new_dutchess\": \"tmp/1893491/Test_newDutchess/page\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99456a-1cda-426d-9d5c-a20b93be64ce",
   "metadata": {},
   "source": [
    "Training data for model 42578 Curacao_Dutchess (278 files):\n",
    "\n",
    "* Training_extra_0001 - Training_extra_0011 (11 files)\n",
    "* p001 - p101 (skipped p010, p020, *p021*, p030, *p036*, p040, *p044*, p050, p060, p070, p080, p090, p100) (88 files)\n",
    "* Sample_regex-0001 - Sample_regex-0100 (skipped: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100) (90 files)\n",
    "* p055 - p100 (skipped 55, 62, 71, 85, 90) \n",
    "* p001 - p054 (skipped 5, 12, 18, 31, 43, 51) - 54: 25-08-1880 577 (89 files)\n",
    "\n",
    "Validation data (35 files):\n",
    "\n",
    "- Validation_extra-0001 - Validation_extra-0005 (skipped 3) (4 files)\n",
    "- p010 - p100 (10 files)\n",
    "- Sample_regex-0010 - Sample_regex-0100 (10 files)\n",
    "- p055, 62, 71, 85, 90\n",
    "- p005, 12, 18, 31, 43, 51 (11 files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feab568a-1a6c-48e9-b66c-08c91d3b0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing: 7213/72dpi (empty)\n",
    "\n",
    "data_files = [\n",
    "    [\"10946395\", \"2483583\", \"CurTSSR3\"],\n",
    "    [\"10946397\", \"2471264\", \"ground_truth\"],\n",
    "    [\"11035298\", \"2493743\", \"CurTSSR7\"],\n",
    "    [\"11035339\", \"2531603\", \"CurTSSR13\"],\n",
    "    [\"11035356\", \"2535323\", \"CurTSSR15\"],\n",
    "    [\"11035397\", \"2488423\", \"CurTSSR4\"],\n",
    "    [\"11035399\", \"2492783\", \"CurTSSR6\"],\n",
    "    [\"11035403\", \"2530883\", \"CurTSSR12\"],\n",
    "    [\"11035557\", \"2549503\", \"CurTSSR16\"],\n",
    "    [\"11035560\", \"2600443\", \"CurTSSR27\"],\n",
    "    [\"11035562\", \"2619783\", \"CurTSSR5_NB\"],\n",
    "    [\"11035576\", \"2550123\", \"CurTSSR17\"],\n",
    "    [\"11035594\", \"2568143\", \"CurTSSR20\"],\n",
    "    [\"11035653\", \"2546503\", \"CurTSSR18\"],\n",
    "    [\"11035655\", \"2568883\", \"CurTSSR21\"],\n",
    "    [\"11035656\", \"2584923\", \"CurTSSR23\"],\n",
    "    [\"11035673\", \"2599743\", \"CurTSSR25\"],\n",
    "    [\"11035675\", \"2623583\", \"CurTSSR15_NB\"],\n",
    "    [\"11035676\", \"2637703\", \"CurTSSR25_NB\"],\n",
    "    [\"11035692\", \"2600203\", \"CurTSSR26\"],\n",
    "    [\"11035715\", \"2637163\", \"CurTSSR20_NB\"],\n",
    "    [\"11035754\", \"2622863\", \"CurTSSR10_NB\"],\n",
    "    [\"11036995\", \"3194127\", \"144dpi\"],\n",
    "    [\"11037173\", \"3194114\", \"35dpi\"],\n",
    "    [\"11037174\", \"3194504\", \"300dpi\"],\n",
    "    [\"11037182\", \"2471264_dpi\", \"dpi_ground_truth\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a4ead-4868-493a-b243-ae53fc0abd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_df = pd.DataFrame(data_files, columns=[\"export_id\", \"experiment_id\", \"experiment_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494dbca-deaf-4ace-874a-eaa98d3df8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_gold = \"202407/2471264_dpi/Training_set_small/page\"\n",
    "data_dir_htr = {}\n",
    "for index, row in data_files_df.iterrows():\n",
    "    if regex.search('[0-9]dpi', row[\"experiment_name\"]):\n",
    "        # data_dir_htr[row[\"experiment_name\"]] = f\"202407/{row['experiment_id']}/TRAINING_VALIDATION_SET_{row['experiment_name']}/page\"\n",
    "        data_dir_htr[row[\"experiment_name\"]] = regex.sub(\"dpi\", \"DPI\", f\"202407/{row['experiment_id']}/Cur_Scanqualtest_{row['experiment_name']}/page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbd4e9-14af-4446-8ef3-6efe1583c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_htr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd503987-9d2c-4a30-8174-b455dcbc9f6b",
   "metadata": {},
   "source": [
    "## 2. Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9807d-8b6f-4a2b-bc6f-18ed0daf66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(data_dir_gold, data_dir_htr):\n",
    "    \"\"\"Read gold and htr data and return results as dicts.\"\"\"\n",
    "    (texts_gold,\n",
    "     metadata_gold,\n",
    "     textregions_gold) = read_transkribus_files.read_files(data_dir_gold)\n",
    "    (texts_htr,\n",
    "     metadata_htr,\n",
    "     textregions_htr) = read_transkribus_files.read_files(data_dir_htr)\n",
    "    for file_name in texts_htr:\n",
    "        texts_htr[file_name] = \"\".join(texts_htr[file_name])\n",
    "    for file_name in texts_gold:\n",
    "        texts_gold[file_name] = \"\".join(texts_gold[file_name])\n",
    "    return texts_gold, texts_htr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52505aaf-8237-4742-8ffa-40d5efea48af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def word_count_text_list(texts):\n",
    "    \"\"\"Return number of tokens in text.\"\"\"\n",
    "    return len([token for text_id in texts.keys()\n",
    "                for token in texts[text_id].split()\n",
    "                if regex.search(\"[a-zA-Z0-9]\", token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18323029-9a4d-4e7a-a985-c1a485faa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_gold, texts_htr = read_files(data_dir_gold, data_dir_htr[\"CurTSSR26\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0965a-2fa3-4f55-93a8-f59efd85843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_text_list(texts_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff1a1d-3ec2-49d6-80ff-03ff2c318b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_text_list(texts_htr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2004c-803d-4e73-8113-f8d8f2837e39",
   "metadata": {},
   "source": [
    "## 3. Compute character error rates (CER) per text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b3c83-df27-4658-8eaa-80e03bf8018c",
   "metadata": {},
   "source": [
    "The character \"y\" (should be \"ij\") and characters with accents (accents should be removed) are forbidden in the text annotations so we replace them before checking the texts. We also remove punctuation because we do not want to include them in the CER computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1d464-6795-4b68-bf98-c370fea90958",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_text_for_illegal_characters(text):\n",
    "    \"\"\"Check text for non-ascii characters.\"\"\"\n",
    "    for line in text.split(\"\\n\"):\n",
    "        if regex.search(\"[^a-xzA-Z0-9/ ]\", line):\n",
    "            print(f\"illegal character found on line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368a936-4fa2-41a9-8342-43d9d3402097",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_illegal_characters(text):\n",
    "    \"\"\"Replace known non-ascii characters by recommended alternatives.\"\"\"\n",
    "    text = regex.sub(\"y\", \"ij\", text)\n",
    "    text = regex.sub(\"ä\", \"a\", text)\n",
    "    text = regex.sub(\"à\", \"a\", text)\n",
    "    text = regex.sub(\"å\", \"a\", text)\n",
    "    text = regex.sub(\"ç\", \"c\", text)\n",
    "    text = regex.sub(\"é\", \"e\", text)\n",
    "    text = regex.sub(\"è\", \"e\", text)\n",
    "    text = regex.sub(\"ë\", \"e\", text)\n",
    "    text = regex.sub(\"ñ\", \"n\", text)\n",
    "    text = regex.sub(\"ó\", \"o\", text)\n",
    "    text = regex.sub(\"ö\", \"o\", text)\n",
    "    text = regex.sub(\"ø\", \"o\", text)\n",
    "    text = regex.sub(\"ü\", \"u\", text)\n",
    "    text = regex.sub(\"ú\", \"u\", text)\n",
    "    text = regex.sub(\"[?.,!;():'\" + '\"-]', \"\", text)\n",
    "    text = regex.sub(\"\\\\[\", \"\", text)\n",
    "    text = regex.sub(\"\\\\]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d31c03-0a69-4404-abc7-17b361322268",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_illegal_characters_and_check(texts):\n",
    "    \"\"\"Remove non-ascii characters and check texts for other characters.\"\"\"\n",
    "    for texts_index in texts:\n",
    "        texts[texts_index] = remove_illegal_characters(texts[texts_index])\n",
    "        check_text_for_illegal_characters(texts[texts_index])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51226b-baf4-4046-a42e-889378d4ba63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_old_cer_string(cer, old_cer_dict, file_name, guessed_years):\n",
    "    \"\"\"Intentionally left blank.\"\"\"\n",
    "    old_cer_string = \"\"\n",
    "    if file_name in guessed_years:\n",
    "        old_cer_string = (str(guessed_years[file_name]) +\n",
    "                          check_guessed_year_order(guessed_years, file_name) +\n",
    "                          \" \")\n",
    "    if file_name in old_cer_dict and old_cer_dict[file_name] != cer:\n",
    "        cer_diff = round(cer - old_cer_dict[file_name], 1)\n",
    "        cer_sign = \"+\" if cer_diff > 0 else \"\"\n",
    "        old_cer_string += f\"({cer_sign}{cer_diff})\"\n",
    "    return old_cer_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd5bcd-ead3-4a52-a37c-04f8de789cd2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_guessed_year_order(guessed_years, file_name_target):\n",
    "    \"\"\"Check if correct document year is in list of guessed years.\"\"\"\n",
    "    last_year = 0\n",
    "    for file_name in sorted(guessed_years.keys()):\n",
    "        if file_name == file_name_target:\n",
    "            if last_year <= guessed_years[file_name_target]:\n",
    "                return \" \"\n",
    "            else:\n",
    "                return \"!\"\n",
    "        last_year = guessed_years[file_name]\n",
    "    print(\"check_guessed_year_order: cannot happen\")\n",
    "    return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105c443-d99a-4a2c-8eef-0a3d1b02670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer_per_text(texts_htr,\n",
    "                         texts_gold,\n",
    "                         old_cer_dict={},\n",
    "                         guessed_years={}):\n",
    "    \"\"\"Compute character error rate per text for texts in input list.\"\"\"\n",
    "    total_cer = 0\n",
    "    total_chars = 0\n",
    "    cer_dict = {\"average_cer\": 100}\n",
    "    texts_gold_clean = remove_illegal_characters_and_check(texts_gold)\n",
    "    texts_htr_clean = remove_illegal_characters_and_check(texts_htr)\n",
    "    for file_name in sorted(texts_htr_clean.keys()):\n",
    "        cer = round(fastwer.score_sent(texts_htr_clean[file_name],\n",
    "                                       texts_gold_clean[file_name],\n",
    "                                       char_level=True), 1)\n",
    "        max_text_length = max(len(texts_htr_clean[file_name]),\n",
    "                              len(texts_gold_clean[file_name]))\n",
    "        total_chars += max_text_length\n",
    "        total_cer += max_text_length * cer\n",
    "        old_cer_string = make_old_cer_string(cer,\n",
    "                                             old_cer_dict,\n",
    "                                             file_name,\n",
    "                                             guessed_years)\n",
    "        cer_dict[file_name] = cer\n",
    "    cer_dict[\"average_cer\"] = round(total_cer/total_chars, 1)\n",
    "    old_cer_string = make_old_cer_string(cer_dict[\"average_cer\"],\n",
    "                                         old_cer_dict,\n",
    "                                         \"average_cer\",\n",
    "                                         guessed_years)\n",
    "    return cer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef57fdc-2b6d-4ee8-b904-b4e56911a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "for experiment_name in sorted(data_dir_htr.keys(),\n",
    "                              key=lambda x: int(regex.sub(\"dpi\", \"\",\n",
    "                                                regex.sub(\"_NB\", \"\",\n",
    "                                                regex.sub(\"CurTSSR\", \"\", x))))):\n",
    "    texts_gold, texts_htr = read_files(data_dir_gold,\n",
    "                                       data_dir_htr[experiment_name])\n",
    "    cer_dict = compute_cer_per_text(texts_htr, texts_gold)\n",
    "    print(f\"{experiment_name:>9}: cer = {cer_dict['average_cer']}\")\n",
    "    results_dict[experiment_name] = cer_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a2be02-134f-49cf-9d9b-5c508d68f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_dict).T.to_csv(\"cers_per_experiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab0a3a-9d30-488f-a23b-62db0b3286a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Character error rates for Loghi region tests:\")\n",
    "pd.DataFrame([{\"dataset\": \"CBAD\",\n",
    "               \"order\": [0, 1, 2, 3, 6, 7, 5, 4],\n",
    "               \"#order\": 8,\n",
    "               \"cer\": 59},\n",
    "              {\"dataset\": \"general\",\n",
    "               \"order\": [0, 6, 1, 2, 3, 5, 4],\n",
    "               \"#order\": 7,\n",
    "               \"cer\": 22},\n",
    "              {\"dataset\": \"republic\",\n",
    "               \"order\": [0, 10, 1, 2, 9, 3, 4, 5, 8, 6, 7],\n",
    "               \"#order\": 11,\n",
    "               \"cer\": 38},\n",
    "              {\"dataset\": \"republicprint\",\n",
    "               \"order\": [0, 32, 27, 1, 23, 12, 7, 16, 10, 2, 3, 4, 5, 6, 8, 9,\n",
    "                         11, 13, 14, 15, 17, 18, 19, 20, 21, 24, 25, 26, 28,\n",
    "                         29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41],\n",
    "               \"#order\": 42,\n",
    "               \"cer\": 62}])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f6e6922-b413-4859-b78e-86524100efd0",
   "metadata": {},
   "source": [
    "dutchess\n",
    " 3.6  1879-1e-042 -\n",
    " 0.5  1879-1e-142 -\n",
    " 3.7  1879-1e-242 -\n",
    " 2.1  1879-3e-046 -\n",
    " 6.1  1880-1e-045 -\n",
    "14.0  1880-1e-146 -\n",
    " 3.2  1880-1e-247 -\n",
    " 5.3  1880-3e-003 -\n",
    " 3.1  1880-5e-012 - \n",
    " 2.2  1881-3e-002 +\n",
    "average cer: 4.4\n",
    "\n",
    "new_ducthess\n",
    " 6.6  1879-1e-042  \n",
    " 3.4  1879-1e-142  \n",
    " 5.4  1879-1e-242  \n",
    " 2.5  1879-3e-046  \n",
    " 8.0  1880-1e-045  \n",
    "14.1  1880-1e-146  \n",
    " 5.7  1880-1e-247  \n",
    "14.0  1880-3e-003  \n",
    " 6.2  1880-5e-012  \n",
    " 1.7  1881-3e-002  \n",
    "average cer: 6.8\n",
    "\n",
    "bestmodel\n",
    " 3.7  1879-1e-042 - \n",
    " 4.1  1879-1e-142 +\n",
    " 3.4  1879-1e-242 -\n",
    " 5.7  1879-3e-046 +\n",
    " 8.5  1880-1e-045 +\n",
    " 4.0  1880-1e-146 -\n",
    " 6.0  1880-1e-247 +\n",
    " 9.3  1880-3e-003 -\n",
    " 5.2  1880-5e-012 -\n",
    " 6.1  1881-3e-002 +\n",
    "average cer: 5.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b04f7-3159-4f37-a294-5e2fe2c0a1c2",
   "metadata": {},
   "source": [
    "## 4. Correct printed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc000a3-2fe7-44ab-a300-fefd4cfada23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_century(tokens, tokens_index, text_id):\n",
    "    \"\"\" check if the current token is part of a century; return the value \"\"\"\n",
    "    centuries = {\"zeventienhonderd\": 1700,\n",
    "                 \"achttienhonderd\": 1800,\n",
    "                 \"negentienhonderd\": 1900}\n",
    "    if tokens[tokens_index] == \"honderd\" and tokens_index > 0:\n",
    "        if tokens[tokens_index - 1] in [\"zeven\", \"zeventien\"]:\n",
    "            year = 1700\n",
    "        elif tokens[tokens_index - 1] in [\"acht\", \"achttien\"]:\n",
    "            year = 1800\n",
    "        elif tokens[tokens_index - 1] in [\"negen\", \"negentien\"]:\n",
    "            year = 1900\n",
    "        else:\n",
    "            print(f\"unexpected token before \\\"honderd\\\" in file {text_id}:\" +\n",
    "                  \" {tokens[tokens_index - 1]}\")\n",
    "            year = 0\n",
    "    elif tokens[tokens_index] in list(centuries.keys()):\n",
    "        year = centuries[tokens[tokens_index]]\n",
    "    else:\n",
    "        year = 0\n",
    "    return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ee898-872e-42b0-b09b-b08fc81d2241",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def guess_year_from_text(text, text_id):\n",
    "    \"\"\" find all years in the text and return the first; otherwise return 0 \"\"\"\n",
    "    years = []\n",
    "    numbers = {\"een\": 1, \"twee\": 2, \"drie\": 3, \"vier\": 4, \"vijf\": 5,\n",
    "               \"zes\": 6, \"zeven\": 7, \"acht\": 8, \"negen\": 9, \"tien\": 10,\n",
    "               \"elf\": 11, \"twaalf\": 12, \"dertien\": 13, \"veertien\": 14,\n",
    "               \"vijftien\": 15, \"zestien\": 16, \"zeventien\": 17, \"achttien\": 18,\n",
    "               \"negentien\": 19,\n",
    "               \"twintig\": 20, \"dertig\": 30, \"veertig\": 40, \"vijftig\": 50,\n",
    "               \"zestig\": 60, \"zeventig\": 70, \"tachtig\": 80, \"negentig\": 90}\n",
    "\n",
    "    tokens = text.lower().split()\n",
    "    for tokens_index in range(0, len(tokens)):\n",
    "        year = check_century(tokens, tokens_index, text_id)\n",
    "        if year > 0:\n",
    "            if (tokens_index < len(tokens) - 1 and\n",
    "               tokens[tokens_index + 1] in numbers):\n",
    "                year += numbers[tokens[tokens_index + 1]]\n",
    "            if (tokens_index < len(tokens) - 2 and\n",
    "               tokens[tokens_index + 1] == \"en\" and\n",
    "               tokens[tokens_index + 2] in numbers):\n",
    "                year += numbers[tokens[tokens_index + 2]]\n",
    "            if (tokens_index < len(tokens) - 3 and\n",
    "               tokens[tokens_index + 2] == \"en\" and\n",
    "               tokens[tokens_index + 3] in numbers):\n",
    "                year += numbers[tokens[tokens_index + 3]]\n",
    "            if (tokens_index < len(tokens) - 4 and\n",
    "               tokens[tokens_index + 3] == \"en\" and\n",
    "               tokens[tokens_index + 4] in numbers):\n",
    "                year += numbers[tokens[tokens_index + 4]]\n",
    "            years.append(year)\n",
    "    if len(years) == 0:\n",
    "        return 0\n",
    "    elif len(years) == 1:\n",
    "        return years[0]\n",
    "    else:\n",
    "        if years[0] >= years[1] - 1:\n",
    "            return years[0]\n",
    "        else:\n",
    "            return years[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e884c98-87dd-400b-b531-f4597a5e6a56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def guess_years(texts):\n",
    "    return {file_name: guess_year_from_text(texts_htr[file_name],\n",
    "            file_name) for file_name in texts_htr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663bd4a2-6572-4271-9768-595b3c74f9f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_printed_text_year(text_year):\n",
    "    \"\"\"\n",
    "        find appropriate index of text format\n",
    "        printed_text.PRINTED_TEXT for a certificate\n",
    "    \"\"\"\n",
    "    printed_text_year = list(printed_text.PRINTED_TEXT.keys())[0]\n",
    "    for year in sorted(printed_text.PRINTED_TEXT.keys()):\n",
    "        if year > printed_text_year and text_year >= year:\n",
    "            printed_text_year = year\n",
    "    return printed_text_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d034a80-14d6-4df2-96f7-1b4bf5269ad3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def same_number_of_words(phrase, search_text, positions):\n",
    "    \"\"\"\n",
    "        check if the proposed replacement phrase has\n",
    "        the same number of words as the original\n",
    "    \"\"\"\n",
    "    guessed_phrase = search_text[positions[0].start(): positions[0].end()]\n",
    "    return len(guessed_phrase.split()) == len(phrase.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2113f-3944-48a1-a8ef-c775c60262ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_match(text, phrase, start=0, end=None, level=0, max_diff=3):\n",
    "    \"\"\" find an approximately matching phrase in the text \"\"\"\n",
    "    match = {}\n",
    "    if (phrase.lower() in SKIP_PHRASES_ALWAYS or\n",
    "        (phrase.lower() in SKIP_PHRASES_AT_START and\n",
    "         start == 0 and\n",
    "         end is None)):\n",
    "        return match\n",
    "    search_text = text[start: end]\n",
    "    if len(phrase) > 2 - level:\n",
    "        positions = utils.find_text_patterns(r'\\b' +\n",
    "                                             phrase.lower() +\n",
    "                                             r'\\b', search_text.lower())\n",
    "        if (len(positions) == 1 and\n",
    "            (positions[0][\"start\"] == 0 or\n",
    "             not regex.search(\"[a-z]\",\n",
    "             search_text[positions[0][\"start\"]-1].lower())) and\n",
    "            (positions[0][\"end\"] == len(search_text) or\n",
    "             not regex.search(\"[a-z]\",\n",
    "                              search_text[positions[0][\"end\"]].lower()))):\n",
    "            positions[0][\"label\"] = phrase\n",
    "            match = {\"start\": positions[0][\"start\"] + start,\n",
    "                     \"end\": positions[0][\"end\"] + start,\n",
    "                     \"label\": \"match\",\n",
    "                     \"correct_phrase\": phrase}\n",
    "        elif len(positions) == 0:\n",
    "            character_errors = 0\n",
    "            while len(positions) == 0 and character_errors <= max_diff:\n",
    "                query = (r'\\b' +\n",
    "                         f\"({phrase.lower()})\" +\n",
    "                         \"{\" +\n",
    "                         f\"e<={character_errors}\"+\"}\" +\n",
    "                         r'\\b')\n",
    "                positions = [match for match in\n",
    "                             regex.finditer(query, search_text.lower())]\n",
    "                character_errors += 1\n",
    "            if (len(positions) == 1 and\n",
    "               same_number_of_words(phrase, search_text, positions)):\n",
    "                match = {\"start\": positions[0].start() + start,\n",
    "                         \"end\": positions[0].end() + start,\n",
    "                         \"label\": \"fuzzy_match\",\n",
    "                         \"correct_phrase\": phrase}\n",
    "                if (positions[0].group()[0] == \" \" or\n",
    "                   positions[0].group()[0] == \"\\n\"):\n",
    "                    match[\"start\"] += 1\n",
    "                if (positions[0].group()[-1] == \" \" or\n",
    "                   positions[0].group()[-1] == \"\\n\"):\n",
    "                    match[\"end\"] -= 1\n",
    "                if positions[0].groups()[0] == phrase:\n",
    "                    match[\"label\"] = \"match\"\n",
    "    return match\n",
    "\n",
    "\n",
    "SKIP_PHRASES_AT_START = [\"des jaars een duizend acht honderd\",\n",
    "                         \"op dit eiland\", \"op den\"]\n",
    "SKIP_PHRASES_ALWAYS = [\"middags te\", \"middags\", \"een\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc579fe-2c65-405d-87d7-ec3a91bc5aa8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_phrases_in_text(text, phrases, max_diff=0):\n",
    "    \"\"\" find phrases in text, only return unique matches \"\"\"\n",
    "    entities = []\n",
    "    start = 0\n",
    "    for phrase in phrases:\n",
    "        if len(entities) > 0:\n",
    "            start = get_min_char_pos(entities, len(entities))\n",
    "        if len(phrase) < 6 or phrases.count(phrase) > 1:\n",
    "            entities.append({})\n",
    "        else:\n",
    "            entities.append(find_match(text,\n",
    "                                       phrase, start=start,\n",
    "                                       max_diff=max_diff))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886bd9b-5c37-44ec-b701-17ae2272a077",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_min_char_pos(entities, index):\n",
    "    \"\"\" get the final position of the last preceding phrase with a match \"\"\"\n",
    "    for counter in range(index-1, 0, -1):\n",
    "        if \"end\" in entities[counter]:\n",
    "            return entities[counter][\"end\"] + 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422b217-e292-47d3-b161-da9f425b126f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_max_char_pos(entities, index):\n",
    "    \"\"\" get the first position of the first next phrase with a match \"\"\"\n",
    "    for counter in range(index+1, len(entities)):\n",
    "        if \"start\" in entities[counter]:\n",
    "            return entities[counter][\"start\"]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dbe5d-11b6-4c50-8010-cb963f6259f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_phrases_in_text_with_entities(text, phrases, entities, max_diff=3):\n",
    "    \"\"\" find phrases in text, only return unique matches \"\"\"\n",
    "    if len(entities) != len(phrases):\n",
    "        print(f\"find_phrases_in_text_with_entities: list entities \" +\n",
    "              \"{len(entities)} should have same length as phrases \" +\n",
    "              \"{len(phrases)}\")\n",
    "    for i in range(0, len(phrases)):\n",
    "        if len(entities[i]) == 0:\n",
    "            start = get_min_char_pos(entities, i)\n",
    "            end = get_max_char_pos(entities, i)\n",
    "            if end is None:\n",
    "                entities[i] = find_match(text,\n",
    "                                         phrases[i],\n",
    "                                         start=start,\n",
    "                                         level=1)\n",
    "            else:\n",
    "                entities[i] = find_match(text,\n",
    "                                         phrases[i],\n",
    "                                         start=start,\n",
    "                                         end=end,\n",
    "                                         level=1,\n",
    "                                         max_diff=max_diff)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eba84d-d081-4900-8f52-38ff84fdf401",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def printed_text_split_in_words(printed_text_in, entities_in):\n",
    "    \"\"\" split expected phrase in word before searching word-by-word \"\"\"\n",
    "    printed_text_out = []\n",
    "    entities_out = []\n",
    "    for i in range(0, len(printed_text_in)):\n",
    "        if (len(entities_in[i]) > 0 or\n",
    "           len(printed_text_in[i].split()) == 1 or\n",
    "           printed_text_in[i].lower() in NO_SPLIT_PHRASES):\n",
    "            printed_text_out.append(printed_text_in[i])\n",
    "            entities_out.append(entities_in[i])\n",
    "        else:\n",
    "            # ideally not the first word of a phrase should be linked\n",
    "            # first but the longest\n",
    "            for word in printed_text_in[i].split():\n",
    "                printed_text_out.append(word)\n",
    "                entities_out.append({})\n",
    "    return printed_text_out, entities_out\n",
    "\n",
    "\n",
    "NO_SPLIT_PHRASES = [\"des jaars een duizend acht honderd\",\n",
    "                    \"laatstelijk gewoond\",\n",
    "                    \"niet te kunnen schrijven\",\n",
    "                    \"op dit eiland\",\n",
    "                    \"middags te\",\n",
    "                    \"middags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8877d3-2f9a-4559-a8d6-8500ce6dd79d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sanity_check_entities(entities, text_id):\n",
    "    starts_seen = {}\n",
    "    ends_seen = {}\n",
    "    for entity in entities:\n",
    "        if \"start\" in entity:\n",
    "            if entity[\"start\"] in starts_seen:\n",
    "                utils.print_with_color(f\"duplicate start: \" +\n",
    "                                       \"{entity['start']} for text_id \" +\n",
    "                                       \"{text_id}! {entity}\\n\")\n",
    "            if entity[\"end\"] in ends_seen:\n",
    "                utils.print_with_color(f\"duplicate end: \" +\n",
    "                                       \"{entity['end']} for text_id \" +\n",
    "                                       \"{text_id}! {entity}\\n\")\n",
    "            starts_seen[entity[\"start\"]] = True\n",
    "            ends_seen[entity[\"end\"]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d40c6-dad5-4f9c-acfd-b1c935bb52e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_entities(entities, entity_replaced):\n",
    "    \"\"\" adjust start and end point of entities after replacing a text \"\"\"\n",
    "    delta = (len(entity_replaced[\"correct_phrase\"]) -\n",
    "             (entity_replaced[\"end\"] -\n",
    "              entity_replaced[\"start\"]))\n",
    "    for entity in entities:\n",
    "        if \"start\" in entity and entity[\"start\"] > entity_replaced[\"start\"]:\n",
    "            entity[\"start\"] += delta\n",
    "        if \"end\" in entity and entity[\"end\"] >= entity_replaced[\"end\"]:\n",
    "            entity[\"end\"] += delta\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750039a-da8b-4af2-9ab9-8eb639cdf062",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def correct_text(text_in, entities):\n",
    "    \"\"\" replace fuzzy matches in text by correct phrases \"\"\"\n",
    "    text_out = text_in\n",
    "    for entity in reversed(entities):\n",
    "        if \"label\" in entity and entity[\"label\"] == \"fuzzy_match\":\n",
    "            text_out = (text_out[:entity[\"start\"]] +\n",
    "                        entity[\"correct_phrase\"] +\n",
    "                        text_out[entity[\"end\"]:])\n",
    "            if len(entity[\"correct_phrase\"]) != (entity[\"end\"] -\n",
    "                                                 entity[\"start\"]):\n",
    "                entities = update_entities(entities, entity)\n",
    "    return text_out, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cc125-d3d2-4f1e-9234-25b242178e96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_printed_text_for_illegal_characters(printed_text):\n",
    "    for year in printed_text.PRINTED_TEXT:\n",
    "        text = \"\\n\".join(printed_text.PRINTED_TEXT[year])\n",
    "        check_text_for_illegal_characters(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90ed52-3ce1-4599-9093-008bf19f8af1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def correct_text_list(texts):\n",
    "    corrected_texts = {}\n",
    "    text_entities = {}\n",
    "    guessed_years = {}\n",
    "    for text_id in sorted(texts.keys()):\n",
    "        guessed_years[text_id] = guess_year_from_text(texts[text_id], text_id)\n",
    "        printed_text_year = get_printed_text_year(guessed_years[text_id])\n",
    "        printed_text_text = [remove_illegal_characters(phrase)\n",
    "                             for phrase in\n",
    "                             printed_text.PRINTED_TEXT[printed_text_year]]\n",
    "        entities = find_phrases_in_text(texts[text_id],\n",
    "                                        printed_text_text,\n",
    "                                        max_diff=0)\n",
    "        entities = find_phrases_in_text_with_entities(texts[text_id],\n",
    "                                                      printed_text_text,\n",
    "                                                      entities, max_diff=0)\n",
    "        entities = find_phrases_in_text_with_entities(texts[text_id],\n",
    "                                                      printed_text_text,\n",
    "                                                      entities)\n",
    "        (printed_text_text,\n",
    "         entities) = printed_text_split_in_words(printed_text_text,\n",
    "                                                 entities)\n",
    "        entities = find_phrases_in_text_with_entities(texts[text_id],\n",
    "                                                      printed_text_text,\n",
    "                                                      entities)\n",
    "        entities = find_phrases_in_text_with_entities(texts[text_id],\n",
    "                                                      printed_text_text,\n",
    "                                                      entities)\n",
    "        sanity_check_entities(entities, text_id)\n",
    "        text_entities[text_id] = entities\n",
    "        (corrected_texts[text_id],\n",
    "         entities) = correct_text(texts[text_id],\n",
    "                                  copy.deepcopy(entities))\n",
    "    return corrected_texts, guessed_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ad5eb-6146-4978-87be-0376c0a97a57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def correct_guessed_years(guessed_years, data_dir_gold):\n",
    "    if data_dir_gold.split(\"/\")[2] == \"Sample_test_1\":\n",
    "        file_years = pd.read_csv(\"Sample_test_1_years.csv\")\n",
    "        file_names = list(guessed_years.keys())\n",
    "        for index in range(0, len(file_names)):\n",
    "            if guessed_years[file_names[index]] != file_years.iloc[index][1]:\n",
    "                print(f\"changed year of {file_names[index]} from \" +\n",
    "                      \"{guessed_years[file_names[index]]} \" +\n",
    "                      \"to {file_years.iloc[index][1]}\")\n",
    "                guessed_years[file_names[index]] = file_years.iloc[index][1]\n",
    "    return guessed_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90babfea-c72e-44df-bd86-e2059b218f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_texts, guessed_years = correct_text_list(texts_htr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb2700-2006-454a-bd0f-dbccbcb12517",
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_years = correct_guessed_years(guessed_years, data_dir_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124ba4b-a311-4eb8-a0ff-71f2017d266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_dict_corrected = compute_cer_per_text(corrected_texts,\n",
    "                                          texts_gold,\n",
    "                                          cer_dict,\n",
    "                                          guessed_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d25ae-f45e-4178-865c-6b6467c8732c",
   "metadata": {},
   "source": [
    "Average CER after correction of printed text (5.7%) was higher than before correction (5.5%)! How can this have happened?\n",
    "1. gold data might be incorrect: yes, it contains y, ç, é, ë, ó (**fixed**)\n",
    "2. printed text targets might be incorrect: yes, it contained ç (**fixed**)\n",
    "3. punctuation is included in CER computation (**fixed**)\n",
    "4. printed text \"middags te\" be comes \"namiddags te\" with written text and is corrected back (**fixed**)\n",
    "6. these old validation test files are probably in the training data of the new model: not many printed text errors left to be corrected\n",
    "7. **validation test file p010.xml is a birth certificate**\n",
    "8. p040.xml: strike-through in printed text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "936e0426-e5d2-49fe-a4b0-322929988b6e",
   "metadata": {},
   "source": [
    "largest CER in Sample_test_1\n",
    "14.4 p053.xml: strike-throughs + margin text + still-born\n",
    "12.9 p045.xml: strike-throughs + margin text\n",
    "11.0 p060.xml: strike-throughs + margin text + still-born\n",
    "10.1 p048.xml: strike-throughs + margin text\n",
    " 8.6 p069.xml: margin text + gold annotation errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d6c13-152d-4fab-a6e7-5ebfb8b154b8",
   "metadata": {},
   "source": [
    "## 5. Find incorrect words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d071b58-cedb-417d-94e2-deff3bb11856",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_best_line_match(lines_htr,\n",
    "                         lines_gold,\n",
    "                         index_htr,\n",
    "                         gold_index_used,\n",
    "                         alignments):\n",
    "    best_index_gold = -1\n",
    "    best_cer = 100\n",
    "    line_htr = lines_htr[index_htr]\n",
    "    index_htr_old = -1\n",
    "    for index_gold in range(0, len(lines_gold)):\n",
    "        cer = fastwer.score_sent(\n",
    "            remove_illegal_characters(line_htr),\n",
    "            remove_illegal_characters(lines_gold[index_gold]),\n",
    "            char_level=True)\n",
    "        if cer < best_cer and (index_gold not in gold_index_used.keys() or\n",
    "                               cer < gold_index_used[index_gold][0]):\n",
    "            best_cer = cer\n",
    "            best_index_gold = index_gold\n",
    "            if index_gold in gold_index_used:\n",
    "                index_htr_old = gold_index_used[index_gold][1]\n",
    "            gold_index_used[index_gold] = [best_cer, index_htr]\n",
    "            if index_htr_old >= 0:\n",
    "                (best_index_gold_old,\n",
    "                 best_cer_old,\n",
    "                 gold_index_used,\n",
    "                 alignments) = find_best_line_match(lines_htr,\n",
    "                                                    lines_gold,\n",
    "                                                    index_htr_old,\n",
    "                                                    gold_index_used,\n",
    "                                                    alignments)\n",
    "                alignments[index_htr_old] = [best_index_gold_old, best_cer_old]\n",
    "    alignments[index_htr] = [best_index_gold, best_cer]\n",
    "    return best_index_gold, best_cer, gold_index_used, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b2428-cc50-46d3-809f-a21043eaec60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def align_lines(lines_htr, lines_gold):\n",
    "    alignments = {}\n",
    "    gold_index_used = {}\n",
    "    for index_htr in range(0, len(lines_htr)):\n",
    "        (best_index_gold,\n",
    "         best_cer,\n",
    "         gold_index_used,\n",
    "         alignments) = find_best_line_match(lines_htr,\n",
    "                                            lines_gold,\n",
    "                                            index_htr,\n",
    "                                            gold_index_used,\n",
    "                                            alignments)\n",
    "    alignments = convert_alignments(alignments)\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ab03a-6c23-410f-8fa2-0ce7310e531e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def count_distances(alignments):\n",
    "    return Counter([alignment[1] - alignment[0] for alignment in alignments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58adad-8589-4fa2-a5d3-25966283bf49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_alignments_order(alignments, lines_htr, lines_gold):\n",
    "    last_gold_index = -1\n",
    "    to_be_deleted = []\n",
    "    distances = count_distances(alignments)\n",
    "    for alignment_index in range(0, len(alignments)):\n",
    "        if last_gold_index < alignments[alignment_index][1]:\n",
    "            last_gold_index = alignments[alignment_index][1]\n",
    "        elif (alignments[alignment_index][2] >\n",
    "              alignments[alignment_index - 1][2]):\n",
    "            to_be_deleted.append(alignment_index)\n",
    "        elif (alignments[alignment_index][2] <\n",
    "              alignments[alignment_index - 1][2]):\n",
    "            to_be_deleted.append(alignment_index - 1)\n",
    "            last_gold_index = alignments[alignment_index][1]\n",
    "        elif (distances[alignments[alignment_index][1] -\n",
    "              alignments[alignment_index][0]] <\n",
    "              distances[alignments[alignment_index - 1][1] -\n",
    "              alignments[alignment_index - 1][0]]):\n",
    "            to_be_deleted.append(alignment_index)\n",
    "        else:\n",
    "            to_be_deleted.append(alignment_index - 1)\n",
    "            last_gold_index = alignments[alignment_index][1]\n",
    "    for to_be_deleted_value in list(reversed(to_be_deleted)):\n",
    "        alignments.pop(to_be_deleted_value)\n",
    "    to_be_added = []\n",
    "    for alignment_index in range(1, len(alignments)):\n",
    "        if ((alignments[alignment_index][0] -\n",
    "             alignments[alignment_index - 1][0] ==\n",
    "             alignments[alignment_index][1] -\n",
    "             alignments[alignment_index - 1][1]) and\n",
    "            alignments[alignment_index][0] -\n",
    "            alignments[alignment_index - 1][0] != 1):\n",
    "            range_end = (alignments[alignment_index][0] -\n",
    "                         alignments[alignment_index - 1][0])\n",
    "            for alignment_index_delta in range(1, range_end):\n",
    "                to_be_added.append((alignment_index,\n",
    "                                    alignments[alignment_index - 1][0] +\n",
    "                                    alignment_index_delta,\n",
    "                                    alignments[alignment_index - 1][1] +\n",
    "                                    alignment_index_delta))\n",
    "    for to_be_added_element in list(reversed(to_be_added)):\n",
    "        e1clean = remove_illegal_characters(lines_htr[to_be_added_element[1]])\n",
    "        e2clean = remove_illegal_characters(lines_gold[to_be_added_element[2]])\n",
    "        alignments.insert(to_be_added_element[0],\n",
    "                          (to_be_added_element[1],\n",
    "                           to_be_added_element[2],\n",
    "                           fastwer.score_sent(\n",
    "                           e1clean,\n",
    "                           e2clean,\n",
    "                           char_level=True)))\n",
    "    return len(to_be_deleted) > 0 or len(to_be_added) > 0, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14f106-3e1f-4a3e-b01d-bea0287a58ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_alignments_order_wrapper(alignments, lines_htr, lines_gold):\n",
    "    alignments_changed = True\n",
    "    while alignments_changed:\n",
    "        alignments_changed, alignments = check_alignments_order(alignments,\n",
    "                                                                lines_htr,\n",
    "                                                                lines_gold)\n",
    "    return alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc73f3-9392-4cdc-8824-df1f73eed356",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fix_split_words(words_htr, words_gold, wrong_words, missed_words):\n",
    "    to_be_deleted = []\n",
    "    for index_wrong in range(1, len(wrong_words)):\n",
    "        if wrong_words[index_wrong] == wrong_words[index_wrong - 1] + 1:\n",
    "            combined_word = (words_htr[wrong_words[index_wrong - 1]] +\n",
    "                             words_htr[wrong_words[index_wrong]])\n",
    "            for index_missed in range(0, len(missed_words)):\n",
    "                if words_gold[missed_words[index_missed]] == combined_word:\n",
    "                    to_be_deleted.append((index_wrong - 1,\n",
    "                                          index_wrong,\n",
    "                                          index_missed))\n",
    "                    break\n",
    "    for to_be_deleted_item in list(reversed(to_be_deleted)):\n",
    "        for to_be_deleted_wrong_index in range(to_be_deleted_item[1],\n",
    "                                               to_be_deleted_item[0] - 1,\n",
    "                                               -1):\n",
    "            wrong_words.pop(to_be_deleted_wrong_index)\n",
    "        missed_words.pop(to_be_deleted_item[2])\n",
    "    return wrong_words, missed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ac023-2a9f-4396-9336-6b5235ffca59",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_words(line_htr, line_gold, lines_htr, lines_gold):\n",
    "    missed_words = []\n",
    "    wrong_words = []\n",
    "    if line_htr.lower() != line_gold.lower():\n",
    "        words_htr = line_htr.lower().split()\n",
    "        words_gold = line_gold.lower().split()\n",
    "        alignments = align_lines(words_htr, words_gold)\n",
    "        alignments = check_alignments_order_wrapper(alignments,\n",
    "                                                    lines_htr,\n",
    "                                                    lines_gold)\n",
    "        index_htr = 0\n",
    "        index_gold = 0\n",
    "        for index_alignment in range(0, len(alignments)):\n",
    "            target_index_htr = alignments[index_alignment][0]\n",
    "            target_index_gold = alignments[index_alignment][1]\n",
    "            while (index_htr < len(words_htr) and\n",
    "                   index_htr < target_index_htr):\n",
    "                wrong_words.append(index_htr)\n",
    "                index_htr += 1\n",
    "            while (index_gold < len(words_gold) and\n",
    "                   index_gold < target_index_gold):\n",
    "                missed_words.append(index_gold)\n",
    "                index_gold += 1\n",
    "            if words_htr[target_index_htr] != words_gold[target_index_gold]:\n",
    "                missed_words.append(target_index_gold)\n",
    "                wrong_words.append(index_htr)\n",
    "            index_htr += 1\n",
    "            index_gold += 1\n",
    "        for index_htr_extra in range(index_htr, len(words_htr)):\n",
    "            wrong_words.append(index_htr)\n",
    "        for index_gold_extra in range(index_gold, len(words_gold)):\n",
    "            missed_words.append(index_gold_extra)\n",
    "        wrong_words, missed_words = fix_split_words(words_htr,\n",
    "                                                    words_gold,\n",
    "                                                    wrong_words,\n",
    "                                                    missed_words)\n",
    "    return wrong_words, missed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135a760-fc74-4940-901c-4a6f7c7fd376",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_lines(lines_htr, lines_gold, alignments):\n",
    "    index_htr = 0\n",
    "    line_analysis = []\n",
    "    for alignment in alignments:\n",
    "        for index_htr_delta in range(1, alignment[0]-index_htr):\n",
    "            line_analysis.append(analyze_words(lines_htr[index_htr +\n",
    "                                               index_htr_delta],\n",
    "                                               \"\",\n",
    "                                               lines_htr,\n",
    "                                               lines_gold))\n",
    "        line_analysis.append(analyze_words(lines_htr[alignment[0]],\n",
    "                                           lines_gold[alignment[1]],\n",
    "                                           lines_htr, lines_gold))\n",
    "        index_htr = alignment[0] + 1\n",
    "    for index_htr_delta in range(1, len(lines_htr)-index_htr):\n",
    "        line_analysis.append(analyze_words(lines_htr[index_htr +\n",
    "                                           index_htr_delta],\n",
    "                                           \"\",\n",
    "                                           lines_htr,\n",
    "                                           lines_gold))\n",
    "    return line_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5d291-a70d-4a73-860d-d26902c46666",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_word_analysis(line_htr, line_gold, line_analysis_line):\n",
    "    words_htr = line_htr.split()\n",
    "    words_gold = line_gold.split()\n",
    "    for index_htr in range(0, len(words_htr)):\n",
    "        if index_htr in line_analysis_line[0]:\n",
    "            read_transkribus_files.print_with_color(words_htr[index_htr],\n",
    "                                                    color_code=1,\n",
    "                                                    end=\" \")\n",
    "        else:\n",
    "            read_transkribus_files.print_with_color(words_htr[index_htr],\n",
    "                                                    color_code=0,\n",
    "                                                    end=\" \")\n",
    "    if len(line_analysis_line[1]) > 0:\n",
    "        read_transkribus_files.print_with_color([words_gold[index_gold]\n",
    "                                                for index_gold in\n",
    "                                                line_analysis_line[1]],\n",
    "                                                color_code=4,\n",
    "                                                end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2112768-e7ac-4e22-8198-d788214da11a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_line_analysis(lines_htr, lines_gold, alignments, line_analysis):\n",
    "    index_htr = 0\n",
    "    for alignment in alignments:\n",
    "        for index_htr_extra in range(index_htr, alignment[0]):\n",
    "            show_word_analysis(lines_htr[index_htr_extra], \"\", [[], []])\n",
    "        show_word_analysis(lines_htr[alignment[0]],\n",
    "                           lines_gold[alignment[1]],\n",
    "                           line_analysis[alignment[0]])\n",
    "        index_htr = alignment[0] + 1\n",
    "    for index_htr_extra in range(index_htr, len(lines_htr)):\n",
    "        show_word_analysis(lines_htr[index_htr_extra], \"\", [[], []])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61919a1b-925f-4ef7-9653-c5e6bec42725",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_alignments(alignments):\n",
    "    alignments_converted = []\n",
    "    for alignments_key in alignments.keys():\n",
    "        alignments_converted.append((alignments_key,\n",
    "                                     alignments[alignments_key][0],\n",
    "                                     alignments[alignments_key][1]))\n",
    "    return alignments_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4af8d0-be80-41d0-8bad-cc6bb8ebfee6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_text(texts_htr, texts_gold, file_name):\n",
    "    lines_htr = remove_illegal_characters(texts_htr[file_name]).split(\"\\n\")\n",
    "    lines_gold = remove_illegal_characters(texts_gold[file_name]).split(\"\\n\")\n",
    "    alignments = align_lines(lines_htr, lines_gold)\n",
    "    cer = fastwer.score_sent(\"\\n\".join(lines_htr),\n",
    "                             \"\\n\".join(lines_gold),\n",
    "                             char_level=True)\n",
    "    read_transkribus_files.print_with_color(f\"{file_name} (cer={cer:.1f}):\\n\",\n",
    "                                            color_code=4)\n",
    "    line_analysis = analyze_lines(lines_htr, lines_gold, alignments)\n",
    "    show_line_analysis(lines_htr, lines_gold, alignments, line_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69504699-89eb-4108-995b-b008ad6b228b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compare_texts(texts_htr, texts_gold):\n",
    "    for file_name in sorted(texts_htr.keys()):\n",
    "        compare_text(texts_htr, texts_gold, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d5a229-a6ac-43ac-918e-3afa20ca35e4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for file_id in texts_htr.keys():\n",
    "    compare_text(texts_htr, texts_gold, file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a58ffb-7def-45bc-844a-34bbffd1f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_file = \"0043_p037.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27e55f-4942-4faf-ac59-7c38d13728d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_text(corrected_texts, texts_gold, selected_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3eeaa-3c53-4b4a-9d23-76c918665946",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_text(corrected_texts, texts_htr, selected_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79244b-6b12-48a6-a736-09feb680e540",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(selected_file)\n",
    "texts_htr[selected_file] == corrected_texts[selected_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f894d2-43f5-45c4-a444-c850bb65b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_texts[selected_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d46fa-83f4-4393-9e59-d36a9524ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_htr[selected_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86886e2b-1204-4d7d-8672-cecda9f517c5",
   "metadata": {},
   "source": [
    "## 6. Named entity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5599136-3d3b-440e-8122-16f8a74d3449",
   "metadata": {},
   "source": [
    "Install Dutch Spacy model `nl_core_news_sm` with: `python -m spacy download nl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68c52b-9a44-4b99-a05f-166f0b0d57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b4e6f-23df-4209-a013-b6d9b6eda2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3775cc-ce42-4b00-a462-bfc4a64ff1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_positions(tokens):\n",
    "    return [{\"start\": token.idx,\n",
    "             \"end\": token.idx + len(token)}\n",
    "            for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c16c4d-f62c-4416-b44d-668e1cee8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_entities(analysis):\n",
    "    token_positions = get_token_positions([token for token in analysis])\n",
    "    return [{\"start\": token_positions[entity.start][\"start\"],\n",
    "             \"end\": token_positions[entity.end - 1][\"end\"],\n",
    "             \"label\": str(entity.label_)}\n",
    "            for entity in analysis.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030f6c6-3295-4e5f-af45-7b7997406da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_entities(entities_in, entities_lower_cased, text):\n",
    "    entities_out = [entity for entity in entities_in\n",
    "                    if entity[\"label\"] == \"PERSON\"]\n",
    "    to_be_deleted = []\n",
    "    for index in range(0, len(entities_out)):\n",
    "        for entity in entities_lower_cased:\n",
    "            if (entities_out[index][\"start\"] == entity[\"start\"] and\n",
    "                entities_out[index][\"end\"] == entity[\"end\"]):\n",
    "                to_be_deleted.append(index)\n",
    "                break\n",
    "    for index in range(len(to_be_deleted) - 1, -1, -1):\n",
    "        entities_out.pop(to_be_deleted[index])\n",
    "    return entities_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a03de1-35a0-47d1-bf96-2c70b1a7fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_token(text, position):\n",
    "    index = position\n",
    "    while index > 0 and regex.search(\"\\\\s\", text[index - 1]):\n",
    "        index -= 1\n",
    "    previous_token = \"\"\n",
    "    while index > 0 and regex.search(\"\\\\S\", text[index - 1]):\n",
    "        index -= 1\n",
    "        previous_token = text[index] + previous_token\n",
    "    return previous_token, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7568f708-69c2-43e4-9a37-266140c69adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_token(text, position):\n",
    "    index = position\n",
    "    while (index < len(text) and\n",
    "           (regex.search(\"\\\\s\", text[index]) or\n",
    "            text[index] == \",\")):\n",
    "        index += 1\n",
    "    next_token = \"\"\n",
    "    while index < len(text) and regex.search(\"\\\\S\", text[index]):\n",
    "        next_token += text[index]\n",
    "        index += 1\n",
    "    return next_token, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20b07d-9224-4b24-8fc2-9221ef7ee250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_entities(entities, text):\n",
    "    for entity in entities:\n",
    "        previous_token, previous_start = get_previous_token(text,\n",
    "                                                            entity[\"start\"])\n",
    "        while (regex.search(\"^[A-Z]\", previous_token) and\n",
    "               previous_token not in SKIP_TOKENS):\n",
    "            entity[\"start\"] = previous_start\n",
    "            (previous_token,\n",
    "             previous_start) = get_previous_token(text, entity[\"start\"])\n",
    "        next_token, next_end = get_next_token(text, entity[\"end\"])\n",
    "        while (regex.search(\"^[A-Z]\", next_token) and\n",
    "               next_token not in SKIP_TOKENS):\n",
    "            entity[\"end\"] = next_end\n",
    "            next_token, next_end = get_next_token(text, entity[\"end\"])\n",
    "    return entities\n",
    "\n",
    "\n",
    "SKIP_TOKENS = [\"Oud\", \"En\", \"Een\", \"Twee\", \"Drie\", \"Vier\", \"Vijf\", \"Zes\",\n",
    "               \"Zeven\", \"Acht\", \"Negen\", \"Tien\", \"in\",\n",
    "               \"Ongehuwd\", \"Aanteekeningen.\", \"Aanteekeningen\",\n",
    "               \"Verbeteringen.\", \"Hospitaal\", \"Compareerden\", \"De\", \"Sep\",\n",
    "               \"No.\", \"Nr.\", \"Fol.\", \"Werk\", \"Heden\", \"Waarvan\", \"Jaars\",\n",
    "               \"des\", \"January\", \"Januari\", \"February\", \"Februari\",\n",
    "               \"Maart\", \"April\", \"Mei\", \"Juni\", \"Juli\", \"July\", \"Augustus\",\n",
    "               \"September\", \"October\", \"November\", \"December\",\n",
    "               \"Zeventienden\",  \"Zestienden\", \"Curaçao\", \"Curacao\", \"Habana\",\n",
    "               \"Achthonderd\", \"Negenhonderd\", \"en\",\n",
    "               \"Twintig\", \"Dertig\", \"Veertig\", \"Vijftig\", \"Zestig\",\n",
    "               \"Zeventig\", \"Tachtig\", \"Negentig\", \"Honderd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438562c-df36-4b2a-8150-53d66fa45aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_entities(entities, text):\n",
    "    for entity in entities:\n",
    "        final_token, next_end = get_previous_token(text, entity[\"end\"])\n",
    "        while (regex.search(\"^[a-z]\", final_token) or\n",
    "               final_token in SKIP_TOKENS or\n",
    "               regex.search(\"[0-9]\", final_token)):\n",
    "            entity[\"end\"] = next_end\n",
    "            final_token, next_end = get_previous_token(text, entity[\"end\"])\n",
    "        first_token, next_start = get_next_token(text, entity[\"start\"])\n",
    "        while (regex.search(\"^[a-z]\", first_token) or\n",
    "               first_token in SKIP_TOKENS or\n",
    "               regex.search(\"[0-9]\", first_token)):\n",
    "            entity[\"start\"] = next_start\n",
    "            first_token, next_start = get_next_token(text, entity[\"start\"])\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4d7fff-f06f-4d18-84b2-1239546778da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_entities(entities):\n",
    "    to_be_deleted = []\n",
    "    for index in range(0, len(entities)):\n",
    "        if entities[index][\"end\"] <= entities[index][\"start\"]:\n",
    "            to_be_deleted.append(index)\n",
    "    to_be_deleted.reverse()\n",
    "    for index in to_be_deleted:\n",
    "        entities.pop(index)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b138ee-2931-40f5-9565-94de1792afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_entities(entities):\n",
    "    to_be_deleted = []\n",
    "    seen = []\n",
    "    for index in range(0, len(entities)):\n",
    "        if len([True\n",
    "                for index2 in range(0, len(entities))\n",
    "                if (index2 != index and\n",
    "                    entities[index2][\"start\"] >=\n",
    "                    entities[index][\"start\"] - 1 and\n",
    "                    entities[index2][\"start\"] <=\n",
    "                    entities[index][\"start\"] + 1 and\n",
    "                    entities[index2][\"end\"] >=\n",
    "                    entities[index][\"end\"] - 2 and\n",
    "                    entities[index2][\"end\"] <=\n",
    "                    entities[index][\"end\"] + 2)]) > 0:\n",
    "            to_be_deleted.append(index)\n",
    "        seen.append((entities[index][\"start\"], entities[index][\"end\"]))\n",
    "    to_be_deleted.reverse()\n",
    "    for index in to_be_deleted:\n",
    "        entities.pop(index)\n",
    "    for index in range(0, len(entities)):\n",
    "        if len([True\n",
    "                for index2 in range(0, len(entities))\n",
    "                if (index2 != index and\n",
    "                    entities[index2][\"start\"] <= entities[index][\"start\"] and\n",
    "                    entities[index2][\"end\"] >= entities[index][\"start\"])]) > 0:\n",
    "            print(f\"warning: overlapping entities [1] {index}\")\n",
    "        elif len([True\n",
    "                  for index2 in range(0, len(entities))\n",
    "                  if (index2 != index and\n",
    "                      entities[index2][\"start\"] <= entities[index][\"end\"] and\n",
    "                      entities[index2][\"end\"] >= entities[index][\"end\"])]) > 0:\n",
    "            print(f\"warning: overlapping entities [2] {index}\")\n",
    "        elif len([True\n",
    "                  for index2 in range(0, len(entities))\n",
    "                  if (index2 != index and\n",
    "                      entities[index2][\"start\"] >= entities[index][\"start\"] and\n",
    "                      entities[index2][\"end\"] <= entities[index][\"end\"])]) > 0:\n",
    "            print(f\"warning: overlapping entities [3] {index}\")\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3567e-2c8e-491c-b729-4d0ccbb338e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_name_triggers(entities, text):\n",
    "    for name_trigger_text in NAME_TRIGGER_TEXTS:\n",
    "        result = regex.search(name_trigger_text, text, regex.IGNORECASE)\n",
    "        if result is not None and result.span()[1] >= 0:\n",
    "            next_token, next_end = get_next_token(text, result.span()[1])\n",
    "            if regex.search(\"^[A-Z]\", next_token):\n",
    "                token_start = next_end - len(next_token)\n",
    "                token_used = len([entity\n",
    "                                  for entity in entities\n",
    "                                  if entity[\"start\"] == token_start]) > 0\n",
    "                if not token_used:\n",
    "                    entities.extend(expand_entities([{\"start\": token_start,\n",
    "                                                      \"end\": next_end,\n",
    "                                                      \"label\": \"PERSON\"}],\n",
    "                                                    text))\n",
    "    return entities\n",
    "\n",
    "\n",
    "NAME_TRIGGER_TEXTS = [\"De personen van\", \"overleden is:\",\n",
    "                      \"alhier is overleden\", \"stand op Curaçao\",\n",
    "                      \"stand op Curacao\", \"zoon van\", \"dochter van\",\n",
    "                      \"gehuwd met\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ae811-3461-4d8f-b671-bb4cf1fd670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_entities(entities, text):\n",
    "    entities = remove_empty_entities(entities)\n",
    "    entities = check_name_triggers(entities, text)\n",
    "    for name in NAMES:\n",
    "        result = regex.search(name, text)\n",
    "        if result is not None and result.span()[0] >= 0:\n",
    "            token_start = result.span()[0]\n",
    "            token_end = result.span()[1]\n",
    "            token_used = len([entity\n",
    "                              for entity in entities\n",
    "                              if (entity[\"start\"] == token_start or\n",
    "                                  entity[\"end\"] == token_end)]) > 0\n",
    "            if not token_used:\n",
    "                entities.extend(expand_entities([{\"start\": token_start,\n",
    "                                                  \"end\": token_end,\n",
    "                                                  \"label\": \"PERSON\"}],\n",
    "                                                text))\n",
    "    entities = remove_duplicate_entities(entities)\n",
    "    return entities\n",
    "\n",
    "\n",
    "NAMES = [\"Hermanus\", \"Jacob\", \"Theodorus\", \"José\", \"Maria\", \"Eduard\",\n",
    "         \"Cardino\", \"Anton\", \"Martino\", \"Modest\", \"Martina\", \"Johannes\",\n",
    "         \"Pedro\", \"Adelaida\", \"Luis\", \"Lindoro\", \"Manuel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4f8ce-5a18-4ec0-b3a9-81a99370f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities(texts_gold, texts_htr, entities_dict,\n",
    "                      htr_model, debug=False):\n",
    "    total_length = 0\n",
    "    total_cer = 0\n",
    "    found = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = []\n",
    "    for file_name in texts_gold.keys():\n",
    "        for entity in entities_dict[file_name]:\n",
    "            gold_phrase = texts_gold[file_name][entity[\"start\"]:entity[\"end\"]]\n",
    "            match = find_match(texts_htr[file_name], gold_phrase, max_diff=3)\n",
    "            if not match:\n",
    "                cer = 100  # 16 / len(gold_phrase)\n",
    "                results.append([htr_model, file_name, cer, gold_phrase, \"\"])\n",
    "            else:\n",
    "                gold_phrase = regex.sub(\"\\n\", \" \", gold_phrase)\n",
    "                htr_phrase = regex.sub(\"\\n\",\n",
    "                                       \" \",\n",
    "                                       texts_htr[file_name][match[\"start\"]:\n",
    "                                                            match[\"end\"]])\n",
    "                cer = round(fastwer.score_sent(htr_phrase,\n",
    "                                               gold_phrase,\n",
    "                                               char_level=True), 1)\n",
    "                found += 1\n",
    "                if match[\"label\"] == \"match\":\n",
    "                    correct += 1\n",
    "                    if debug:\n",
    "                        print(f\"CORRECT: {gold_phrase}\")\n",
    "                elif debug:\n",
    "                    print(f\"PARTIAL: {gold_phrase} # {htr_phrase}\")\n",
    "                results.append([htr_model, file_name, cer,\n",
    "                                gold_phrase, htr_phrase])\n",
    "            total_length += len(gold_phrase)\n",
    "            total_cer += cer * len(gold_phrase)\n",
    "            total += 1\n",
    "    average_cer = round(total_cer/total_length, 1)\n",
    "    print(f\"correct: {int(100*found/total)}%; \" +\n",
    "          f\" partial {int(100*(found-correct)/total)}%\")\n",
    "    print(f\"average cer: {average_cer}%\")\n",
    "    return average_cer, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc3086-ae5b-484b-8ee4-5aa11209c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_entities_length(entities_dict):\n",
    "    total_length = 0\n",
    "    nbr_of_entities = 0\n",
    "    for file_name in entities_dict:\n",
    "        for entity in entities_dict[file_name]:\n",
    "            nbr_of_entities += 1\n",
    "            total_length += entity[\"end\"] - entity[\"start\"]\n",
    "    return round(total_length / nbr_of_entities, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe380132-00b3-4952-af47-b1ba8b45e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_entities(entities):\n",
    "    if len(entities) <= 1:\n",
    "        return entities\n",
    "    first_entity = entities.pop(0)\n",
    "    entities_out = sort_entities(entities)\n",
    "    for index in range(0, len(entities_out)):\n",
    "        if (entities_out[index][\"start\"] > first_entity[\"start\"] or\n",
    "            (entities_out[index][\"start\"] == first_entity[\"start\"] and\n",
    "             entities_out[index][\"end\"] > first_entity[\"end\"])):\n",
    "            entities_out.insert(first_entity, index)\n",
    "            break\n",
    "    if len(entities_out) == len(entities):\n",
    "        entities_out.append(first_entity)\n",
    "    return entities_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bdbf2-a48f-4d68-a67f-36c77d33d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text(entities, text):\n",
    "    for entity in entities:\n",
    "        entity[\"text\"] = text[entity[\"start\"]:entity[\"end\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851dafb-d180-4d44-8706-d2d43e623704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entities_with_newlines(entities):\n",
    "    return [entity\n",
    "            for entity in entities\n",
    "            if not regex.search(r\"\\n\", entity[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a0adf-c741-46c1-9422-b123df64508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entities(texts_gold):\n",
    "    entities_dict = {}\n",
    "    for file_name in texts_gold.keys():\n",
    "        text = texts_gold[file_name]\n",
    "        analysis_lower_cased = nlp(text.lower())\n",
    "        entities_lower_cased = get_spacy_entities(analysis_lower_cased)\n",
    "        analysis = nlp(text)\n",
    "        entities = get_spacy_entities(analysis)\n",
    "        entities = select_entities(entities, entities_lower_cased, text)\n",
    "        entities = expand_entities(entities, text)\n",
    "        entities = shrink_entities(entities, text)\n",
    "        entities = sanity_check_entities(entities, text)\n",
    "        entities = add_text(entities, text)\n",
    "        entities = remove_entities_with_newlines(entities)\n",
    "        entities_dict[file_name] = entities\n",
    "    return entities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c731af-320d-4e63-983b-0be6a8bbad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cers_per_name = []\n",
    "cers_per_experiment = {}\n",
    "for htr_model in sorted(data_dir_htr.keys(),\n",
    "                        key=lambda x: int(regex.sub(\"_NB\", \"\", regex.sub(\"CurTSSR\", \"\", x)))):\n",
    "    print(colored(f\"{htr_model}\", attrs=['bold']))\n",
    "    texts_gold, texts_htr = read_files(data_dir_gold, data_dir_htr[htr_model])\n",
    "    entities_dict = find_entities(texts_gold)\n",
    "    average_cer, results_per_model = evaluate_entities(texts_gold,\n",
    "                                                       texts_htr,\n",
    "                                                       entities_dict,\n",
    "                                                       htr_model)\n",
    "    cers_per_name.extend(results_per_model)\n",
    "    cers_per_experiment[htr_model] = average_cer\n",
    "pd.DataFrame(cers_per_name, columns=[\"experiment_name\",\n",
    "                                     \"file_name\",\n",
    "                                     \"cer\",\n",
    "                                     \"gold_phrase\",\n",
    "                                     \"htr_phrase\"]).to_csv(\"cers_per_name.csv\", index=False)\n",
    "pd.DataFrame(cers_per_experiment,\n",
    "             index=[0]).T.to_csv(\"cers_names_per_experiment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21f111-a016-4ea0-8623-41f5dea6e3f1",
   "metadata": {},
   "source": [
    "**Evaluation of name accuracy (all person names including signatures):**\n",
    "\n",
    "| System | Model |    Correct | Partially Correct |\n",
    "| ------ | ----- | ---------- | ----------------- |\n",
    "| Transkribus | HTR_Curacao_bestModel | 70% | 38% |\n",
    "| Transkribus | Curacao_Dutchess      | 75% | 39% |\n",
    "| Loghi       | general               | 55% | 38% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032303c6-b220-4de3-9087-f623721f1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_entities_length(entities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e36e3-2073-41bd-87b2-40cc6397ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_text(texts_gold[\"0040_p033.xml\"], entities_dict[\"0040_p033.xml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3946429-41f2-4ec7-b50f-55290fc55921",
   "metadata": {},
   "source": [
    "## 7. Find professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58705c-2b18-4420-8419-29e1d71a51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(entities_dict):\n",
    "    entities_count = {}\n",
    "    for file_name in entities_dict.keys():\n",
    "        for entity in entities_dict[file_name]:\n",
    "            if entity[\"label\"] in entities_count:\n",
    "                entities_count[entity[\"label\"]] += 1\n",
    "            else:\n",
    "                entities_count[entity[\"label\"]] = 1\n",
    "    return entities_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610300b-2822-4f98-9373-9fa6eb65bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_professions(texts):\n",
    "    entities = {}\n",
    "    for file_name in texts.keys():\n",
    "        entities[file_name] = []\n",
    "        for profession in PROFESSIONS_LIST:\n",
    "            for match in regex.finditer(profession,\n",
    "                                        regex.sub(\"\\n\",\n",
    "                                                  \" \",\n",
    "                                                  texts[file_name]),\n",
    "                                        regex.IGNORECASE):\n",
    "                entities[file_name].append({\"start\": match.start(),\n",
    "                                            \"end\": match.end(),\n",
    "                                            \"label\": profession})\n",
    "    return entities\n",
    "\n",
    "\n",
    "PROFESSIONS_LIST = [\"aanspreker\",  \"agent van politie\", \"agent\", \"beroep geen\",\n",
    "                    \"bode\", \"boekbinder\", \"broodbakster\", \"broodbak ster\",\n",
    "                    \"chauffeur\", \"geemploijeerde\", \"geemployeerde\",\n",
    "                    \"gemploijeerde\", \"goudsmid\", \"gouvernementsambtenaar\",\n",
    "                    \"gouvernements ambtenaar\", \"handelaar\", \"hoedenmaakster\",\n",
    "                    \"hoedenmaak ster\", \"hoedenvlechtster\", \"kantonrechter\",\n",
    "                    \"kleermaker\", \"kleer maker\", \"koopman\", \"koopvrouw\",\n",
    "                    \"kuiper\", \"kui per\", \"landbouwer\", \"magazijnsknecht\",\n",
    "                    \"metselaar\", \"naaister\", \"onbekend van beroep\",\n",
    "                    \"opzichter\", \"pontvoerder\", \"pottenbaakster\",\n",
    "                    \"schoenmaker\", \"schoen maker\", \"schilder\",\n",
    "                    \"schrijnwerker\", \"sjouwer\", \"stoker\", \"timmerman\",\n",
    "                    \"veldarbeider\", \"veldwachter\", \"visscher\", \"waschvrouw\",\n",
    "                    \"wasch vrouw\", \"werkman\", \"werk man\", \"werkvrouw\",\n",
    "                    \"werk vrouw\", \"werktuigkundige\", \"zeeman\",\n",
    "                    \"zonder beroep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04717d5-8bb6-4638-8199-13b9fae69902",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_htr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b13e05-8286-42cd-b865-0b29d65ca50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for htr_model in data_dir_htr.keys():\n",
    "    print(colored(f\"{htr_model}\", attrs=['bold']))\n",
    "    texts_gold, texts_htr = read_files(data_dir_gold, data_dir_htr[htr_model])\n",
    "    entities_dict = find_professions(texts_gold)\n",
    "    evaluate_entities(texts_gold, texts_htr, entities_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d6bb6-1515-4d71-9849-f3c80bd826ed",
   "metadata": {},
   "source": [
    "**Evaluation of profession accuracy**:\n",
    "\n",
    "| System | Model |    Correct | Partially Correct |\n",
    "| ------ | ----- | ---------- | ----------------- |\n",
    "| Transkribus | HTR_Curacao_bestModel | 78% | 11% |\n",
    "| Transkribus | Curacao_Dutchess      | 80% |  8% |\n",
    "| Loghi       | general               | 52% | 25% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12590e5-ff1c-428a-973b-998ad338f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_entities_length(entities_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdd0fa-6317-4377-b482-b33a2dda8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in entities_dict:\n",
    "    print(file_name, len(entities_dict[file_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d511610-f91f-4eb1-a050-bdbf2e8bb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_text(texts_gold[\"0040_p033.xml\"], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad594f-b618-4284-9f2e-d84c8ee88d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in texts_gold.keys():\n",
    "    text = texts_gold[file_name]\n",
    "    for match in regex.finditer(\"beroep\", text, regex.IGNORECASE):\n",
    "        profession_start = match.span()[1]\n",
    "        while (profession_start < len(text) and\n",
    "               regex.search(\"\\\\s\", text[profession_start])):\n",
    "            profession_start += 1\n",
    "        profession_end = profession_start\n",
    "        while (profession_end < len(text) and\n",
    "               regex.search(\"\\\\S\", text[profession_end])):\n",
    "            profession_end += 1\n",
    "        profession = text[profession_start: profession_end].lower()\n",
    "        if profession not in PROFESSIONS_LIST:\n",
    "            print(profession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e0575-1f57-4e7e-82e7-d2dbeac0ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in texts_gold.keys():\n",
    "    if regex.search(\"kui\", texts_gold[file_name], regex.IGNORECASE):\n",
    "        print(file_name, texts_gold[file_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828ec13-2769-4229-bef4-8c7070db4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec89a2-291f-4af3-8a1e-d80ccb4306e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
