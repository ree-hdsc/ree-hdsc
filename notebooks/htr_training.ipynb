{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b488d16-7562-4dbf-be6a-a8c10ded2365",
   "metadata": {},
   "source": [
    "# HTR Training\n",
    "\n",
    "Combine different sets of documents, train HTR on Transkribus and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299d22e-ab21-4322-9013-3a4d2e51c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex\n",
    "import shutil\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "from scripts.read_transkribus_files import get_text_from_file\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c2a5c-8236-4ace-86ec-9b39b89e58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRS = [ \"tmp/1586842/Training_set\",\n",
    "              \"tmp/1586854/Validation_set\",\n",
    "              \"tmp/1609526/Training_set_2\",\n",
    "              \"tmp/1609530/Validation_set_2\",\n",
    "              \"tmp/1616742/Sample_three-column\" ]\n",
    "IGNORE_DIRS = [ \"tmp/1616742/Sample_three-column\" ]\n",
    "\n",
    "YEAR_FILES = [ os.path.basename(file_name) + \"_years.csv\" for file_name in DATA_DIRS ]\n",
    "USAGE_FILES = [ os.path.basename(file_name) + \"_usage.csv\" for file_name in DATA_DIRS ]\n",
    "NAME_FILES = [ os.path.basename(file_name) + \"_names.csv\" for file_name in DATA_DIRS ]\n",
    "\n",
    "MAX_WIDTH = 1000\n",
    "MAX_HEIGHT = 750\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a6e9d-384f-4ce4-a7fb-17687e466915",
   "metadata": {},
   "source": [
    "## 1. Find years available training data\n",
    "\n",
    "Some of the names of the scans in the training data have changed from the central repository names. Here we check the scans, specify the year of the scan and store these in year files (`_years.csv`) for each data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f639e80-1043-4ffd-8c37-68ef0aaadd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_years_of_files(data_dirs=DATA_DIRS, year_files=YEAR_FILES):\n",
    "    for data_dir_id in range(0, len(data_dirs)):\n",
    "        last_year = \"\"\n",
    "        file_counter = 0\n",
    "        data_dir = data_dirs[data_dir_id]\n",
    "        year_file = year_files[data_dir_id]\n",
    "        file_names = sorted(os.listdir(os.path.join(data_dir, \"page\")))\n",
    "        try:\n",
    "            years = list(pd.read_csv(year_file, index_col=0)[\"0\"])\n",
    "        except:\n",
    "            years = []\n",
    "        for file_name in file_names:\n",
    "            file_counter += 1\n",
    "            if file_counter > len(years):\n",
    "                try:\n",
    "                    display(Image.open(os.path.join(data_dir, regex.sub(\".xml\", \".jpg\", file_name))))\n",
    "                except:\n",
    "                    display(Image.open(os.path.join(data_dir, regex.sub(\".xml\", \".JPG\", file_name))))\n",
    "                print(f\"data_dir: {data_dir}; file name: {file_name}; last year: {last_year};\", end=\" \")\n",
    "                print(f\"file: {file_counter}/{len(file_names)}\")\n",
    "                year = input().strip()\n",
    "                if year == \"\":\n",
    "                    year = last_year\n",
    "                years.append(year)\n",
    "                pd.DataFrame(years).to_csv(year_file)\n",
    "                last_year = year\n",
    "                clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69274431-874e-4dae-9ae7-b7b88676e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years(year_files=YEAR_FILES):\n",
    "    all_years = []\n",
    "    for file_name in year_files:\n",
    "        years = pd.read_csv(file_name, index_col=0)\n",
    "        all_years.extend(years[\"0\"])\n",
    "    return all_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9582a6-75b5-4ccd-8fab-dd0dee48d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_years_of_files(data_dirs=[ \"tmp/1616639/Sample_test_1\" ], year_files=[ \"Sample_test_1_years.csv\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83c7b0-121f-4007-92c6-a44aae912977",
   "metadata": {},
   "source": [
    "## 2. Determine file usability\n",
    "\n",
    "Some scans contain parts of multiple certificates or damaged certficates. We want to exclude these from HTR training and layout training. We label them and store the labels in usage files (`_usage.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bcbcc-aafe-46bb-82f2-1453bfa862ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_usability(data_dirs=DATA_DIRS, usage_files=USAGE_FILES):\n",
    "    for data_dir_id in range(0, len(data_dirs)):\n",
    "        file_counter = 0\n",
    "        data_dir = data_dirs[data_dir_id]\n",
    "        usage_file = usage_files[data_dir_id]\n",
    "        file_names = sorted(os.listdir(os.path.join(data_dir, \"page\")))\n",
    "        try:\n",
    "            usage = list(pd.read_csv(usage_file, index_col=0)[\"0\"])\n",
    "        except:\n",
    "            usage = []\n",
    "        for file_name in file_names:\n",
    "            file_counter += 1\n",
    "            if file_counter > len(usage):\n",
    "                try:\n",
    "                    display(Image.open(os.path.join(data_dir, regex.sub(\".xml\", \".jpg\", file_name))))\n",
    "                except:\n",
    "                    display(Image.open(os.path.join(data_dir, regex.sub(\".xml\", \".JPG\", file_name))))\n",
    "                print(f\"data_dir: {data_dir}; file name: {file_name};\", end=\" \")\n",
    "                print(f\"file: {file_counter}/{len(file_names)}\")\n",
    "                usage_value = input().strip()\n",
    "                if usage_value == \"\":\n",
    "                    usage_value = \"yes\"\n",
    "                else:\n",
    "                    usage_value = \"no\"\n",
    "                usage.append(usage_value)\n",
    "                pd.DataFrame(usage).to_csv(usage_file)\n",
    "                clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47583933-1ecc-45ea-a5ec-6b1630495c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_file_usability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e04a2-187f-46a1-b2b8-567740737d9b",
   "metadata": {},
   "source": [
    "## 3. Plot file counts per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d0e35-755b-4334-a4de-f1fb162b0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year2decade(year):\n",
    "    return int(regex.sub(\".$\", \"\", str(year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557393c-579c-4028-a6c5-eeaabdf9af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decades(year_files=YEAR_FILES):\n",
    "    return [ year2decade(year) for year in get_years(year_files=YEAR_FILES) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a2c92-8fec-4f70-ae17-d9911b28fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_usage(usage_files=USAGE_FILES):\n",
    "    usage = []\n",
    "    for file_name in usage_files:\n",
    "        usage.extend(list(pd.read_csv(file_name, index_col=0)[\"0\"]))\n",
    "    return usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a625e-f4fa-428f-9751-200a0c3d95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_file_counts(file_counts):\n",
    "    x = [ x[0] for x in list(file_counts.index) ]\n",
    "    y = list(file_counts.values)\n",
    "    plt.xticks(ticks=x, labels=[ str(x_value) + \"0\" for x_value in x ])\n",
    "    plt.title(f\"Number of selected scans per decade (total={sum(file_counts.values)})\")\n",
    "    plt.bar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e1cdd-9e3f-47c6-a088-79c7167b72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_OF_SKIPPED_FILES = 50 # while annotation is not finished\n",
    "\n",
    "decades = get_decades()[:-NBR_OF_SKIPPED_FILES]\n",
    "usage = get_file_usage()[:-NBR_OF_SKIPPED_FILES]\n",
    "decades = [ decades[i] for i in range(0, len(usage)) if usage[i] == \"yes\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ef1ce-a524-449b-bc8c-9126e9b57e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_counts(pd.DataFrame(decades).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a089b1-7c15-4321-82e5-bdc9fea97b3e",
   "metadata": {},
   "source": [
    "## 4. Select complementary scans (unfinished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd60aca-7332-4678-be25-8173f9f29bfd",
   "metadata": {},
   "source": [
    "Plan:\n",
    "\n",
    "1. find years\n",
    "2. remove difficult cases\n",
    "3. fill up decades to 15\n",
    "4. sort by name\n",
    "5. train layout and baseline detection\n",
    "6. train htr, check CER\n",
    "7. evaluate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd82827-a0ce-4e1e-845a-8c2b9589c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR = \"../../data/Overlijden\"\n",
    "\n",
    "def random_select_scan(year):\n",
    "    year_dir = os.path.join(FILE_DIR, \"O.R. \" + str(year))\n",
    "    file_names = []\n",
    "    for region_dir in os.listdir(year_dir):\n",
    "        file_names.extend([ os.path.join(region_dir, file_name) for file_name in os.listdir(os.path.join(year_dir, region_dir)) ])\n",
    "    return file_names[random.randint(0, len(file_names) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b72f33-7697-4dc2-8bce-b499f4318fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years_from_dir(dir_name):\n",
    "    years = []\n",
    "    for file_name in os.listdir(dir_name):\n",
    "        file_name_parts = file_name.split()\n",
    "        years.append(int(file_name_parts[1]))\n",
    "    return years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99e5c1-8b78-456a-a05b-eb88355ee5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = get_years()[:-NBR_OF_SKIPPED_FILES]\n",
    "years = [ years[i] for i in range(0, len(usage)) if usage[i] == \"yes\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad9c41-8e0d-4b49-bfeb-6cbe5b79f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = os.path.join(FILE_DIR, \"x-samples\", \"complementary-2023\")\n",
    "\n",
    "target_dir_years = get_years_from_dir(TARGET_DIR)\n",
    "for decade_start in [ 1860, 1870, 1910, 1920, 1940]:\n",
    "    available_years = sorted(set([ year for year in years if year >= decade_start and year < decade_start + 10 ]))\n",
    "    for year in range(decade_start, decade_start + 10):\n",
    "        while year not in available_years and year not in target_dir_years:\n",
    "            file_name = os.path.join(FILE_DIR, \"O.R. \" + str(year), random_select_scan(year))\n",
    "            display(Image.open(file_name))\n",
    "            print(\"accept this file?\")\n",
    "            accept_value = input().strip()\n",
    "            if accept_value == \"\":\n",
    "                shutil.copy(file_name, TARGET_DIR)\n",
    "                available_years.append(year)\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40545a9-5129-4a11-94a0-e887581087a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_select_scan(1871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b21f8-4edc-45f8-824e-63da39bcfc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, meta_data, file_regions = get_text_from_file(os.path.join(DATA_DIRS[0], \"page\", \"p001.xml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce026b7b-bc36-41a7-8f34-9dad6fce49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "saw_trigger_word = False\n",
    "for line in  text.split(\"\\n\"):\n",
    "    if saw_trigger_word:\n",
    "        print(line)\n",
    "        break\n",
    "    if regex.search(\"^Heden\", line):\n",
    "        saw_trigger_word = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636b7f1-f520-4abc-8253-d72e98e70d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07fb8add-60c6-48fd-8e05-6108c64cfbfd",
   "metadata": {},
   "source": [
    "## 5. Determine archive file names\n",
    "\n",
    "Find the archive file names of the scans in the training data. We use the years stored in the years files and ask the user for the folio number. Then the matching scans from the archive are shown and the user can choose one by entering the matching id number. Choice \"0\" here will enable the user to change the provided id number. The resulting archive file names are stored in name files per collection (`_names.csv`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c789eda-f828-4ec7-ac7d-d71902a66dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHIVE_FILE_DIR = \"../../data/Overlijden\"\n",
    "\n",
    "def check_archive_file_name(archive_file_name):\n",
    "    file_name_parts = archive_file_name.split()\n",
    "    year_dir = \" \".join(file_name_parts[:2])\n",
    "    region_dir = regex.sub(\"\\.$\", \"\", \" \".join(file_name_parts[:-1]))\n",
    "    file_name_with_dirs = os.path.join(ARCHIVE_FILE_DIR, year_dir, region_dir, archive_file_name)\n",
    "    return os.path.isfile(file_name_with_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ee18fd-c2fe-4787-a8da-38715ea96faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_images(year, folio_number):\n",
    "    year_dir = f\"O.R. {year}\"\n",
    "    folio_number = str(folio_number).zfill(3)\n",
    "    candidate_images = []\n",
    "    for region_dir in os.listdir(os.path.join(ARCHIVE_FILE_DIR, year_dir)):\n",
    "        if regex.search(year_dir, region_dir):\n",
    "            for image_file_name in os.listdir(os.path.join(ARCHIVE_FILE_DIR, year_dir, region_dir)):\n",
    "                if regex.search(f\"{folio_number}\\.jpg$\", image_file_name, regex.IGNORECASE):\n",
    "                    candidate_images.append(os.path.join(ARCHIVE_FILE_DIR, year_dir, region_dir, image_file_name))\n",
    "    return candidate_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18723b37-6063-43c9-829e-d26e2fbb5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_archive_file_names(data_dirs=DATA_DIRS, name_files=NAME_FILES, year_files=YEAR_FILES):\n",
    "    for data_dir_id in range(0, len(data_dirs)):\n",
    "        file_counter = 0\n",
    "        data_dir = data_dirs[data_dir_id]\n",
    "        name_file = name_files[data_dir_id]\n",
    "        year_file = year_files[data_dir_id]\n",
    "        file_names = sorted(os.listdir(os.path.join(data_dir, \"page\")))\n",
    "        years = list(pd.read_csv(year_file, index_col=0)[\"0\"])\n",
    "        try:\n",
    "            archive_file_names = list(pd.read_csv(name_file, index_col=0)[\"0\"])\n",
    "        except:\n",
    "            archive_file_names = []\n",
    "        for file_name in file_names:\n",
    "            year = years[file_counter]\n",
    "            file_counter += 1\n",
    "            while file_counter > len(archive_file_names):\n",
    "                clear_output(wait=True)\n",
    "                try:\n",
    "                    display(Image.open(os.path.join(data_dir, \n",
    "                                                    regex.sub(\".xml\", \".jpg\", file_name))).resize((MAX_WIDTH, MAX_HEIGHT)))\n",
    "                except:\n",
    "                    display(Image.open(os.path.join(data_dir, \n",
    "                                                    regex.sub(\".xml\", \".JPG\", file_name))).resize((MAX_WIDTH, MAX_HEIGHT)))\n",
    "                print(f\"data_dir: {data_dir}; file name: {file_name}; year: {year}\", end=\" \")\n",
    "                print(f\"file: {file_counter}/{len(file_names)}\")\n",
    "                print(\"Enter folio number\")\n",
    "                folio_number = input().strip()\n",
    "                candidate_images = get_candidate_images(year, folio_number)\n",
    "                for index in range(0, len(candidate_images)):\n",
    "                    #display(index+1, Image.open(candidate_images[index]).resize((0.5 * MAX_WIDTH, 0.5 * MAX_HEIGHT)))\n",
    "                    display(index+1, Image.open(candidate_images[index]).resize((500,500)))\n",
    "                print(f\"Enter id number of scan (1-{len(candidate_images)})\")\n",
    "                chosen_index = int(input().strip()) - 1\n",
    "                if chosen_index >= 0:\n",
    "                    archive_file_name = os.path.basename(candidate_images[chosen_index])\n",
    "                    archive_file_names.append(archive_file_name)\n",
    "                    pd.DataFrame(archive_file_names).to_csv(name_file)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2d591-0b44-406f-92fc-c4bf15c54ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_archive_file_names(data_dirs=[ \"tmp/1616639/Sample_test_1\" ], \n",
    "                             name_files=[ \"Sample_test_1_names.csv\" ], \n",
    "                             year_files=[ \"Sample_test_1_years.csv\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5fe9b-07b0-4d11-b2d7-e02f39f16b45",
   "metadata": {},
   "source": [
    "## 6. Check archive file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae000dbf-1e8e-4b9a-a4ab-355cf5a9aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_directory_name_to_archive_file_name(archive_file_name):\n",
    "    file_name_parts = archive_file_name.split()\n",
    "    year_dir = \" \".join(file_name_parts[:2])\n",
    "    region_dir = regex.sub(\"\\.$\", \"\", \" \".join(file_name_parts[:-1]))\n",
    "    file_name_with_dirs = os.path.join(ARCHIVE_FILE_DIR, year_dir, region_dir, archive_file_name)\n",
    "    return file_name_with_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eaec63-3d47-4ead-b740-22a396427f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_archive_file_names(data_dirs=DATA_DIRS, name_files=NAME_FILES, start_data_dir = \"\", start_file_name = \"\"):\n",
    "    for data_dir_id in range(0, len(data_dirs)):\n",
    "        file_counter = 0\n",
    "        data_dir = data_dirs[data_dir_id]\n",
    "        if start_data_dir != \"\" and data_dir != start_data_dir:\n",
    "            continue\n",
    "        start_data_dir = \"\"\n",
    "        name_file = name_files[data_dir_id]\n",
    "        file_names = sorted(os.listdir(os.path.join(data_dir, \"page\")))\n",
    "        archive_file_names = list(pd.read_csv(name_file, index_col=0)[\"0\"])\n",
    "        for file_name in file_names:\n",
    "            archive_file_name = str(archive_file_names[file_counter])\n",
    "            archive_file_name_with_dir = add_directory_name_to_archive_file_name(archive_file_name)\n",
    "            file_counter += 1\n",
    "            if start_file_name != \"\" and file_name != start_file_name:\n",
    "                continue\n",
    "            start_file_name = \"\"\n",
    "            clear_output(wait=True)\n",
    "            try:\n",
    "                file_name_with_dir = os.path.join(data_dir, regex.sub(\".xml\", \".jpg\", file_name))\n",
    "                display(Image.open(file_name_with_dir).resize((int(0.4 * MAX_WIDTH), int(0.4 * MAX_HEIGHT))))\n",
    "            except:\n",
    "                file_name_with_dir = os.path.join(data_dir, regex.sub(\".xml\", \".JPG\", file_name))\n",
    "                display(Image.open(file_name_with_dir).resize((int(0.4 * MAX_WIDTH), int(0.4 * MAX_HEIGHT))))\n",
    "            print(f\"file name: {data_dir}/{file_name}; size: {os.path.getsize(file_name_with_dir)}; file: {file_counter}/{len(file_names)}\")\n",
    "            if archive_file_name != \"nan\":\n",
    "                display(Image.open(archive_file_name_with_dir).resize((int(0.4 * MAX_WIDTH), int(0.4 * MAX_HEIGHT))))\n",
    "                print(f\"archive file name: {archive_file_name}; size: {os.path.getsize(archive_file_name_with_dir)}\")\n",
    "            response = input().strip()\n",
    "            if response == \"0\":\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95811ae-2f1f-4c5e-80f2-2ddba71dec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_archive_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364fc6c-5c8c-4901-b386-8daf0933eaa6",
   "metadata": {},
   "source": [
    "## 7. Make image file name link table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df1800-7f0c-4a70-a861-fcff8f0a82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_file_link_table(data_dirs=DATA_DIRS, name_files=NAME_FILES):\n",
    "    table_dict =  { \"collectie\": [], \"bestandsnaam\": [], \"archiefnaam\": [], \"opmerking\": [] }\n",
    "    for data_dir_id in range(0, len(data_dirs)):\n",
    "        data_dir = data_dirs[data_dir_id]\n",
    "        if data_dir not in IGNORE_DIRS:\n",
    "            name_file = name_files[data_dir_id]\n",
    "            file_names = sorted(os.listdir(os.path.join(data_dir, \"page\")))\n",
    "            archive_file_names = list(pd.read_csv(name_file, index_col=0)[\"0\"])\n",
    "            file_counter = 0\n",
    "            for file_name in file_names:\n",
    "                archive_file_name = str(archive_file_names[file_counter])\n",
    "                table_dict[\"collectie\"].append(os.path.basename(data_dir))\n",
    "                table_dict[\"bestandsnaam\"].append(file_name)\n",
    "                table_dict[\"archiefnaam\"].append(archive_file_name)\n",
    "                table_dict[\"opmerking\"].append(\"\")\n",
    "                file_counter += 1\n",
    "    return pd.DataFrame(table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07004903-0823-42fe-b1c1-91b608168a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_table = make_image_file_link_table()\n",
    "link_table.to_csv(\"koppeltabel_HTR_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d4403-3a8f-4432-ac83-2a888cd6692a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
