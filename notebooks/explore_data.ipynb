{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2bc757-f218-4c26-b6c6-2454b8dc007b",
   "metadata": {},
   "source": [
    "# Explore manually annotated Cura√ßao files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0e29a-453f-4a4e-9171-e554d72128e3",
   "metadata": {},
   "source": [
    "## 1. Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f712aeb-f9cd-4776-8985-d7ad699d7e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6753339-cc90-42c7-a7c5-7d64dd8d53ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_dir = \"../../data/Training_set_V2/\"\n",
    "data_dir = \"../../data/Sample_regex/Sample_regex/page/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc9148d-d272-40b2-8612-23c3b7610c2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_from_file(file_name):\n",
    "    tree = ET.parse(file_name)\n",
    "    root = tree.getroot()\n",
    "    return get_text_from_xml(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542a5e2-6db4-4c4b-9b12-80c4a4368506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_from_xml(root):\n",
    "    text = \"\"\n",
    "    for textline in root.findall(\".//{*}TextLine\"):\n",
    "        custom_dict = make_custom_dict(textline.attrib)\n",
    "        for unicode in textline.findall(\"./{*}TextEquiv/{*}Unicode\"):\n",
    "            text += remove_strikethroughs(unicode.text, custom_dict) + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74eb62-a95c-441c-b3cd-f8202ed6df8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_custom_dict(text_line_attributes):\n",
    "    if \"custom\" not in text_line_attributes:\n",
    "        return {}\n",
    "    custom_tokens = text_line_attributes[\"custom\"].split()\n",
    "    custom_dict = {}\n",
    "    while custom_tokens:\n",
    "        custom_key = custom_tokens.pop(0)\n",
    "        custom_value = custom_tokens.pop(0)\n",
    "        while custom_tokens and not re.search(\"}$\", custom_value):\n",
    "            custom_value += \" \" + custom_tokens.pop(0)\n",
    "        if custom_key in custom_dict:\n",
    "            custom_dict[custom_key].append(ast.literal_eval(json_string_add_quotes(custom_value)))\n",
    "        else:\n",
    "            custom_dict[custom_key] = [ast.literal_eval(json_string_add_quotes(custom_value))]\n",
    "    return custom_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f21338-20d2-4f83-a4e7-66a48fd1d772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_strikethroughs(text_line, custom_dict):\n",
    "    if \"textStyle\" not in custom_dict:\n",
    "        return text_line\n",
    "    chars = list(text_line)\n",
    "    for strikethrough in custom_dict[\"textStyle\"]:\n",
    "        if \"strikethrough\" in strikethrough:\n",
    "            start = int(strikethrough[\"offset\"])\n",
    "            for i in range(start, start + int(strikethrough[\"length\"])):\n",
    "                chars[i] = \" \"\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a307ac6-4f4c-4f1a-82ba-f01498516d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def json_string_add_quotes(string):\n",
    "    return re.sub(\"{ *\", \"{ '\", \n",
    "               re.sub(\": *\", \"': '\", \n",
    "                   re.sub(\"; *\", \"', '\",\n",
    "                       re.sub(\"} *'\", \"} \",\n",
    "                           re.sub(\"; *}\", \"' }\", string)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb7abc-b1bb-48d6-9719-33b52723d81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_file_name(file_id):\n",
    "    return \"p\" + str(file_id).zfill(3) + \".xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98c4ed-69d9-4937-8e0a-9052694b2f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_with_color(string, color_code=1):\n",
    "    print(f\"\\x1b[3{color_code}m{string}\\x1b[m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a43456-ec67-4c0e-99d0-2a4a34c945fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    texts = {}\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if re.search(\"\\.xml$\", file_name):\n",
    "            file_id = int(re.sub(\"\\D\", \"\", file_name))\n",
    "            try:\n",
    "                texts[file_id] = get_text_from_file(os.path.join(data_dir, file_name))\n",
    "            except:\n",
    "                print_with_color(f\"error processing file {file_id}\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bd915-fe8d-4fc0-835c-69ad9bd09e61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = read_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f2ef2-d628-4876-bfd1-ded589cc8fac",
   "metadata": {},
   "source": [
    "## 2. Find entities in texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb715fe-d3af-49e7-a296-cd2121027068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a58dd-4ff9-4295-9441-de917473ef4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_names(entities):\n",
    "    name = \"\"\n",
    "    for part in entities:\n",
    "        if re.search(\"^B\", part[\"entity\"]) and name != \"\":\n",
    "            print(name)\n",
    "            name = \"\"\n",
    "        if re.search(\"(GPE|PERSON)$\", part[\"entity\"]):\n",
    "            if name != \"\":\n",
    "                name += \" \"\n",
    "            name += part[\"word\"]\n",
    "    if name != \"\":\n",
    "        print(name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50688d0-2fed-43f5-a5ad-1828a3e45c6b",
   "metadata": {},
   "source": [
    "Tested models (initial number indicates monthly downloads):\n",
    "* (345) wietsedv/bert-base-dutch-cased-finetuned-conll2002-ner (several false positives)\n",
    "* (74) Matthijsvanhof/bert-base-dutch-cased-finetuned-NER (not useful, tags everything)\n",
    "* (16) wietsedv/bert-base-dutch-cased-finetuned-sonar-ner (some false positives)\n",
    "* (13) proycon/bert-ner-cased-conll2002-nld (did not find any entities)\n",
    "* (10) proycon/bert-ner-cased-sonar1-nld (found only one entity)\n",
    "* (10) Matthijsvanhof/bert-base-dutch-cased-finetuned-NER8 (not useful, tags everything)\n",
    "* (4) [wietsedv/bert-base-dutch-cased-finetuned-udlassy-ner](https://huggingface.co/wietsedv/bert-base-dutch-cased-finetuned-udlassy-ner) (few false positives) **SELECTED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0ba95-6d64-401b-aebb-b49a23874413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_bert_pipeline = transformers.pipeline(task='ner', model='wietsedv/bert-base-dutch-cased-finetuned-udlassy-ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d26c1-b6a9-4b0a-a48b-a3cd743b7f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities = run_bert_pipeline(texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1c588-1596-457f-8149-c821fbe94f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_names(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603d2cf-c1db-4406-b378-c08e8837712d",
   "metadata": {},
   "source": [
    "## 3. Visualize entities\n",
    "\n",
    "For list of entity tags of model `wietsedv/bert-base-dutch-cased-finetuned-udlassy-ner`, see [OntoNotes](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf), page 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115f52c-a345-4bf5-ac6f-0bd4cb3a9924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35af923-180b-4b4a-b4f1-912cc682017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_entities(entities_in):\n",
    "    entities_out = []\n",
    "    for entity in entities_in:\n",
    "        start_tag = entity[\"entity\"][0]\n",
    "        label = entity[\"entity\"][2:]\n",
    "        if start_tag == \"B\" or not entities_out:\n",
    "            entities_out.append({\"start\": entity[\"start\"], \"end\": entity[\"end\"], \"label\": label})\n",
    "        else:\n",
    "            entities_out[-1][\"end\"] = entity[\"end\"]\n",
    "    return entities_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56502de2-83fd-4a6a-b200-74449bbea1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def render_text(text, entities):\n",
    "    displacy.render({ \"text\": re.sub(\"\\\\n\", \" \", text), \n",
    "                      \"ents\": convert_entities(entities) }, \n",
    "                      options = { \"colors\": { \"PERSON\": \"orange\" } }, style = \"ent\", manual = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53782c7c-79d2-4b84-b85e-8a742e954da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_text(texts[2], entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5764092-bb7a-4205-aff1-e0f60750c294",
   "metadata": {},
   "source": [
    "## 4. Post-process entities\n",
    "\n",
    "Expand entities which end in the middle of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408c464-2237-45be-adb0-b91b58c4705d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_entities(entities_in, text):\n",
    "    entities_out = []\n",
    "    for entity_in in entities_in:\n",
    "        entity_out = entity_in.copy()\n",
    "        while (entity_out[\"end\"] < len(text) and \n",
    "               (re.search(\"\\w\", text[entity_out[\"end\"]]) or re.search(\"[.,-]\", text[entity_out[\"end\"]]))):\n",
    "            entity_out[\"word\"] += text[entity_out['end']]\n",
    "            entity_out[\"end\"] += 1\n",
    "        entities_out.append(entity_out)\n",
    "    return entities_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a5f39-1112-4fc2-aee3-171a0aae31fb",
   "metadata": {},
   "source": [
    "Combine successive entities where the second one has a label starting with I or the same label as the previous entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519aa385-eebf-4c80-b7e8-574546acaec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_entities(entities_in):\n",
    "    entities_out = []\n",
    "    for entity_in in entities_in:\n",
    "        entity_out = entity_in.copy()\n",
    "        if len(entities_out) == 0:\n",
    "            entities_out.append(entity_out)\n",
    "        elif re.search(\"^I-\", entity_out[\"entity\"]):\n",
    "            expand_last_entity(entities_out, entity_out)\n",
    "        else:\n",
    "            entity_out[\"entity\"] = re.sub(\"^[BIE]-\", \"B-\", entity_out[\"entity\"])\n",
    "            if entity_out[\"start\"] < entities_out[-1][\"start\"]:\n",
    "                print(\"error: entities are not sorted by position!\")\n",
    "            elif entity_out[\"start\"] <= entities_out[-1][\"end\"] + 1 and entity_out[\"entity\"] == entities_out[-1][\"entity\"]:\n",
    "                expand_last_entity(entities_out, entity_out)\n",
    "            else:\n",
    "                entities_out.append(entity_out)\n",
    "    return entities_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31be041-f952-4905-aa32-50e67714770b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_last_entity(entities, entity):\n",
    "    entities[-1][\"word\"] += \" \" + entity[\"word\"]\n",
    "    entities[-1][\"end\"] = entity[\"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d9944-7f04-4719-bb02-2d326467ad00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_and_render_texts(texts):\n",
    "    for text_id in texts:\n",
    "        text = texts[text_id]\n",
    "        entities = run_bert_pipeline(text)\n",
    "        entities = combine_entities(expand_entities(entities, text))\n",
    "        print(f\"Text {text_id}\")\n",
    "        render_text(text, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4704120-01b7-44b5-ac6c-710a789c7efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_and_render_texts({ text_id:texts[text_id] for text_id in texts if text_id < 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fccebfa-cd62-43a1-a6ad-5f4b1dc0602e",
   "metadata": {},
   "source": [
    "## 5. Get name of deceased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9bef6-836d-49cc-92be-b26317f921fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_text_patterns(query, text):\n",
    "    positions = []\n",
    "    pattern = re.compile(query)\n",
    "    for m in pattern.finditer(text.lower()):\n",
    "        positions.append({\"start\": m.start(), \"end\": m.end()})\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681fc8f-1c54-4aa1-bd2e-e5ff73858150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_name_of_deceased(text, entities):\n",
    "    deceased = []\n",
    "    positions = find_text_patterns(\"overleden is:?,?\", text) + find_text_patterns(\"is overleden:?,?\", text)\n",
    "    for position in positions:\n",
    "        name_deceased = \"\"\n",
    "        for entity in entities:\n",
    "            if entity[\"start\"] == position[\"end\"] + 1:\n",
    "                name_deceased = entity[\"word\"]\n",
    "        deceased.append(name_deceased)\n",
    "    positions = find_text_patterns(\"levens?loos\", text)\n",
    "    return deceased, len(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09f21e-4bc3-4fe9-aa5b-cec1355a0d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_deceased_names(results, nbr_of_names_found, nbr_of_stillborns_found):\n",
    "    if len(results[0]) != 0 and re.search(\"\\w\", results[0][0]):\n",
    "        nbr_of_names_found += 1\n",
    "    if results[1] > 0:\n",
    "        nbr_of_stillborns_found += 1\n",
    "    return nbr_of_names_found, nbr_of_stillborns_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec810024-dc96-43d2-b0e9-229818c63a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbr_of_names_found = 0\n",
    "nbr_of_stillborns_found = 0\n",
    "\n",
    "for text_id in sorted(texts.keys()):\n",
    "    text = texts[text_id]\n",
    "    entities = run_bert_pipeline(text)\n",
    "    entities = combine_entities(expand_entities(entities, text))\n",
    "    print(f\"Text {text_id}:\", end=\" \")\n",
    "    results = get_name_of_deceased(text, entities)\n",
    "    nbr_of_names_found, nbr_of_stillborns_found = evaluate_deceased_names(results, nbr_of_names_found, nbr_of_stillborns_found)\n",
    "    print(results)\n",
    "print(f\"Records: {len(texts)}; Names found: {nbr_of_names_found};\", end=\" \")\n",
    "print(f\"Stillborns: {nbr_of_stillborns_found}; Missing: {len(texts)-nbr_of_names_found-nbr_of_stillborns_found}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1012f-2345-4f77-bfae-4a31c05495bc",
   "metadata": {},
   "source": [
    "## 6. Get decease date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3bb60e-0c61-4ea3-9fb4-f8439a370fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b20a3-953c-49f6-bde5-e5331d35a85d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ordinals = { \"eersten\": 1, \"tweeden\": 2, \"derden\": 3, \"vierden\": 4, \"vijfden\": 5,\n",
    "             \"zesden\": 6, \"zevenden\": 7, \"achtsten\": 8, \"negenden\": 9, \"tienden\": 10,\n",
    "             \"elfden\": 11, \"twaalfden\": 12, \"dertienden\": 13, \"veertienden\": 14, \"vijftienden\": 15,\n",
    "             \"zestienden\": 16, \"zeventienden\": 17, \"achttienden\": 18, \"negentienden\": 19, \"twintigsten\": 20,\n",
    "             \"eenentwintigsten\": 21, \"tweeentwintigsten\": 22, \"drieentwintigsten\": 23, \"vierentwintigsten\": 24, \"vijfentwintigsten\": 25,\n",
    "             \"zesentwintigsten\": 26, \"zevenentwintigsten\": 27, \"achtentwintigsten\": 28, \"negenentwintigsten\": 29, \"dertigsten\": 30,\n",
    "             \"eenendertigsten\": 31,\n",
    "             \"een en twintigsten\": 21, \"twee en twintigsten\": 22, \"drie en twintigsten\": 23, \"vier en twintigsten\": 24, \"vijf en twintigsten\": 25,\n",
    "             \"zes en twintigsten\": 26, \"zeven en twintigsten\": 27, \"acht en twintigsten\": 28, \"negen en twintigsten\": 29, \n",
    "             \"een en dertigsten\": 31,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca86f2-f4f4-4ecf-86d1-9b4e9b117477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cardinals = {             \"een\": 1,  \"twee\": 2,     \"drie\": 3,     \"vier\": 4,      \"vijf\": 5,      \"zes\": 6,      \"zeven\": 7,      \"acht\": 8,     \"negen\": 9,\n",
    "              \"tien\": 10, \"elf\": 11, \"twaalf\": 12,  \"dertien\": 13, \"veertien\": 14, \"vijftien\": 15, \"zestien\": 16, \"zeventien\": 17, \"achttien\":18, \"negentien\": 19,\n",
    "                                     \"twintig\": 20, \"dertig\": 30,  \"veertig\": 40,  \"vijftig\": 50,  \"zestig\": 60,  \"zeventig\": 70,  \"tachtig\": 80, \"negentig\": 90, } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2bd2c-2554-4942-ae67-5f065c121891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "others = { \"en\": 0, \"honderd\": 100, \"duizend\": 1000, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce63903-b191-4283-9ea9-9b00625cc2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_months = { '': 0, \"januari\": 1, \"februari\": 2, \"maart\": 3, \"april\": 4, \"mei\": 5, \"juni\": 6,\n",
    "                \"juli\": 7, \"augustus\": 8, \"september\": 9, \"oktober\": 10, \"november\": 11, \"december\": 12,\n",
    "                \"july\": 7, \"october\": 10, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a480b-34b2-45d5-bb2a-9e5fcbb73acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_next_token(position, text):\n",
    "    while position < len(text) - 1 and re.search(\"\\s\", text[position]):\n",
    "        position += 1\n",
    "    token = \"\"\n",
    "    while position < len(text) - 1 and not re.search(\"\\s\", text[position]):\n",
    "        token += text[position]\n",
    "        position += 1\n",
    "    return token, position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203f949-eaf7-4678-96a5-810562e83d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup(text_in):\n",
    "    text_out = re.sub(\"\\s+\", \" \", text_in)\n",
    "    text_out = re.sub(\"- \", \"\", text_out)\n",
    "    return re.sub(\"\\W?$\", \"\", text_out.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2077510-eb9e-44c9-8767-970427f16826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date_day(position, text):\n",
    "    return number_parser(text[position:])\n",
    "    day = \"\"\n",
    "    next_token, end_position = get_next_token(position, text)\n",
    "    next_next_token, dummy = get_next_token(position + len(next_token) + 1, text)\n",
    "    if cleanup(next_token) in dict(ordinals, **cardinals).keys() and not re.search(\"^en$\", next_next_token, re.IGNORECASE):\n",
    "        day = next_token\n",
    "    elif cleanup(next_token) + \"n\" in dict(ordinals, **cardinals).keys() and not re.search(\"^en$\", next_next_token, re.IGNORECASE):\n",
    "        day = next_token\n",
    "    else:\n",
    "        next_next_token, end_position = get_next_token(position + len(next_token) + 1, text)\n",
    "        next_token += \" \" + next_next_token\n",
    "        next_next_token, end_position = get_next_token(position + len(next_token) + 1, text)\n",
    "        next_token += \" \" + next_next_token\n",
    "        if cleanup(next_token) in dict(ordinals, **cardinals).keys():\n",
    "            day = next_token\n",
    "        elif cleanup(next_token) + \"n\" in dict(ordinals, **cardinals).keys():\n",
    "            day = next_token\n",
    "    if day:\n",
    "        return day, end_position - position\n",
    "    else:\n",
    "        return day, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e9111-45d2-49e0-ab88-803a79672463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date_month(position, text):\n",
    "    month = \"\"\n",
    "    next_token, end_position = get_next_token(position, text)\n",
    "    if cleanup(next_token) in date_months.keys():\n",
    "        month = next_token\n",
    "    elif re.search(\"-$\", next_token):\n",
    "        next_next_token, next_end_position = get_next_token(end_position, text)\n",
    "        next_token = re.sub(\"-$\", \"\", next_token)\n",
    "        next_token += next_next_token\n",
    "        if cleanup(next_token) in date_months.keys():\n",
    "            month = next_token\n",
    "            end_position = next_end_position\n",
    "    if month:\n",
    "        return month, end_position - position\n",
    "    else:\n",
    "        return month, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4f9be-d3c3-4a05-bcee-25ad235eb289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date_year(position, text):\n",
    "    year = \"\"\n",
    "    next_token, next_position = get_next_token(position, text)\n",
    "    if next_token.lower() != \"des\":\n",
    "        return number_parser(text[position:])\n",
    "    next_token, next_position = get_next_token(next_position, text)\n",
    "    return number_parser(text[next_position:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f93c34-3a3f-4648-8cb1-0461c39753da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def longest_number_match(text):\n",
    "    longest_match = \"\"\n",
    "    longest_match_length = 0\n",
    "    text_index = 0\n",
    "    while text_index < len(text) and re.search(\"\\s\", text[text_index]):\n",
    "        text_index += 1\n",
    "    for i in range(text_index, text_index + 25):\n",
    "        phrase = cleanup(text[text_index: i])\n",
    "        if phrase in cardinals.keys() and phrase != longest_match:\n",
    "            longest_match = phrase\n",
    "            longest_match_length = int(i)\n",
    "        elif phrase in ordinals.keys() and phrase != longest_match:\n",
    "            longest_match = phrase\n",
    "            longest_match_length = int(i)\n",
    "        elif phrase in others.keys() and phrase != longest_match:\n",
    "            longest_match = phrase\n",
    "            longest_match_length = int(i)\n",
    "    return longest_match, longest_match_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da031d75-1f94-41d7-9d0c-da8b530a7916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_off_hundreds_thousands(tokens):\n",
    "    if re.search(\".(honderd|duizend)\", tokens[0].lower()):\n",
    "        tokens.insert(0, re.sub(\"(honderd|duizend).*\", \"\", tokens[0].lower()))\n",
    "        tokens[1] = re.sub(\".*(honderd|duizend)\", \"\\\\1\", tokens[1].lower())\n",
    "\n",
    "\n",
    "def number_parser(text):\n",
    "    if not text:\n",
    "        return 0, 0\n",
    "    first_number, first_offset = longest_number_match(text)\n",
    "    second_number, second_offset = longest_number_match(text[first_offset:])\n",
    "    if cleanup(first_number) == \"en\":\n",
    "        number, offset = number_parser(text[first_offset:])\n",
    "        return number, offset + first_offset\n",
    "    if cleanup(first_number) in cardinals:\n",
    "        if cleanup(second_number) == \"honderd\":\n",
    "            number, offset = number_parser(text[first_offset + second_offset:])\n",
    "            return 100 * cardinals[cleanup(first_number)] + number, first_offset + second_offset + offset\n",
    "        if cleanup(second_number) == \"duizend\":\n",
    "            number, offset = number_parser(text[first_offset + second_offset:])\n",
    "            return 1000 * cardinals[cleanup(first_number)] + number, first_offset + second_offset + offset\n",
    "        number, offset = number_parser(text[first_offset:])\n",
    "        return cardinals[cleanup(first_number)] + number, first_offset + offset\n",
    "    if cleanup(first_number) in ordinals:\n",
    "        number, offset = number_parser(text[first_offset:])\n",
    "        return ordinals[cleanup(first_number)] + number, first_offset + offset\n",
    "    return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5dd2a3-c2bd-462e-875e-e5480a32c07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dates(texts, text_id, pattern):\n",
    "    dates = []\n",
    "    text = texts[text_id]\n",
    "    entities = run_bert_pipeline(text)\n",
    "    entities = combine_entities(expand_entities(entities, text))\n",
    "    day = \"\"\n",
    "    positions = find_text_patterns(pattern, text)\n",
    "    for position in positions:\n",
    "        day, token_length_day = get_date_day(position[\"end\"], text)\n",
    "        month, token_length_month = get_date_month(position[\"end\"] + token_length_day, text)\n",
    "        year, token_length_year = get_date_year(position[\"end\"] + token_length_day + token_length_month, text)\n",
    "        dates.append((day,month,year))\n",
    "    return summarize_dates(dates)\n",
    "\n",
    "\n",
    "def complete_date(date):\n",
    "    return date[0] != 0 and date[1] != \"\" and date[2] != 0\n",
    "\n",
    "\n",
    "def contains_complete_date(dates):\n",
    "    if not dates:\n",
    "        return False\n",
    "    elif complete_date(dates[0]):\n",
    "        return True\n",
    "    else:\n",
    "        return contains_complete_date(dates[1:])\n",
    "\n",
    "\n",
    "def summarize_dates(dates_in):\n",
    "    keep_only_complete_dates = contains_complete_date(dates_in)\n",
    "    dates_out = []\n",
    "    for date in dates_in:\n",
    "        (day, month, year) = date\n",
    "        if complete_date(date):\n",
    "            dates_out.append(date)\n",
    "        elif not keep_only_complete_dates and (day != 0 or month != \"\" or year != 0):\n",
    "            dates_out.append(date)\n",
    "    return dates_out    \n",
    "\n",
    "def print_dates(texts, text_id, dates, note=\"\"):\n",
    "    summarized_dates = summarize_dates(dates)\n",
    "    if not summarized_dates:\n",
    "        print_with_color(f\"Text {text_id}: (no dates found)\")\n",
    "    for date in summarize_dates(dates):\n",
    "        (day, month, year) = date\n",
    "        try:\n",
    "            print(f\"Text {text_id}: {day} {month} {year} ({ordinals[cleanup(day)]}-{date_months[cleanup(month)]}-{year}) {note}\")\n",
    "        except:\n",
    "            try:\n",
    "                print(f\"Text {text_id}: {day} {month} {year} ({cardinals[cleanup(day)]}-{date_months[cleanup(month)]}-{year}) {note}\")\n",
    "            except:\n",
    "                if day != 0 and month != \"\" and year != 0:\n",
    "                    print(f\"Text {text_id}: {day} {month} {year} {note}\")\n",
    "                else:\n",
    "                    print_with_color(f\"Text {text_id}: {day} {month} {year} {note}\")\n",
    "\n",
    "def get_death_date(texts, text_id):\n",
    "    dates = get_dates(texts, text_id, \"op den\")\n",
    "    if not dates:\n",
    "        death_dates = get_dates(texts, text_id, \"op\")\n",
    "        document_dates = get_document_date(texts, text_id)\n",
    "        for date in death_dates:\n",
    "            if date[2] != 0:\n",
    "                dates.append(date)\n",
    "            elif document_dates and document_dates[0][2] != 0 and (date[0] != 0 or date[1] != \"\"):\n",
    "                if date_months[cleanup(document_dates[0][1])] < date_months[cleanup(date[1])]:\n",
    "                    dates.append((date[0], date[1], document_dates[0][2] - 1))\n",
    "                else:\n",
    "                    dates.append((date[0], date[1], document_dates[0][2]))\n",
    "            elif not document_dates or document_dates[0][2] == 0:\n",
    "                if date[0] != 0 or date[1] != \"\" or date[2] != 0:\n",
    "                    dates.append(date)\n",
    "    print_dates(texts, text_id, dates)\n",
    "    return dates\n",
    "    \n",
    "    \n",
    "def get_document_date(texts, text_id):\n",
    "    dates = get_dates(texts, text_id, \"heden\")\n",
    "    if not dates:\n",
    "        dates = get_dates(texts, text_id, \"heden den\")\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddc5b4-df1c-4c3e-a1ee-8d482c9b314f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_and_render_texts({ text_id:texts[text_id] for text_id in texts if text_id == 21})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea38c5-1350-4a1d-ba88-40f865c0ca44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nbr_of_incomplete_dates = 0\n",
    "for text_id in sorted(texts.keys()):\n",
    "    dates = get_death_date(texts, text_id)\n",
    "    for date in dates:\n",
    "        if date[0] == 0 or date[1] == \"\" or date[2] == 0:\n",
    "            nbr_of_incomplete_dates += 1\n",
    "print(f\"number of incomplete dates: {nbr_of_incomplete_dates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03f829-cea2-4b02-b189-69de2dff29fb",
   "metadata": {},
   "source": [
    "**Notes Training set V2:**\n",
    "\n",
    "- 3: spelling error: twinttigsten\n",
    "- 8: spelling error: teen duizend\n",
    "- 18: spelling error: twintigste\n",
    "- 27 spelling error: decemder\n",
    "- 61: spelling error: achtiende\n",
    "\n",
    "**Notes Sample regex:**\n",
    "\n",
    "- 1: extra space\n",
    "- 2: extra space\n",
    "- 5: extra space\n",
    "- 10: spelling error\n",
    "- 12: extra space\n",
    "- 15: spelling error\n",
    "- 19: month as number\n",
    "- 21 no month\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51213e-b848-4fd9-bbcb-97619cd6e14a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
