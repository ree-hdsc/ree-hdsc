{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d6401a-0168-495f-8457-d9a0d427cb81",
   "metadata": {},
   "source": [
    "# Get names from scans\n",
    "\n",
    "Extract the parts of the scans of the Curacao Death Registers which contain the name of the deceased. This information is based on data annotation by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43299ca-5ca4-466e-947c-f5c020a4cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex\n",
    "import sys\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "from scripts.read_transkribus_files import read_files\n",
    "import matplotlib.pyplot as plt\n",
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0834e8-a479-4c45-a6f1-189b7b80af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_file_name(coordinates_file_name):\n",
    "    \"\"\" change Transkribus file name to corresponding image file name and add location of directory \"\"\"\n",
    "    return \"../website/private/hdsc/data/\" + regex.sub(\".xml$\", \".JPG\", coordinates_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f4bb12-3cab-4c56-a23b-a1cbdffcbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_color(string, color_code=1):\n",
    "    print(f\"\\x1b[3{color_code}m{string}\\x1b[m\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3235509-577b-430e-b15f-9a0bdfc9ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84814ee-c1ed-4e21-871f-4497ad18e2c6",
   "metadata": {},
   "source": [
    "## 1. Read logfile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892fbb7-4f58-4359-a77a-f52f404236e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_logfile_data(logfile_name):\n",
    "    \"\"\" read manually checked locations of deceased names in scans \"\"\"\n",
    "    return pd.read_csv(logfile_name, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad376a-bd62-42ff-9ac6-f31d88af01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_logfile_data_to_scan_data(logfile_data):\n",
    "    \"\"\" remove duplicates from logfile data; keep only final annotation \"\"\"\n",
    "    scan_data = {}\n",
    "    for index, row in logfile_data.iterrows():\n",
    "        coordinates_file_name, status, text_line_id, coords_id, deceased_name, ip_address, date, text_line_name = row\n",
    "        scan_data[coordinates_file_name] = [index, status, text_line_id, coords_id, ip_address, text_line_name]\n",
    "    return scan_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd55788-e782-48fb-9f78-ce340b9c5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGFILE_NAME = \"../website/private/hdsc/etc/logfile\"\n",
    "\n",
    "logfile_data = read_logfile_data(LOGFILE_NAME)\n",
    "logfile_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758daa9a-9636-4819-88bd-9fad229fea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_data = convert_logfile_data_to_scan_data(logfile_data)\n",
    "pd.DataFrame.from_dict(scan_data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3a5c0-5d26-4708-8b67-0769a336eba6",
   "metadata": {},
   "source": [
    "## 2. Find best coordinates for guess of deceased name position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745937a6-9aae-4548-b880-5e2dc596c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_from_line(line):\n",
    "    \"\"\" extract polygon coordinates from points attribute of Coords tag in Transkribus xml, format: '123,456 789,012 ...'\"\"\"\n",
    "    split_line = [ pair.split(\",\") for pair in line.split() ]\n",
    "    return [ ( int(x), int(y) ) for x, y in split_line ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d883e-828d-4e07-b855-3105f47ef460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_left(polygon):\n",
    "    \"\"\" find top left position of polygon (alternative: use polygon2rectangle) \"\"\"\n",
    "    top_coordinate = sys.maxsize\n",
    "    left_coordinate = sys.maxsize\n",
    "    for pair in polygon[0][\"points\"]:\n",
    "        if pair[1] < top_coordinate:\n",
    "            top_coordinate = pair[1]\n",
    "        if pair[0] < left_coordinate:\n",
    "            left_coordinate = pair[0]\n",
    "    return top_coordinate, left_coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc0afe-75ff-45ef-ba2f-fc7ab4b0f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_polygons(polygons):\n",
    "    \"\"\" sort polygons based on top left point: first by vertical position, then by horizontal position \"\"\"\n",
    "    extended_polygons = []\n",
    "    for polygon in polygons:\n",
    "        top_coordinate, left_coordinate = find_top_left(polygon)\n",
    "        extended_polygons.append([top_coordinate, left_coordinate, polygon])\n",
    "    return [ extended_polygon[2] \n",
    "             for extended_polygon in sorted(extended_polygons, \n",
    "                                            key=lambda ep: (ep[0], ep[1])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa15d6b-9143-4bd6-88ee-e2e89735ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_START_SORTING = 574\n",
    "INDEX_REMOVE_MARIGINALIA = 2307\n",
    "\n",
    "def get_text_polygons(coordinates_file_name, index):\n",
    "    \"\"\" read polygons from Transkribus file (universal version)\"\"\"\n",
    "    root = ET.parse(coordinates_file_name).getroot()\n",
    "    polygons = []\n",
    "    polygons_by_name = {}\n",
    "    for text_region in root.findall(\".//{*}TextRegion\"):\n",
    "        text_region_polygons = []\n",
    "        for text_line in text_region.findall(\"./{*}TextLine\"):\n",
    "            text_region_polygons.append([])\n",
    "            for coords in text_line.findall(\"./{*}Coords\"):\n",
    "                coordinates = get_coordinates_from_line(coords.attrib[\"points\"])\n",
    "                text_region_polygons[-1].append({\"points\": coordinates, \"name\": text_line.attrib[\"id\"]})\n",
    "                polygons_by_name[text_line.attrib[\"id\"]] = coordinates\n",
    "        if index < INDEX_START_SORTING:\n",
    "            polygons.extend(text_region_polygons)\n",
    "        elif index < INDEX_REMOVE_MARIGINALIA:\n",
    "            polygons.extend(sort_polygons(text_region_polygons))\n",
    "        else:\n",
    "            if len(text_region_polygons) > len(polygons):\n",
    "                polygons = sort_polygons(text_region_polygons)\n",
    "    return polygons, polygons_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86ee31-1cf4-48df-8aeb-54e64943810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_polygons_new(coordinates_file_name):\n",
    "    \"\"\" read polygons from Transkribus file (ignores mariginalia)\"\"\"\n",
    "    root = ET.parse(coordinates_file_name).getroot()\n",
    "    polygons = []\n",
    "    polygons_by_name = {}\n",
    "    for text_region in root.findall(\".//{*}TextRegion\"):\n",
    "        text_region_polygons = []\n",
    "        for text_line in text_region.findall(\"./{*}TextLine\"):\n",
    "            text_region_polygons.append([])\n",
    "            for coords in text_line.findall(\"./{*}Coords\"):\n",
    "                coordinates = get_coordinates_from_line(coords.attrib[\"points\"])\n",
    "                text_region_polygons[-1].append({\"points\": coordinates, \"name\": text_line.attrib[\"id\"]})\n",
    "                polygons_by_name[text_line.attrib[\"id\"]] = coordinates\n",
    "        if len(text_region_polygons) > len(polygons):\n",
    "            polygons = sort_polygons(text_region_polygons)\n",
    "    return polygons, polygons_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b90c10-3f3f-4611-a8f6-14963734f9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_polygons_old(coordinates_file_name):\n",
    "    \"\"\" read polygons from Transkribus file (considers mariginalia)\"\"\"\n",
    "    root = ET.parse(coordinates_file_name).getroot()\n",
    "    polygons = []\n",
    "    polygons_by_name = {}\n",
    "    for text_region in root.findall(\".//{*}TextRegion\"):\n",
    "        text_region_polygons = []\n",
    "        for text_line in text_region.findall(\"./{*}TextLine\"):\n",
    "            text_region_polygons.append([])\n",
    "            for coords in text_line.findall(\"./{*}Coords\"):\n",
    "                coordinates = get_coordinates_from_line(coords.attrib[\"points\"])\n",
    "                text_region_polygons[-1].append({\"points\": coordinates, \"name\": text_line.attrib[\"id\"]})\n",
    "                polygons_by_name[text_line.attrib[\"id\"]] = coordinates\n",
    "        polygons.extend(sort_polygons(text_region_polygons))\n",
    "    return polygons, polygons_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd86a6-fe9c-492c-ba12-b1535379f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon2rectangle(coordinates):\n",
    "    \"\"\" find coordinates of minimal rectangle completely enclosing polygon \"\"\"\n",
    "    x_min, x_max, y_min, y_max = (sys.maxsize, 0, sys.maxsize, 0)\n",
    "    for x, y in coordinates:\n",
    "        if x < x_min: x_min = x\n",
    "        if x > x_max: x_max = x\n",
    "        if y < y_min: y_min = y\n",
    "        if y > y_max: y_max = y\n",
    "    return x_min, y_min, x_max, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138d0a2-85da-4fdb-b40c-56bb822c081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encloses_point(rectangle, point):\n",
    "    \"\"\" check if point is inside rectangle \"\"\"\n",
    "    return(rectangle[0] <= point[0] and rectangle[2] >= point[0] and\n",
    "           rectangle[1] <= point[1] and rectangle[3] >= point[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884142f1-6fe4-4232-b03e-19ea2aed1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_polygon_for_y(polygons, y):\n",
    "    \"\"\" find polygon closest to horizontal line indicated by argument y \"\"\"\n",
    "    best_distance, best_text_line_id, best_coords_id = (sys.maxsize, -1, -1)\n",
    "    for text_line_id in range(0, len(polygons)):\n",
    "        for coords_id in range(0, len(polygons[text_line_id])):\n",
    "            rectangle = polygon2rectangle(polygons[text_line_id][coords_id][\"points\"])\n",
    "            distance = abs(y - rectangle[1])\n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_text_line_id = text_line_id\n",
    "                best_coords_id = coords_id\n",
    "    return best_text_line_id, best_coords_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f517a-c858-452b-b95d-77642d76c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polygon_coordinates(polygons, polygons_by_name, text_line_id, coords_id, text_line_name):\n",
    "    if text_line_name != \"xxx\" and text_line_name != \"NA\":\n",
    "        return polygons_by_name[text_line_name]\n",
    "    else:\n",
    "        return polygons[text_line_id][coords_id][\"points\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08358199-2cf0-4455-b1fe-16f1552ba09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_point_name_hits(best_point_x, best_point_y, best_line_y, decades):\n",
    "    \"\"\" evaluate values of best_point_x, best_point_y and best_line_y: how often do they predict the right deceased name polygon \"\"\"\n",
    "    hit_counts = 0\n",
    "    for coordinates_file_name in scan_data:\n",
    "        index, status, text_line_id, coords_id, ip_address, text_line_name = scan_data[coordinates_file_name]\n",
    "        if index >= 600 and status == \"save\" and regex.search(decades, coordinates_file_name):\n",
    "            polygons, polygons_by_name = get_text_polygons(\"../website/private/hdsc/data/page/\" + coordinates_file_name, \n",
    "                                                           index)\n",
    "            polygon = get_polygon_coordinates(polygons, polygons_by_name, text_line_id, coords_id, text_line_name)\n",
    "            rectangle = polygon2rectangle(polygon)\n",
    "            if encloses_point(rectangle, (best_point_x, best_point_y)):\n",
    "                hit_counts += 1\n",
    "            else:\n",
    "                best_text_line_id, best_coords_id = get_best_polygon_for_y(polygons, best_line_y)\n",
    "                if best_text_line_id == text_line_id and best_coords_id == coords_id:\n",
    "                    hit_counts += 1\n",
    "    return hit_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261c0ed-c7f9-4b1b-afab-1db3328ffdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_default_point(best_point_x, best_point_y, best_line_y, incr=10, decades=\"18[34][0-9]\"):\n",
    "    \"\"\" find best guess for position inside deceased name frame: best_point_x, best_point_y; keep best_line_y constant \"\"\"\n",
    "    while True:\n",
    "        hit_counts = count_point_name_hits(best_point_x, best_point_y, best_line_y, decades)\n",
    "        print(best_point_x, best_point_y, \"#\", best_line_y, hit_counts, incr)\n",
    "        if count_point_name_hits(best_point_x + incr, best_point_y, best_line_y, decades) > hit_counts:\n",
    "            best_point_x += incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x + incr, best_point_y + incr, best_line_y, decades) > hit_counts:\n",
    "            best_point_x += incr\n",
    "            best_point_y += incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x, best_point_y + incr, best_line_y, decades) > hit_counts:\n",
    "            best_point_y += incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x - incr, best_point_y + incr, best_line_y, decades) > hit_counts:\n",
    "            best_point_x -= incr\n",
    "            best_point_y += incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x - incr, best_point_y, best_line_y, decades) > hit_counts:\n",
    "            best_point_x -= incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x - incr, best_point_y - incr, best_line_y, decades) > hit_counts:\n",
    "            best_point_x -= incr\n",
    "            best_point_y -= incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x, best_point_y - incr, best_line_y, decades) > hit_counts:\n",
    "            best_point_y -= incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x + incr, best_point_y - incr, best_line_y, decades) > hit_counts:\n",
    "            best_point_x += incr\n",
    "            best_point_y -= incr\n",
    "            continue\n",
    "        break\n",
    "    return best_point_x, best_point_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e1985-f4c6-410c-9184-f44b23d4d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_line_y_default(best_point_x, best_point_y, best_line_y, incr=10, decades=\"18[34][0-9]\"):\n",
    "    \"\"\" find horizontal line closests to top of most deceased name boxes: best_line_y; \n",
    "        keep best_point_x and best_point_y constant \"\"\"\n",
    "    while True:\n",
    "        hit_counts = count_point_name_hits(best_point_x, best_point_y, best_line_y, decades)\n",
    "        print(best_line_y, \"#\", best_point_x, best_point_y, hit_counts, incr)\n",
    "        if count_point_name_hits(best_point_x, best_point_y, best_line_y + incr, decades) > hit_counts:\n",
    "            best_line_y += incr\n",
    "            continue\n",
    "        if count_point_name_hits(best_point_x, best_point_y, best_line_y + incr, decades) > hit_counts:\n",
    "            best_line_y -= incr\n",
    "            continue\n",
    "        break\n",
    "    return best_line_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517e5d5-ec96-405b-81d8-a643c34fda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADES = \"18[34][0-9]\"\n",
    "X_DEFAULT = 688\n",
    "Y_DEFAULT = 471\n",
    "LINE_Y_DEFAULT = 510\n",
    "\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, LINE_Y_DEFAULT, incr=10, decades=DECADES)\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, best_line_y, incr=5, decades=DECADES)\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, best_line_y, incr=2, decades=DECADES)\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, best_line_y, incr=1, decades=DECADES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f10dd4-c5de-4c67-87c2-06b28335bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_point_x, best_point_y = find_best_default_point(X_DEFAULT, Y_DEFAULT, LINE_Y_DEFAULT, incr=10, decades=DECADES)\n",
    "best_point_x, best_point_y = find_best_default_point(best_point_x, best_point_y, LINE_Y_DEFAULT, incr=5, decades=DECADES)\n",
    "best_point_x, best_point_y = find_best_default_point(best_point_x, best_point_y, LINE_Y_DEFAULT, incr=2, decades=DECADES)\n",
    "best_point_x, best_point_y = find_best_default_point(best_point_x, best_point_y, LINE_Y_DEFAULT, incr=1, decades=DECADES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4316c66-76bb-4b10-8f8d-032e45dc2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECADES = \"192[0-9]\"\n",
    "X_DEFAULT = 818\n",
    "Y_DEFAULT = 610\n",
    "LINE_Y_DEFAULT = 745\n",
    "\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, LINE_Y_DEFAULT, incr=10, decades=DECADES)\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, best_line_y, incr=5, decades=DECADES)\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, best_line_y, incr=2, decades=DECADES)\n",
    "best_line_y = find_best_line_y_default(X_DEFAULT, Y_DEFAULT, best_line_y, incr=1, decades=DECADES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c1f3a-f2d7-4b24-9883-9b5f70a9621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_point_x, best_point_y = find_best_default_point(X_DEFAULT, Y_DEFAULT, LINE_Y_DEFAULT, incr=10, decades=DECADES)\n",
    "best_point_x, best_point_y = find_best_default_point(best_point_x, best_point_y, LINE_Y_DEFAULT, incr=5, decades=DECADES)\n",
    "best_point_x, best_point_y = find_best_default_point(best_point_x, best_point_y, LINE_Y_DEFAULT, incr=2, decades=DECADES)\n",
    "best_point_x, best_point_y = find_best_default_point(best_point_x, best_point_y, LINE_Y_DEFAULT, incr=1, decades=DECADES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f26d5-ba89-451b-9d34-10227ca8eb61",
   "metadata": {},
   "source": [
    "## 3. Compute areas of identified deceased name frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640b231-3c04-456d-a781-3be9b8ff56ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rectangle_area(rectangle):\n",
    "    \"\"\" compute the are of a rectangle \"\"\"\n",
    "    x_min, y_min, x_max, y_max = rectangle\n",
    "    return (x_max - x_min) * (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf770fb-afd0-4417-8f1c-733e16d58c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_areas(scan_data):\n",
    "    \"\"\" compute areas of rectangles related to polygons and show minimum, average and maximum value \"\"\"\n",
    "    min_area, max_area, area_count, area_total = sys.maxsize, 0, 0, 0\n",
    "    for coordinates_file_name in scan_data:\n",
    "        index, status, text_line_id, coords_id, ip_address, text_line_name = scan_data[coordinates_file_name]\n",
    "        if status == \"save\":\n",
    "            polygons, polygons_by_name = get_text_polygons(\"../website/private/hdsc/data/page/\" + coordinates_file_name, index)\n",
    "            polygon = get_polygon_coordinates(polygons, polygons_by_name, text_line_id, coords_id, text_line_name)\n",
    "            rectangle = polygon2rectangle(polygon)\n",
    "            area = compute_rectangle_area(rectangle)\n",
    "            if area < min_area:\n",
    "                min_area = area\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "            area_count += 1\n",
    "            area_total += area\n",
    "    return min_area, max_area, int(area_total/area_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be37145-de12-44ec-ae2a-48c70a337e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_areas(scan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833a237-0ba2-4834-a275-bd299dc212a4",
   "metadata": {},
   "source": [
    "## 4. Compute aspect ratios of identified deceased name frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778dadf6-ccdc-4053-a459-f1e8181da996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rectangle_aspect_ratio(rectangle):\n",
    "    \"\"\" compute the are of a rectangle \"\"\"\n",
    "    x_min, y_min, x_max, y_max = rectangle\n",
    "    return (x_max - x_min) / (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d46649-f5ef-41b0-bd36-90fe8c2693fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aspect_ratios(scan_data):\n",
    "    \"\"\" compute aspect ratios of rectangles related to polygons and show minimum, average and maximum value \"\"\"\n",
    "    min_ratio, max_ratio, ratio_count, ratio_total = sys.maxsize, 0, 0, 0\n",
    "    for coordinates_file_name in scan_data:\n",
    "        index, status, text_line_id, coords_id, ip_address, text_line_name = scan_data[coordinates_file_name]\n",
    "        if status == \"save\":\n",
    "            polygons, polygons_by_name = get_text_polygons(\"../website/private/hdsc/data/page/\" + coordinates_file_name, index)\n",
    "            polygon = get_polygon_coordinates(polygons, polygons_by_name, text_line_id, coords_id, text_line_name)\n",
    "            rectangle = polygon2rectangle(polygon)\n",
    "            ratio = compute_rectangle_aspect_ratio(rectangle)\n",
    "            if ratio < min_ratio:\n",
    "                min_ratio = ratio\n",
    "            if ratio > max_ratio:\n",
    "                max_ratio = ratio\n",
    "            ratio_count += 1\n",
    "            ratio_total += ratio\n",
    "    return min_ratio, max_ratio, int(ratio_total/ratio_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d80af-db82-496c-b193-905cb5f7de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_aspect_ratios(scan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901837ca-174d-4e2f-a0c5-3046bbc96405",
   "metadata": {},
   "source": [
    "## 5. Cut out polygons from scans containing deceased name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a195abb-859d-4cf7-9954-58222c6704aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "COVERED_BACKGROUND = 0\n",
    "TRANSPARENT_BACKGROUND = 255\n",
    "FILL_COLOR = (198, 178, 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be67e75-49ae-4016-bba1-409397f12f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on https://stackoverflow.com/questions/22588074/polygon-crop-clip-using-python-pil\n",
    "\n",
    "def mask_polygon(image, polygon, covered_background):\n",
    "    \"\"\" highlight polygon (= deceased name) on image of scan: transparency 255, while rest: transparency 0 \"\"\"\n",
    "    image_with_transparency = image.convert(\"RGBA\")\n",
    "    numpy_image = numpy.asarray(image_with_transparency)\n",
    "    masked_image = Image.new('P', (numpy_image.shape[1], numpy_image.shape[0]), covered_background)\n",
    "    ImageDraw.Draw(masked_image).polygon(polygon, outline=0, fill=TRANSPARENT_BACKGROUND)\n",
    "    mask = numpy.array(masked_image)\n",
    "    masked_numpy_image = numpy.empty(numpy_image.shape, dtype='uint8')\n",
    "    masked_numpy_image[:,:,:3] = numpy_image[:,:,:3]\n",
    "    masked_numpy_image[:,:,3] = mask\n",
    "    return Image.fromarray(masked_numpy_image, \"RGBA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eed105-d934-4513-a48e-9f7869edb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_pixel_value(image):\n",
    "    \"\"\" determine most frequent pixel value in image; only check transparent parts \"\"\"\n",
    "    pixel_values = {}\n",
    "    image_data = image.getdata()\n",
    "    for data in image_data:\n",
    "        if len(data) <= 3 or data[3] != 0:\n",
    "            rounded_data = [ str(int(data[0]*0.1) * 10), \n",
    "                             str(int(data[1]*0.1) * 10), \n",
    "                             str(int(data[2]*0.1) * 10) ]\n",
    "            rounded_data = \" \".join(rounded_data)\n",
    "            if rounded_data in pixel_values:\n",
    "                pixel_values[rounded_data] += 1\n",
    "            else:\n",
    "                pixel_values[rounded_data] = 1\n",
    "    minimal_value = sorted(pixel_values.items(), key=lambda pvi: pvi[1], reverse=True)[0][0]\n",
    "    return(int(minimal_value.split()[0]),\n",
    "           int(minimal_value.split()[1]),\n",
    "           int(minimal_value.split()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc78953-3a1c-4d95-87b2-d6f3cc08e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_rectangle(rectangle, border_width):\n",
    "    \"\"\" add margin of size border_width to rectangle \"\"\"\n",
    "    return( rectangle[0] - border_width, \n",
    "            rectangle[1] - border_width,\n",
    "            rectangle[2] + border_width, \n",
    "            rectangle[3] + border_width )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d24b0-19ce-4bc7-ab32-f96bacd1c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_pixel_value(pixel_value, spread=20):\n",
    "    \"\"\" add a random value to a pixel value \"\"\"\n",
    "    return pixel_value[0] + random.randint(0, spread), pixel_value[1] + random.randint(0, spread), pixel_value[2] + random.randint(0, spread), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d32659-235d-408b-bb53-dc1ac82431c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_background(masked_image):\n",
    "    \"\"\" change background color of masked image and remove transparency \"\"\"\n",
    "    masked_image_data = masked_image.getdata()\n",
    "    updated_data = []\n",
    "    frequent_pixel_value = most_frequent_pixel_value(masked_image)\n",
    "    for data in masked_image_data:\n",
    "        if data[3] == 0:\n",
    "            updated_data.append(randomize_pixel_value(frequent_pixel_value))\n",
    "        else:\n",
    "            updated_data.append(data[:3])\n",
    "    masked_image.putdata(updated_data)\n",
    "    masked_image = masked_image.convert(\"RGB\")\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95938d68-0ed4-4ef3-b728-dbe61bf45af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_frames_from_scans(scan_data):\n",
    "    \"\"\" extract name frames from scans and store images in directory images \"\"\"\n",
    "    counter = 0\n",
    "    for coordinates_file_name in dict(sorted(scan_data.items(), \n",
    "                                      key=lambda scan_data_item: scan_data_item[1][0])):\n",
    "        index, status, text_line_id, coords_id, ip_address, text_line_name = scan_data[coordinates_file_name]\n",
    "        if status == \"save\":\n",
    "            polygons, polygons_by_name = get_text_polygons(\"../website/private/hdsc/data/page/\" + coordinates_file_name, index)\n",
    "            polygon = get_polygon_coordinates(polygons, polygons_by_name, text_line_id, coords_id, text_line_name)\n",
    "            rectangle = polygon2rectangle(polygon)\n",
    "            image_file_name = make_image_file_name(coordinates_file_name)\n",
    "            image = Image.open(image_file_name)\n",
    "            masked_image = mask_polygon(image, polygon, 0).crop(expand_rectangle(rectangle, 10))\n",
    "            masked_image = fill_background(masked_image)\n",
    "            masked_image.save(\"images/\" + os.path.basename(image_file_name))\n",
    "        counter += 1\n",
    "        if 10 * int(counter/10) == counter:\n",
    "            squeal(f\"{counter}/{len(scan_data)}\")\n",
    "    squeal(f\"{counter}/{len(scan_data)}\")\n",
    "    if \"masked_image\" in vars():\n",
    "        return masked_image\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb99a6-9fb4-4953-8fec-44c7785ffba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    get_name_frames_from_scans(scan_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d26ae-76d1-45a8-9d63-68a5332bd214",
   "metadata": {},
   "source": [
    "## 6. Check image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378c831-a9e9-425d-b8c2-8d95d681361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_property_values(scan_data, property_name=\"ratio\"):\n",
    "    \"\"\" compute the ratios of all images related to scan_data \"\"\"\n",
    "    property_values = {}\n",
    "    min_property, max_property, property_count, property_total = sys.maxsize, 0, 0, 0\n",
    "    for coordinates_file_name in scan_data:\n",
    "        index, status, text_line_id, coords_id, ip_address, text_line_name = scan_data[coordinates_file_name]\n",
    "        if status == \"save\":\n",
    "            polygons, polygons_by_name = get_text_polygons(\"../website/private/hdsc/data/page/\" + coordinates_file_name, index)\n",
    "            polygon = get_polygon_coordinates(polygons, polygons_by_name, text_line_id, coords_id, text_line_name)\n",
    "            rectangle = polygon2rectangle(polygon)\n",
    "            if property_name == \"ratio\":\n",
    "                property_value = compute_rectangle_aspect_ratio(rectangle)\n",
    "            elif property_name == \"area\":\n",
    "                property_value = compute_rectangle_area(rectangle)\n",
    "            elif property_name == \"height\":\n",
    "                property_value = rectangle[3] - rectangle[1]\n",
    "            elif property_name == \"length\":\n",
    "                property_value = rectangle[2] - rectangle[0]\n",
    "            else:\n",
    "                sys.exit(f\"compute_image_property: unknown property name: {property_name}\")\n",
    "            property_values[coordinates_file_name] = [ index, property_value, ip_address ]\n",
    "    return property_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da353570-b201-46ef-85b4-f9219946b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_by_property_values(property_values, start, end):\n",
    "    \"\"\" show selected images by property values from start to end \"\"\"\n",
    "    for coordinates_file_name, ratio in sorted(property_values.items(), \n",
    "                                               key=lambda ratio_item: ratio_item[1][1])[start:end]:\n",
    "        index, property_value, ip_address = property_values[coordinates_file_name]\n",
    "        print(f\"{index} # {round(property_value, 1)} # {ip_address} # {coordinates_file_name}:\")\n",
    "        display(Image.open(\"images/\" + regex.sub(\".xml\", \".JPG\", coordinates_file_name)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ef868-4d0c-4fd8-a605-17627746b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_extreme_property_values(scan_data, property_name=\"ratio\", n=5):\n",
    "    \"\"\" show extreme name frames by frame property \"\"\"\n",
    "    property_values = compute_property_values(scan_data, property_name)\n",
    "    show_images_by_property_values(property_values, 0, n)\n",
    "    show_images_by_property_values(property_values, len(property_values) - n, len(property_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd587961-c6c0-42e7-a481-9de7a612c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_extreme_property_values(scan_data, property_name=\"ratio\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869623d1-dca8-401f-9413-23022c724f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_extreme_property_values(scan_data, property_name=\"area\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a1417-4b16-475e-af62-94b95ba7a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data = pd.DataFrame([ data[1]for data in compute_property_values(scan_data, \"length\").values() ]).value_counts()\n",
    "plt.bar([x[0] for x in length_data.index], length_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9201bbe-7fa8-419a-a55d-c2d56ee9fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_data = pd.DataFrame([ data[1] for data in compute_property_values(scan_data, \"height\").values() ]).value_counts()\n",
    "plt.bar([x[0] for x in height_data.index], height_data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cae172-9fa4-48e2-b1ac-9a880f2754dd",
   "metadata": {},
   "source": [
    "## 7. Combine images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf918d42-6438-4c51-aab5-ee4190b6333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MAX_VALUE = 1600\n",
    "\n",
    "def fits_in_gap(gaps, combined_image, image):\n",
    "    \"\"\" find smallest gap at the end of a line where the name fits; return False otherwise \"\"\" \n",
    "    smallest_gap, smallest_i = sys.maxsize, -1\n",
    "    for i in range(0, len(gaps)):\n",
    "        x, y = gaps[i]\n",
    "        if x + image.size[0] <= X_MAX_VALUE and X_MAX_VALUE - x - image.size[0] < smallest_gap:\n",
    "            smallest_gap = X_MAX_VALUE - x + image.size[0]\n",
    "            smallest_i = i\n",
    "    if smallest_i >= 0:\n",
    "        return gaps[smallest_i][0], gaps[smallest_i][1], smallest_i\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4c2b4-3461-4ad5-bb76-6c6d7317b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_characters_per_name(deceased_names):\n",
    "    names_counter, characters_counter = (0, 0)\n",
    "    for scan in deceased_names:\n",
    "        for line in scan:\n",
    "            for name in line:\n",
    "                names_counter += 1\n",
    "                characters_counter += len(regex.sub(\"\\s\", \"\", name))\n",
    "    return round(characters_counter/names_counter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f157c6-5fdc-4491-adce-d23cab0d173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_combined_image_file_name(file_counter):\n",
    "    \"\"\" make the name of the images with combined names\"\"\"\n",
    "    return f\"combined_images/{file_counter.zfill(4)}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba579ac-e84d-4b86-b015-71e674dcaf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_combine_images(deceased_names, years, file_counter):\n",
    "    deceased_names.append([[]])\n",
    "    years.append([[]])\n",
    "    file_counter += 1\n",
    "    return 0, 0, [], file_counter, deceased_names, years, Image.new( \"RGB\", (1600, 1200), (255, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb1cbd-7152-4473-b374-d2e54b008b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(scan_data, patterns):\n",
    "    \"\"\" combine as many as possible name frames in several images \"\"\" \n",
    "    x, y, gaps, file_counter, deceased_names, years, combined_image = initialize_combine_images([], [], 0)\n",
    "    for patterns_i in range(0, len(patterns)):\n",
    "        pattern = patterns[patterns_i]\n",
    "        for coordinates_file_name in dict(sorted(scan_data.items(), \n",
    "                                                 key=lambda scan_data_item: scan_data_item[1][0])): # index value\n",
    "            if regex.search(pattern, coordinates_file_name):\n",
    "                index, status, text_line_id, coords_id, ip_address, text_line_name = scan_data[coordinates_file_name]\n",
    "                if status ==\"save\":\n",
    "                    try:\n",
    "                        image_file_name = \"images/\" + os.path.basename(make_image_file_name(coordinates_file_name))\n",
    "                        image = Image.open(image_file_name)\n",
    "                    except Exception:\n",
    "                        print(f\"problem processing file {image_file_name}\")\n",
    "                        continue\n",
    "                    if fits_in_gap(gaps, combined_image, image):\n",
    "                        x_gap, y_gap, i_gap = fits_in_gap(gaps, combined_image, image)\n",
    "                        combined_image.paste(image, (x_gap, y_gap + int((100 - image.size[1]) / 2)))\n",
    "                        deceased_names[-1][int(0.5 + y_gap/100)].append(logfile_data.iloc[index][4])\n",
    "                        years[-1][int(0.5 + y_gap/100)].append(coordinates_file_name.split()[1])\n",
    "                        x_gap += image.size[0] + 30\n",
    "                        gaps = gaps[:i_gap] + [[x_gap, y_gap]] + gaps[i_gap+1:]\n",
    "                        continue\n",
    "                    elif x + image.size[0] <= X_MAX_VALUE:\n",
    "                       pass\n",
    "                    elif y < 1100:\n",
    "                        gaps.append([x, y])\n",
    "                        x = 0\n",
    "                        y += 100\n",
    "                        deceased_names[-1].append([])\n",
    "                        years[-1].append([])\n",
    "                    else:\n",
    "                        combined_image.save(make_combined_image_file_name(str(file_counter)))\n",
    "                        x, y, gaps, file_counter, deceased_names, years, combined_image = initialize_combine_images(deceased_names, years, file_counter)\n",
    "                        combined_image = Image.new( \"RGB\", (1600, 1200), (255, 255, 255))\n",
    "                    combined_image.paste(image, (x, y + int((100 - image.size[1]) / 2)))\n",
    "                    deceased_names[-1][-1].append(logfile_data.iloc[index][4])\n",
    "                    years[-1][-1].append(coordinates_file_name.split()[1])\n",
    "                    x += image.size[0] + 30\n",
    "        if (x > 0 or y > 0):\n",
    "            combined_image.save(make_combined_image_file_name(str(file_counter)))\n",
    "            if patterns_i < len(patterns) - 1:\n",
    "                x, y, gaps, file_counter, deceased_names, years, combined_image = initialize_combine_images(deceased_names, years, file_counter)\n",
    "                combined_image = Image.new( \"RGB\", (1600, 1200), (255, 255, 255))\n",
    "    print(f\"created {file_counter} combined image files; showing final file below:\")\n",
    "    display(combined_image)\n",
    "    return deceased_names, years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e5a36-e1e5-40d6-a603-02f366b887d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [ \"183[0-9]\", \"184[0-9]\", \"192[0-9]\" ]\n",
    "\n",
    "deceased_names, years = combine_images(scan_data, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be18ce6-d273-4fa7-a0df-12b14fc5108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"metadata sections: {len(deceased_names)}\\nfinal metadata section: {deceased_names[-1]}\\n{years[-1][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1438a-2f36-49c0-b657-cda77d5b5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_characters_per_name(deceased_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906e942-85be-48ce-b6e6-21c97b4aa65e",
   "metadata": {},
   "source": [
    "## 8. Check combined images\n",
    "\n",
    "20231107 corrected names:\n",
    "* page 49 corrected sveral\n",
    "* page 130 bernardo wever\n",
    "* page 130 Fabias Sebastiaan Josephina (removed)\n",
    "* page 171 last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c190bdd-d11d-4a21-9c4e-fffd50fe7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_name_in_deceased_names_pages(deceased_names, name):\n",
    "    for page in deceased_names:\n",
    "        for line in page:\n",
    "            for line_name in line:\n",
    "                if regex.search(name, line_name, regex.IGNORECASE):\n",
    "                    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f13c4b-2b13-4260-b774-83ec60585c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_name_in_logfile_data(logfile_data, name):\n",
    "    return logfile_data.loc[logfile_data[4] == name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3db7d-0226-4a79-a78c-4d13c528bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_name_in_deceased_names_pages(deceased_names, \"francisco martelij\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9dd8fd-f915-440d-882f-f7617c401f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_name_in_logfile_data(logfile_data, \"Englentina Martina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8de2e2-7cce-43e8-a4b8-611bf183892a",
   "metadata": {},
   "source": [
    "## 9 Add names to checked baseline generated by Transkribus\n",
    "\n",
    "The baselines are computed by Transkribus under Tools / P2PaLA with the Public Model Spruchakten_3_blonly-2019-11-07 14:09:29 19746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216ebf2-9e80-4217-a98b-7d5e962e1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_name_case(name):\n",
    "    \"\"\" convert ALLCAPS name words to words with only capitalized first character \"\"\"\n",
    "    names = name.split()\n",
    "    for i in range(0, len(names)):\n",
    "        if regex.search(\"^[A-Z]+$\", names[i]):\n",
    "            names[i] = names[i][0] + names[i][1:].lower()\n",
    "    return \" \".join(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab66b6-81de-441e-bf41-2ad3ce5a8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_element(element_tag, text):\n",
    "    \"\"\" add gold standard text to TextLine element in Transkribus xml file \"\"\"\n",
    "    textequiv_tag = ET.Element(\"TextEquiv\")\n",
    "    unicode_tag = ET.Element(\"Unicode\")\n",
    "    unicode_tag.text = text\n",
    "    textequiv_tag.append(unicode_tag)\n",
    "    element_tag.append(textequiv_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34826102-7f41-4444-9f1c-bf234ed16715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_textequiv_elements(root):\n",
    "    \"\"\" remove all TextEquiv elements with text from a Transkribus xml file \"\"\"\n",
    "    for textline in root.findall(\".//{*}TextLine\"):\n",
    "        for child in textline:\n",
    "            if regex.sub(\"{.*}\", \"\", child.tag) == \"TextEquiv\":\n",
    "                textline.remove(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc3dea-f5e3-4ed9-89f3-5078e4188c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_DIR = \"combined_images/page\"\n",
    "NAMESPACE = \"http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15\"\n",
    "MAX_NBR_OF_LINES_PER_FILE = 12\n",
    "\n",
    "def add_ground_truth_text_to_transkribus_xml(transkribus_xml_dir, deceased_names):\n",
    "    \"\"\" add ground truth text to Transkribus xml files, first removes previous texts from the files \"\"\"\n",
    "    ET.register_namespace(\"\", NAMESPACE)\n",
    "    file_index = 0\n",
    "    for file_name in sorted(os.listdir(transkribus_xml_dir)):\n",
    "        squeal(file_name)\n",
    "        tree = ET.parse(os.path.join(transkribus_xml_dir, file_name))\n",
    "        root = tree.getroot()\n",
    "        delete_textequiv_elements(root)\n",
    "        line_index = 0\n",
    "        for textline_tag in root.findall(\".//{*}TextLine\"):\n",
    "            for textequiv_tag in textline_tag.findall(\".//{*}TextEquiv\"):\n",
    "                textline_tag.remove(textequiv_tag)\n",
    "            add_text_to_element(textline_tag, fix_name_case(\" \".join(deceased_names[file_index][line_index])))\n",
    "            line_index += 1\n",
    "        tree.write(os.path.join(transkribus_xml_dir, file_name))\n",
    "        if line_index != MAX_NBR_OF_LINES_PER_FILE:\n",
    "            print(f\"warning: incomplete file: {file_name}; file_index: {file_index}; number of lines: {line_index}; final line: {deceased_names[file_index][line_index-1]}\")\n",
    "        file_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a37986-cc83-41cb-95bb-08153c5535c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    add_ground_truth_text_to_transkribus_xml(FILE_DIR, deceased_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114ca17-49b8-4c41-b6d8-19d91233aede",
   "metadata": {},
   "source": [
    "## 10. Check processed validation data of training run\n",
    "\n",
    "Training is done with Transkribus / Tools / Train a new model, with options \n",
    "* Automatic selection of validation set: 10% from train set\n",
    "* Use existing polygons for training\n",
    "* Base model: HTR-Curacao_bestModel\n",
    "* Language: Dutch; Flemish (nld)\n",
    "* Max-nr. of Epochs: 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201b415-5242-4e10-b864-969cebe5618f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_split_ids(name_list):\n",
    "    \"\"\" return total number of words in the names up until the current name \"\"\" \n",
    "    split_id = 0\n",
    "    word_split_ids = []\n",
    "    for name in name_list:\n",
    "        word_split_ids.append(split_id)\n",
    "        split_id += len(name.split())\n",
    "    return word_split_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abca5e-0228-45fe-b57a-bf06d1831806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line_in_names(guessed_line, correct_names):\n",
    "    \"\"\" split guessed line in names based on espected names; fails if the number of words differs \"\"\"\n",
    "    guessed_words = guessed_line.split()\n",
    "    word_split_ids = get_word_split_ids(correct_names)\n",
    "    guessed_names = []\n",
    "    for word_split_ids_id in range(0, len(word_split_ids)):\n",
    "        if word_split_ids_id < len(word_split_ids) - 1:\n",
    "            guessed_names.append(\" \".join(guessed_words[word_split_ids[word_split_ids_id]: word_split_ids[word_split_ids_id + 1]]))\n",
    "        else:\n",
    "            guessed_names.append(\" \".join(guessed_words[word_split_ids[word_split_ids_id]:]))\n",
    "    return guessed_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0bb9af-a742-497e-bb1d-b2e20eba592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_names(guessed_name, correct_name, correct_words_count, total_words_count, correct_names_count):\n",
    "    \"\"\" compare guessed names with expected correct names \"\"\"\n",
    "    guessed_words = guessed_name.split()\n",
    "    correct_words = correct_name.split()\n",
    "    correct_words_count_start = correct_words_count\n",
    "    for guessed_words_id in range(0, len(guessed_words)):\n",
    "        total_words_count += 1\n",
    "        try:\n",
    "            if guessed_words[guessed_words_id].lower() == correct_words[guessed_words_id].lower():\n",
    "                correct_words_count += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    if guessed_name.lower() == correct_name.lower():\n",
    "        correct_names_count += 1\n",
    "    else:\n",
    "        print(correct_words_count - correct_words_count_start, \"#\", guessed_name, \"#\", correct_name)\n",
    "    return correct_words_count, total_words_count, correct_names_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f566069-db3c-480b-b075-53e9da15614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lines_via_names(guessed_line, correct_names):\n",
    "    \"\"\" compare guessed lines with expected lines by splitting them in names first: fails when the number of words differ \"\"\" \n",
    "    guessed_names = split_line_in_names(guessed_line, correct_names)\n",
    "    correct_words_count, total_words_count, correct_names_count, total_names_count = 0, 0, 0, 0\n",
    "    for guessed_names_id in range(0, len(guessed_names)):\n",
    "        total_names_count += 1\n",
    "        correct_words_count, total_words_count, correct_names_count = compare_names(guessed_names[guessed_names_id], \n",
    "                                                                                    correct_names[guessed_names_id], \n",
    "                                                                                    correct_words_count, \n",
    "                                                                                    total_words_count, \n",
    "                                                                                    correct_names_count)\n",
    "\n",
    "    return correct_words_count, total_words_count, correct_names_count, total_names_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f99a2-4409-45d7-b672-153cc75f68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_dict(line):\n",
    "    \"\"\" return dict with frequencies of lower-cased words on line \"\"\"\n",
    "    word_dict = {}\n",
    "    for word in line.split():\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] += 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211842e-63cc-4382-a956-b0af4117543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_missed_names(missed_names, guessed_words, file_id, line_id):\n",
    "    \"\"\" show the missed words in guessed names with file_id, line_id and non-matched strings \"\"\"\n",
    "    for name in missed_names:\n",
    "        print(file_id, line_id, end=\" \")\n",
    "        for name_part in name.split():\n",
    "            if name_part.lower() in guessed_words:\n",
    "                print(name_part, end=\" \")\n",
    "            else:\n",
    "                print_with_color(name_part)\n",
    "                print(\"\", end=\" \")\n",
    "        print([ word for word in guessed_words if guessed_words[word] > 0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e7deb-b3a7-4e4a-9c49-4451e030c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lines_via_words(guessed_line, correct_names, years_line):\n",
    "    \"\"\" compare lines by comparing individual words: does not respect word order \"\"\"\n",
    "    guessed_words = get_word_dict(guessed_line.lower())\n",
    "    correct_words_count, total_words_count, correct_names_count, total_names_count = 0, 0, 0, 0\n",
    "    missed_names = []\n",
    "    years_eval_line = {}\n",
    "    for i in range(0, len(correct_names)):\n",
    "        correct_name = correct_names[i]\n",
    "        year = years_line[i]\n",
    "        if year not in years_eval_line:\n",
    "            years_eval_line[year] = { \"correct\": 0, \"wrong\": 0 }\n",
    "        total_names_count += 1\n",
    "        guessed_name_is_correct = True\n",
    "        for correct_word in correct_name.lower().split():\n",
    "            total_words_count += 1\n",
    "            if correct_word not in guessed_words or guessed_words[correct_word] <= 0:\n",
    "                guessed_name_is_correct = False\n",
    "            else:\n",
    "                correct_words_count += 1\n",
    "                guessed_words[correct_word] -= 1\n",
    "        if guessed_name_is_correct:\n",
    "            correct_names_count += 1\n",
    "            years_eval_line[year][\"correct\"] += 1\n",
    "        else:\n",
    "            missed_names.append(correct_name)\n",
    "            years_eval_line[year][\"wrong\"] += 1\n",
    "    return correct_words_count, total_words_count, correct_names_count, total_names_count, missed_names, guessed_words, years_eval_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9dc050-1ba3-4e06-b4b1-1dc638489d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_id(root):\n",
    "    \"\"\" get the id of the xml document from the Transkribus metadata \"\"\" \n",
    "    return root.findall(\".//{*}TranskribusMetadata\")[0].attrib[\"docId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795cee8-1541-4462-8fe2-ff96a4331057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_stamp(root):\n",
    "    \"\"\" get the time stamp of the file from the tag LastChange \"\"\"\n",
    "    time_stamp = root.findall(\".//{*}LastChange\")[0].text\n",
    "    return regex.sub(\"\\..*$\", \"\", time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597d9d6-500f-4aeb-87d0-c4986790e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_export_job_id(validation_file_dir):\n",
    "    \"\"\" get the export job id from file log.txt \"\"\"\n",
    "    in_file = open(os.path.join(validation_file_dir, \"../../../log.txt\"), \"r\")\n",
    "    line = in_file.readline().strip()\n",
    "    in_file.close()\n",
    "    return regex.sub(\"^.* \", \"\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4eec5f-7892-45cf-892a-352de1a81964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_years_eval_file(years_eval_file, years_eval_line):\n",
    "    \"\"\" insert line scores per year in file scores per year variable \"\"\"\n",
    "    for year in years_eval_line:\n",
    "        if year not in years_eval_file:\n",
    "            years_eval_file[year] = { \"correct\": 0, \"wrong\": 0 }\n",
    "        years_eval_file[year][\"correct\"] += years_eval_line[year][\"correct\"]\n",
    "        years_eval_file[year][\"wrong\"] += years_eval_line[year][\"wrong\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38b0c6-6969-433a-b088-a5a0254416db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decade_scores(years_eval_file):\n",
    "    \"\"\" convert scores per year to scores per decade \"\"\"\n",
    "    decade_scores = {}\n",
    "    for year in years_eval_file:\n",
    "        decade = regex.sub(\".$\", \"\", year) + \"0s\"\n",
    "        if decade not in decade_scores:\n",
    "            decade_scores[decade] = { \"correct\": 0, \"wrong\": 0 }\n",
    "        decade_scores[decade][\"correct\"] += years_eval_file[year][\"correct\"]\n",
    "        decade_scores[decade][\"wrong\"] += years_eval_file[year][\"wrong\"]\n",
    "    return { decade: [ round(100 * decade_scores[decade][\"correct\"] / \n",
    "                           (decade_scores[decade][\"correct\"] + decade_scores[decade][\"wrong\"]), 1),\n",
    "                       decade_scores[decade][\"correct\"] + decade_scores[decade][\"wrong\"] ]\n",
    "             for decade in decade_scores }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06086b-7673-4bba-b3ef-1dd45b858cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_data(validation_file_dir, deceased_names, years, known_min_freq=1, unknown_min_freq=2, fix_lines=False):\n",
    "    \"\"\" evaluate the validation data generated by Transkribus; this version is word-order insensitive \"\"\"\n",
    "    correct_words_file, total_words_file , correct_names_file, total_names_file = 0, 0, 0, 0\n",
    "    years_eval_file = {}\n",
    "    for file_name in sorted(os.listdir(VALIDATION_FILE_DIR)):\n",
    "        file_id = int(regex.sub(\".xml$\", \"\", file_name)) - 1\n",
    "        tree = ET.parse(os.path.join(VALIDATION_FILE_DIR, file_name))\n",
    "        root = tree.getroot()\n",
    "        line_id = 0\n",
    "        for unicode_tag in root.findall(\".//{*}TextLine/{*}TextEquiv/{*}Unicode\"):\n",
    "            name_line = unicode_tag.text\n",
    "            if type(name_line) != str:\n",
    "                name_line = \"\"\n",
    "            if fix_lines:\n",
    "                name_line = fix_name_line(name_line, known_min_freq, unknown_min_freq)\n",
    "            (correct_words_line, \n",
    "             total_words_line, \n",
    "             correct_names_line, \n",
    "             total_names_line, \n",
    "             missed_names, \n",
    "             guessed_words,\n",
    "             years_eval_line) = compare_lines_via_words(name_line, \n",
    "                                                        deceased_names[file_id][line_id],\n",
    "                                                        years[file_id][line_id])\n",
    "            update_years_eval_file(years_eval_file, years_eval_line)\n",
    "            correct_words_file += correct_words_line\n",
    "            total_words_file += total_words_line\n",
    "            correct_names_file += correct_names_line\n",
    "            total_names_file += total_names_line\n",
    "            line_id += 1\n",
    "\n",
    "    words_correct_percentage = round(100 * correct_words_file/total_words_file, 1)\n",
    "    names_correct_percentage = round((100 * correct_names_file / total_names_file), 1)\n",
    "    decade_scores = make_decade_scores(years_eval_file)\n",
    "    return(get_export_job_id(validation_file_dir),\n",
    "           get_doc_id(root),\n",
    "           get_time_stamp(root), \n",
    "           { \"words_acc\": words_correct_percentage, \"names_acc\": names_correct_percentage }, \n",
    "           decade_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c977acb-8401-48c5-8f3f-441b696d71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_FILE_DIR = \"tmp/1619947/TRAINING_VALIDATION_SET_combined_images_with_ground_truth_(4250)/page/\" \n",
    "# Transkribus: CER: 20.3%; WER: 41.4%; time: 2:02h; epoch: 250\n",
    "VALIDATION_FILE_DIR = \"tmp/1625029/combined_images_validation/page\"\n",
    "# Transkribus: model combined images with ground truth (4250); use existing line polygons; Language model from training data\n",
    "VALIDATION_FILE_DIR = \"tmp/1632818/combined_images_validation_5258/page\"\n",
    "# 20231113 combined images (6750) 1830s 1840s runs: 150, 300, 550\n",
    "VALIDATION_FILE_DIR = \"tmp/1652679/combined_images_valudation_6750/page\"\n",
    "# 20231120 1830s 250 runs\n",
    "VALIDATION_FILE_DIR = \"tmp/1664712/TRAINING_VALIDATION_SET_combined_images_1830s/page\"\n",
    "# 20231120 1840s 250 runs\n",
    "VALIDATION_FILE_DIR = \"tmp/1665816/TRAINING_VALIDATION_SET_combined_images_1840s/page\"\n",
    "# 20231120 1920s 250 runs\n",
    "VALIDATION_FILE_DIR = \"tmp/1666399/TRAINING_VALIDATION_SET_combined_images_1920s/page\"\n",
    "# 20231120 1920s 250 runs; batch size 6\n",
    "VALIDATION_FILE_DIR = \"tmp/1667139/TRAINING_VALIDATION_SET_combined_images_1920s_batch_size_6/page\"\n",
    "# 20231120 1920s 250 runs; batch size 6\n",
    "VALIDATION_FILE_DIR = \"tmp/1667611/TRAINING_VALIDATION_SET_combined_images_(7513)/page\"\n",
    "\n",
    "evaluate_validation_data(VALIDATION_FILE_DIR, deceased_names, years, fix_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1041c97-153b-4e35-b1d2-3d290846e42b",
   "metadata": {},
   "source": [
    "| **HTR model** | **job id** | **doc id** | **date** | **name accuracy** | **1830s** | **1840s** | **1920s** | **post-processed** | **1830s** | **1840s** | **1920s** | TPR | missed | cutoff | |\n",
    "| ------------- | ---------- | ---------- | -------- | :---------------: | :----------------: | :---: | :---: | :---: | :---: | :---: | :---: | :-: | :---: | :---: | :-: |\n",
    "| Model 4250 from Dutchess | 6699433 | 1625029 | 2023-10-24T14:40:45 | - | ||| - | ||| |||\n",
    "| HTR_Curacao_best_model (HCbm) | 6699264 | 1625029 | 2023-10-24T14:18:33 | 11.1% | ||| 16.0% | ||| |||\n",
    "| The Dutchess I | 6699351 | 1625029 | 2023-10-24T14:30:39 | 26.0% | ||| 29.5% | ||| |||\n",
    "| Model 4250 from HCbm | 6699155 | 1625029 | 2023-10-24T14:07:52 | 53.1% | ||| 53.8% | ||| |||\n",
    "| Model 5258 from HCbm | 6748708 | 1632818 | 2023-10-31T09:27:16 | 29.1% | 26.8% | 35.7% | 8.3% | 38.8% | 36.2% | 44.2% | 22.9% | |||\n",
    "| Model 6750 from HCbm 150 epochs | 6879183 | 1652679 | 2023-11-13T09:45:48 | 46.0% | 41.9% | 48.5% | --- | 46.0% |  44.1% | 47.2% | --- | |||\n",
    "| Model 6750 from HCbm 300 epochs | 6879187 | 1652679 | 2023-11-13T09:46:15 | 52.9% | 49.3% | 55.0% | --- | 51.8% |  52.2% | 51.5% | --- | |||\n",
    "| Model 6750 from HCbm 550 epochs | 6879193 | 1652679 | 2023-11-13T09:48:13 | 47.1% | 41.9% | 50.2% | --- | 48.5% |  47.8% | 48.9% | --- | |||\n",
    "| 1830s from HCbm 250 epochs | 6973533 | 1664712 | 2023-11-19T17:16:01 | --- | 55.3% | --- | --- | --- | 58.7% | --- | --- | 94% | 6% | 13 |\n",
    "| 1840s from HCbm 250 epochs | 6974337 | 1665816 | 2023-11-20T11:39:33 | --- | --- | 63.7% | --- | --- | --- | 66.0% | --- | 95% | 3% | 3 |\n",
    "| 1920s from HCbm 250 epochs | 6982409 | 1666399 | 2023-11-20T16:12:18 | --- | --- | --- | 7.2% | --- | --- | --- | 10.2% | 71% | 29% | 220 |\n",
    "| as above with batch size 6 | 6985194 | 1667139 | 2023-11-20T20:08:43 | --- | --- | --- | 11.4% | --- | --- | --- | 13.9% | 72% | 28% | 160 |\n",
    "| 3 decades from HCbm 250 epochs | 6985330 | 1667611 | 2023-11-20T20:32:43 | 59.1% | 63.3% | 69.3% | 42.2% | 59.9% | 61.3% | 70.2 | 45.2% | 94% | 6% | 8 | 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58116814-595f-45c0-a7fa-53b3f459d4bc",
   "metadata": {},
   "source": [
    "## 11. Postprocessing names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3589f1f-294a-4813-90f8-75234ab3856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_name(name_in):\n",
    "    for phrase in [ \"\\(a\\)\", \n",
    "                    \"zich noemende en schrijfende*\",\n",
    "                    \"zich noemende en schrijvende*\",\n",
    "                    \"zich noemende en schryfende*\",\n",
    "                    \"zich noemende en schryvende*\",\n",
    "                    \"zich noemende en schrijfende*\",\n",
    "                    \"zich ook noemende en schrijvende*\",\n",
    "                    \"zich ook noemende en schryfende*\",\n",
    "                    \"zich ook noemende en schryvende*\",\n",
    "                    \"zich noemende en teekenende*\",\n",
    "                    \"zich ook noemende en teekenende*\",\n",
    "                    \"zich ook noemende*\",\n",
    "                    \"zich noemde*\",\n",
    "                    \"zich noemende*\",\n",
    "                    \"zich teekenende*\",\n",
    "                    \"zich ook schrijfende*\",\n",
    "                    \"zich ook schrijvende*\",\n",
    "                    \"zich ook schryfende*\",\n",
    "                    \"zich ook schryvende*\",\n",
    "                    \"zich schrijfende*\",\n",
    "                    \"zich schrijvende*\",\n",
    "                    \"zich schryfende*\",\n",
    "                    \"zich schryvende*\",\n",
    "                    \"zich ook te noemen\",\n",
    "                    \"z\\.n\\.e\\.s\\.\",\n",
    "                    \"z\\.n\\.e\\.t\\.\",\n",
    "                    \"z\\.n\\.\",\n",
    "                    \"z\\.t\\.\",\n",
    "                    \"z\\. t\\.\",\n",
    "                    \"ook genaamd\",\n",
    "                    \"ook gen\\.\",\n",
    "                    \"of ook wel\", \n",
    "                    \"of ook\",\n",
    "                    \"ook\",\n",
    "                    \"alias\",\n",
    "                    \"o\\.g\\.\"\n",
    "                    \"genaamd\",\n",
    "                    \"door de wandeling\", ]:\n",
    "        if regex.search(f\" {phrase} \", name_in):\n",
    "            split_names = regex.split(f\" {phrase} \", name_in)\n",
    "            names_out = []\n",
    "            for name in split_names:\n",
    "                split_name_parts = split_name(name)\n",
    "                if len(split_name_parts) > 1:\n",
    "                    names_out.extend(split_name_parts)\n",
    "                else:\n",
    "                    names_out.append(name)\n",
    "            return names_out\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291da43-39a2-4c07-b4a2-da31faf0d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_names(known_names):\n",
    "    for row_id, row in known_names.iterrows():\n",
    "        split_names = split_name(row[\"Namen\"])\n",
    "        if len(split_names) > 0:\n",
    "            known_names.iloc[row_id] = \"\"\n",
    "            for name in split_names:\n",
    "                known_names.loc[len(known_names)] = name\n",
    "        if 1000 * int(row_id / 1000) == row_id:\n",
    "            squeal(f\"{row_id}/{len(known_names)} (split_names)\")\n",
    "    return known_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02d8cd-0f77-4979-9131-6da0dbb4cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower_names(known_names):\n",
    "    for row_id, row in known_names.iterrows():\n",
    "        name_words = row[\"Namen\"].split()\n",
    "        for i in range(0, len(name_words)):\n",
    "            if regex.search(\"[A-Z][A-Z]+\", name_words[i]) and not regex.search(\"[a-z]\", name_words[i]):\n",
    "                name_words[i] = name_words[i][0] + name_words[i][1:].lower()\n",
    "        new_name = \" \".join(name_words)\n",
    "        if new_name != row[\"Namen\"]:\n",
    "            known_names.iloc[row_id] = \" \".join(name_words)\n",
    "        if 1000 * int(row_id / 1000) == row_id:\n",
    "            squeal(f\"{row_id}/{len(known_names)} (to_lower_names)\")\n",
    "    return known_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ef1ed-7089-4aee-8e14-99a5f98c9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFIX_TOKENS = \"@ d. da de del dela den der di don dos du el la las le lo los 's san santa sint st st. 't t te ten ter v. v.d. van vander von\".split() \n",
    "\n",
    "def move_infix_tokens(known_names):\n",
    "    for row_id, row in known_names.iterrows():\n",
    "        if regex.search(\",\",  row[\"Namen\"]):\n",
    "            if not regex.search(\",.*,\",  row[\"Namen\"]):\n",
    "                prefix, suffix = regex.split(\" *, *\", row[\"Namen\"])\n",
    "                suffix_tokens = suffix.split()\n",
    "                only_known_infix_tokens = True\n",
    "                for suffix_token in suffix_tokens:\n",
    "                    if suffix_token.lower() not in INFIX_TOKENS:\n",
    "                        only_known_infix_tokens = False\n",
    "                        break\n",
    "                if only_known_infix_tokens:\n",
    "                    prefix_tokens = prefix.split()\n",
    "                    results = prefix_tokens[:-1]\n",
    "                    results.extend(suffix_tokens)\n",
    "                    results.extend([prefix_tokens[-1]])\n",
    "                    known_names.loc[row_id] = \" \".join(results)\n",
    "        if 1000 * int(row_id / 1000) == row_id:\n",
    "            squeal(f\"{row_id}/{len(known_names)} (move_infix_tokens)\")\n",
    "    return known_names                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b27115-9d14-44c3-8313-3c1b25850768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_of(known_names):\n",
    "    for row_id, row in known_names.iterrows():\n",
    "        if regex.search(\" of \", row[\"Namen\"]):\n",
    "            name_words = row[\"Namen\"].split()\n",
    "            name_words_id = 0\n",
    "            while name_words[name_words_id] != \"of\":\n",
    "                name_words_id += 1\n",
    "            known_names.iloc[row_id] = \" \".join(name_words[:name_words_id] + name_words[name_words_id+2:])\n",
    "            known_names.loc[len(known_names)] = \" \".join(name_words[:name_words_id-1] + name_words[name_words_id+1:])\n",
    "            out_file = open(\"tmp_of\", \"a\")\n",
    "            print(known_names.iloc[row_id][\"Namen\"], file=out_file)\n",
    "            print(known_names.iloc[len(known_names)-1][\"Namen\"], file=out_file)\n",
    "            out_file.close()\n",
    "        if 1000 * int(row_id / 1000) == row_id:\n",
    "            squeal(f\"{row_id}/{len(known_names)} (fix_of)\")\n",
    "    squeal(f\"{row_id}/{len(known_names)} (fix_of)\")\n",
    "    return known_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664d8ec-873f-425a-8a02-0f6020b61b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_NAMES_FILE = \"../../data/Overlijden/x-misc/Namen en beroepen Curacao.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19ae38-78a6-4106-9114-530eabd462fc",
   "metadata": {},
   "source": [
    "Cleaning the names takes a lot of time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1414bcf-03cb-49f6-9b50-8391899c37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names = pd.read_csv(KNOWN_NAMES_FILE)\n",
    "known_names = split_names(known_names)\n",
    "known_names = to_lower_names(known_names)\n",
    "known_names = move_infix_tokens(known_names)\n",
    "known_names = fix_of(known_names)\n",
    "known_names = known_names[known_names.Namen != \"\"].drop_duplicates().sort_values(by=\"Namen\")\n",
    "known_names.to_csv(\"tmp.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f6f5e-8cf8-489b-8449-d9b00f67fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(name_list):\n",
    "    \"\"\" count the words/name parts in the name list \"\"\"\n",
    "    word_freqs = {}\n",
    "    for name in name_list:\n",
    "        for word in name.lower().split():\n",
    "            if word not in word_freqs:\n",
    "                word_freqs[word] = 1\n",
    "            else:\n",
    "                word_freqs[word] += 1\n",
    "    return word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1de6fa-37b7-4fca-b872-ff93cc643b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_names(guessed_name, word_freqs, unknown_min_freq):\n",
    "    \"\"\" return the names with the smallest Levenshtein distance, sort by frequency \"\"\"\n",
    "    best_distance = sys.maxsize\n",
    "    best_words = []\n",
    "    guessed_name_lower = guessed_name.lower()\n",
    "    for word in [ word for word in word_freqs if word_freqs[word] >= unknown_min_freq ]:\n",
    "        word_distance = distance(word, guessed_name_lower)\n",
    "        if word_distance < best_distance:\n",
    "            best_distance = word_distance\n",
    "            best_words = [ word ]\n",
    "        elif word_distance == best_distance:\n",
    "            best_words.append(word)\n",
    "    return best_distance, sorted(best_words, key=lambda word: word_freqs[word], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3418f-d3be-4e41-bfff-0d66b6367f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_name_line(name_line, known_min_freq=10, unknown_min_freq=10):\n",
    "    \"\"\" replace infrequent name parts on the line by close frequent name parts \"\"\"\n",
    "    name_words_in = name_line.lower().split()\n",
    "    name_words_out = []\n",
    "    for name_word in name_words_in:\n",
    "        if name_word in word_freqs and word_freqs[name_word] >= known_min_freq:\n",
    "            name_words_out.append(name_word)\n",
    "        else:\n",
    "            name_words_out.append(get_most_similar_names(name_word, word_freqs, unknown_min_freq)[1][0])\n",
    "    return \" \".join(name_words_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8e8a3-5781-40f8-adaa-06d506809f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_min_freqs(known_min_freq, unknown_min_freq, step_size, target=1):\n",
    "    \"\"\" optimize the threshold frequencies for known name parts and replacing name parts \"\"\"\n",
    "    best_freq = evaluate_validation_data(VALIDATION_FILE_DIR, \n",
    "                                         deceased_names,\n",
    "                                         years,\n",
    "                                         known_min_freq, \n",
    "                                         unknown_min_freq,\n",
    "                                         fix_lines=True)[target]\n",
    "    best_i, best_j = 0, 0\n",
    "    for i in [ -step_size, 0, step_size ]:\n",
    "        for j in [ -step_size, 0, step_size ]:\n",
    "            test_freq = evaluate_validation_data(VALIDATION_FILE_DIR, \n",
    "                                                 deceased_names,\n",
    "                                                 years,\n",
    "                                                 known_min_freq + i, \n",
    "                                                 unknown_min_freq +j,\n",
    "                                                 fix_lines=True)[target]\n",
    "            if test_freq  > best_freq:\n",
    "                best_freq, best_i, best_j = test_freq, i, j\n",
    "    if best_i != 0 or best_j != 0:\n",
    "        return optimize_min_freqs(known_min_freq + best_i, unknown_min_freq + best_j, step_size)\n",
    "    else:\n",
    "        return known_min_freq, unknown_min_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed877452-260d-4a17-b8aa-56e83f058a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = count_words(known_names[\"Namen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef56b2-c67d-4185-8309-7836874ccfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_min_freqs(1, 2, 1, target=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a8816-9658-4bc6-a733-2d4f912500e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_validation_data(VALIDATION_FILE_DIR, deceased_names, years, 1, 2, fix_lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db58d0f-f33e-4c01-959b-c9c93a33b0bf",
   "metadata": {},
   "source": [
    "## 12. Estimate which guessed words are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba61c6-e315-4d07-8ff2-596a72d1fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word_to_freq_dict(frequency_dict, word, count=1):\n",
    "    \"\"\" add word with frequency to frequency list \"\"\"\n",
    "    if word in frequency_dict:\n",
    "        frequency_dict[word] += count\n",
    "    else:\n",
    "        frequency_dict[word] = count\n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a043f8-1a24-42f3-a122-70c7ec6709c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_words(deceased_names, test_file_ids, cutoff=0):\n",
    "    \"\"\" alternative for get_known_names_words: return words appearing in names in train data, exclude test data \"\"\"\n",
    "    train_words = {}\n",
    "    for file_id in range(0, len(deceased_names)):\n",
    "        if file_id not in test_file_ids:\n",
    "            for line in deceased_names[file_id]:\n",
    "                for word in \" \".join(line).lower().split():\n",
    "                    add_word_to_freq_dict(train_words, word)\n",
    "    return { word: True for word in train_words if train_words[word] >= cutoff }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d2fe08-5c0f-4ea1-ba1a-2393a1c8bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_known_names_words(known_names, cutoff=0):\n",
    "    \"\"\" return words present in known names with a minimum frequency of \"cutoff\" \"\"\"\n",
    "    name_words = {}\n",
    "    for known_name in known_names[\"Namen\"]:\n",
    "        for name_word in known_name.lower().split():\n",
    "            add_word_to_freq_dict(name_words, name_word)\n",
    "    return {name_word: True for name_word in name_words if name_words[name_word] >= cutoff }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5d2ed-8143-48f2-a653-1108cb4ebb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_word_assessments(validation_file_dir):\n",
    "    \"\"\" count correct and wrong words in test files based on bag of words per line \"\"\"\n",
    "    correct_test_words, wrong_test_words, test_file_ids = {}, {}, []\n",
    "    for file_name in sorted(os.listdir(validation_file_dir)):\n",
    "        file_id = int(regex.sub(\".xml$\", \"\", file_name)) - 1\n",
    "        test_file_ids.append(file_id)\n",
    "        tree = ET.parse(os.path.join(validation_file_dir, file_name))\n",
    "        root = tree.getroot()\n",
    "        line_id = 0\n",
    "        for unicode_tag in root.findall(\".//{*}TextLine/{*}TextEquiv/{*}Unicode\"):\n",
    "            name_line = unicode_tag.text\n",
    "            if type(name_line) != str:\n",
    "                name_line = \"\"\n",
    "            guessed_words = get_word_dict(name_line.lower())\n",
    "            correct_words = get_word_dict(\" \".join(deceased_names[file_id][line_id]).lower())\n",
    "            for guessed_word in guessed_words:\n",
    "                if guessed_word in correct_words:\n",
    "                    add_word_to_freq_dict(correct_test_words, \n",
    "                                          guessed_word, \n",
    "                                          min(guessed_words[guessed_word], correct_words[guessed_word]))\n",
    "                    if guessed_words[guessed_word] > correct_words[guessed_word]:\n",
    "                        add_word_to_freq_dict(wrong_test_words, \n",
    "                                              guessed_word, \n",
    "                                              guessed_words[guessed_word] - correct_words[guessed_word])\n",
    "                else:\n",
    "                    add_word_to_freq_dict(wrong_test_words, guessed_word, guessed_words[guessed_word])\n",
    "            line_id += 1\n",
    "    return correct_test_words, wrong_test_words, test_file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93832f-ceb8-45d9-9584-87e7d9d0b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_assessments(validation_file_dir, deceased_names, known_names, cutoff=0):\n",
    "    \"\"\" check how often correct words appear in known words and not; do the same for wrong words \"\"\"\n",
    "    correct_test_words, wrong_test_words, test_file_ids = get_test_word_assessments(validation_file_dir)\n",
    "    known_words = get_known_names_words(known_names, cutoff)\n",
    "    correct_assessed_correctly, correct_assessed_wrongly = 0, 0\n",
    "    wrong_assessed_correctly, wrong_assessed_wrongly = 0, 0\n",
    "    for word in correct_test_words:\n",
    "        if word in known_words:\n",
    "            correct_assessed_correctly += correct_test_words[word]\n",
    "        else:\n",
    "            correct_assessed_wrongly += correct_test_words[word]\n",
    "    for word in wrong_test_words:\n",
    "        if word in known_words:\n",
    "            wrong_assessed_wrongly += wrong_test_words[word]\n",
    "        else:\n",
    "            wrong_assessed_correctly += wrong_test_words[word]\n",
    "    return correct_assessed_correctly, correct_assessed_wrongly, wrong_assessed_correctly, wrong_assessed_wrongly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e016732-bdb0-4354-aea2-8e540c334e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cutoff_values(max_cutoff):\n",
    "    \"\"\" evaluate different vocabulary frequency cutoff values \"\"\"\n",
    "    correct_accuracies, wrong_accuracies, cutoff = [], [], 0\n",
    "    while cutoff <= max_cutoff:\n",
    "        (correct_assessed_correctly, \n",
    "         correct_assessed_wrongly, \n",
    "         wrong_assessed_correctly, \n",
    "         wrong_assessed_wrongly) = count_assessments(VALIDATION_FILE_DIR, \n",
    "                                                     deceased_names, \n",
    "                                                     known_names,\n",
    "                                                     cutoff=cutoff)\n",
    "        correct_total = correct_assessed_correctly + correct_assessed_wrongly\n",
    "        wrong_total = wrong_assessed_correctly + wrong_assessed_wrongly\n",
    "        correct_accuracy = round(correct_assessed_correctly / correct_total, 3)\n",
    "        wrong_accuracy = round(wrong_assessed_correctly / wrong_total, 3)\n",
    "        correct_assessed_as_correct = round(correct_accuracy * correct_total)\n",
    "        correct_assessed_as_wrong = round((1 - correct_accuracy) * correct_total)\n",
    "        wrong_assessed_as_correct = round((1 - wrong_accuracy) * wrong_total)\n",
    "        wrong_assessed_as_wrong = round(wrong_accuracy * wrong_total)\n",
    "        percentage_false_positives = wrong_assessed_as_correct / ( wrong_assessed_as_correct + correct_assessed_as_correct )\n",
    "        percentage_missed = correct_assessed_as_wrong / ( correct_assessed_as_wrong + correct_assessed_as_correct )\n",
    "        print(f\"cutoff: {cutoff}; correct: accuracy: {correct_accuracy}, total: {correct_total}; wrong: accuracy: {wrong_accuracy}, total: {wrong_total};\",\n",
    "              f\"percentage false positives: {round(100 * percentage_false_positives,1)}%;\", \n",
    "              f\"percentage missed: {round(100 * percentage_missed, 1)}%\")\n",
    "        correct_accuracies.append(correct_accuracy)\n",
    "        wrong_accuracies.append(wrong_accuracy)\n",
    "        if cutoff < 20:\n",
    "            cutoff += 1\n",
    "        elif cutoff < 300:\n",
    "            cutoff += 10\n",
    "        else:\n",
    "            cutoff += 100\n",
    "    return correct_accuracies, wrong_accuracies, correct_assessed_as_correct, correct_assessed_as_wrong, wrong_assessed_as_correct, wrong_assessed_as_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee8007-baab-424e-81af-d8a422d26599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(correct_accuracies, wrong_accuracies):\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 5))\n",
    "    ax.set_title(\"correct_accuracies vs wrong_accuracies\")\n",
    "    ax.set_xlabel(\"correct_accuracies\")\n",
    "    ax.set_ylabel(\"wrong_accuracies\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    ax.plot(correct_accuracies, wrong_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7666b1-2850-4177-9904-929d7c2787c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(correct_accuracies, \n",
    " wrong_accuracies, \n",
    " correct_assessed_as_correct, \n",
    " correct_assessed_as_wrong, \n",
    " wrong_assessed_as_correct, \n",
    " wrong_assessed_as_wrong) = evaluate_cutoff_values(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670e199-abec-41d2-a3c3-e671700d4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(correct_accuracies, wrong_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a5295-f381-48b5-aedb-2cf4b2d821b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[correct_assessed_as_correct, correct_assessed_as_wrong, correct_assessed_as_correct + correct_assessed_as_wrong], \n",
    "              [wrong_assessed_as_correct, wrong_assessed_as_wrong, wrong_assessed_as_correct + wrong_assessed_as_wrong]], \n",
    "             columns=[\"assessed as correct\", \"assessed as wrong\", \"total\"], index=[\"correct\", \"wrong\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233a453-9466-4829-ab9c-4e792237f806",
   "metadata": {},
   "source": [
    "## 99. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02239fc-e45d-46d9-87ce-8ff80f788be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a84606-1f22-4ada-9cb8-58629111d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNotebook(unittest.TestCase):    \n",
    "    def test_split_line_in_names(self):\n",
    "        self.assertEqual(split_line_in_names(\"Jan Piet Klaas Marie Jozef Benjamin\", [\"A B C\", \"D\", \"E F\"]),\n",
    "                         ['Jan Piet Klaas', 'Marie', 'Jozef Benjamin'] )\n",
    "\n",
    "    def test_get_word_dict(self):\n",
    "        self.assertEqual(get_word_dict(\"Jan Piet Klaas Jan\"),\n",
    "                         { 'Jan': 2, \"Piet\": 1, \"Klaas\": 1 } )\n",
    "\n",
    "    def test_move_infix_tokens(self):\n",
    "        self.assertEqual(move_infix_tokens(pd.DataFrame([\"Jose Costa, da\"], columns=[\"Namen\"]))[\"Namen\"][0], \"Jose da Costa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8cfec-7170-4963-a677-5e5d78f3ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454ec26-a0ed-4e35-9abc-2a296333be49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
