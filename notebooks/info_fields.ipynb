{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9c3230-798b-43f4-a3f4-a96ad3f18dc2",
   "metadata": {},
   "source": [
    "# Info fields via regular expressions\n",
    "\n",
    "Extract persons from the info fields StartEntryInfo and EndEntryInfo of the [slave registers of Suriname](https://datasets.iisg.amsterdam/dataset.xhtml?persistentId=hdl:10622/CSPBHO) via regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50668a0b-3088-4428-b62b-31e21cf0624f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import regex\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "from scripts import get_deceased_name, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977e92a-a650-477a-8d20-9586820b8a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None:\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6b0d73-efb0-48a3-a874-88b5bde5afd2",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef938b3-c035-43c8-b692-00dd3d3609c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_FILE = \"../../data/suriname/Dataset Suriname Slave and Emancipation Registers Version 1.1.csv\"\n",
    "\n",
    "data = pd.read_csv(DATA_FILE, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73856dba-7646-4480-966c-d89d544b731c",
   "metadata": {},
   "source": [
    "## 2. Extract entities from info field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5d584-512b-4010-96c0-d8a874403049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_entities_by_type(entities):\n",
    "    \"\"\" select entities of a type (\"entity\") that may contain a person name \"\"\"\n",
    "    return [ entity for entity in entities \n",
    "                    if regex.search(\"(PERSON|ORG|GPE|FAC|NORP|WORK_OF_ART|EVENT|LOC)\",\n",
    "                                    entity[\"entity\"]) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee086448-69c3-457c-bc76-732209e3acee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_entities_from_data(data):\n",
    "    \"\"\" apply machine learning model to extract entities from string; only keep persons \"\"\"\n",
    "    entities = {}\n",
    "    nbr_of_entities = 0\n",
    "    for index, row in data.iterrows():\n",
    "        text = row[DATA_FIELD]\n",
    "        if isinstance(text, str):\n",
    "            entities[index] = select_entities_by_type(get_deceased_name.get_entities_from_text(text))\n",
    "            nbr_of_entities += len(entities[index])\n",
    "        if index % 100 == 0:\n",
    "            squeal(f\"total lines: {len(data)}; processed lines: {index}; found entities: {nbr_of_entities}\")\n",
    "        if CUT_OFF > 0 and nbr_of_entities >= CUT_OFF:\n",
    "            break\n",
    "    squeal(f\"total lines: {len(data)}; processed lines: {index}; found entities: {nbr_of_entities}\")\n",
    "    return entities, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf5729-0059-47b7-90f2-288a80539eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FIELD = \"EndEntryInfo\"\n",
    "INDEX_FIELD = \"Id_source\"\n",
    "CUT_OFF = 2000\n",
    "\n",
    "# entities, last_index = get_entities_from_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3276733b-4ad3-45d1-83e7-1a2c2da1b339",
   "metadata": {},
   "source": [
    "## 3. Inspect entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37651fef-88c6-4f70-8908-3f1d39716021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_entity_labels(entities):\n",
    "    \"\"\" add label field to entity, required for rendering \"\"\"\n",
    "    for entity in entities:\n",
    "        entity[\"label\"] = entity[\"entity\"]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282fefdc-584f-4dcb-92f9-76c04a9df678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inspect_entities(data, entities, first_index=0, last_index=-1):\n",
    "    \"\"\" show info texts with identified entities \"\"\"\n",
    "    for index, row in data.iterrows():\n",
    "        if first_index < 0 or index > first_index:\n",
    "            text = row[DATA_FIELD]\n",
    "            if index in entities:\n",
    "                print(index)\n",
    "                utils.render_text(text, entities[index])\n",
    "            elif isinstance(text, str):\n",
    "                utils.render_text(text,[])\n",
    "            if last_index >= 0 and index >= last_index:\n",
    "                break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad04d38-dfe9-437d-b7c3-19554e3b14be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inspect_entities(data, { index: add_entity_labels(entities[index]) for index in entities }, last_index=last_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287bc006-e00d-40fb-a2f5-66e617eddac3",
   "metadata": {},
   "source": [
    "## 4. Access context of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fda3c-e3a5-4d82-8b36-1f7cd657e07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_white_space(character):\n",
    "    \"\"\" check if single character contains white space \"\"\"\n",
    "    return regex.search(r\"^\\s$\", character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee273a1-f642-4736-8c5b-ee6d75aee234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_before_entity(text, entity):\n",
    "    \"\"\" get token before entity in text, with start position \"\"\"\n",
    "    previous_token = \"\"\n",
    "    end = entity[\"start\"]\n",
    "    while end > 0 and is_white_space(text[end-1]):\n",
    "        end -= 1\n",
    "    start = end - 1\n",
    "    while start > 0 and not is_white_space(text[start-1]):\n",
    "        start -= 1\n",
    "    if start >= 0:\n",
    "        previous_token = text[start: end]\n",
    "    return previous_token, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327cb9cb-f34e-47a8-a8bb-da1d90558a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_after_entity(text, entity):\n",
    "    \"\"\" get token after entity in text, with end position \"\"\"\n",
    "    next_token = \"\"\n",
    "    start = entity[\"end\"]\n",
    "    while start < len(text) and is_white_space(text[start]):\n",
    "        start += 1\n",
    "    end = start + 1\n",
    "    while end < len(text) and not is_white_space(text[end]):\n",
    "        end += 1\n",
    "    if start < len(text):\n",
    "        next_token = text[start: end]\n",
    "    return next_token, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6ca07-85ea-4ee4-98f5-8ca0f1658543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_first_token_of_entity(text, entity):\n",
    "    \"\"\" get first token of entity, with end position \"\"\"\n",
    "    first_token = \"\"\n",
    "    start = entity[\"start\"]\n",
    "    while start < entity[\"end\"] and is_white_space(text[start]):\n",
    "        start += 1\n",
    "    end = start + 1\n",
    "    while end < entity[\"end\"] and not is_white_space(text[end]):\n",
    "        end += 1\n",
    "    if start < entity[\"end\"]:\n",
    "        first_token = text[start: end]\n",
    "    return first_token, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312cf54-2e26-4066-a7c8-84f710a4372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_last_token_of_entity(text, entity):\n",
    "    \"\"\" get last token of entity, with start position \"\"\"\n",
    "    last_token = \"\"\n",
    "    end = entity[\"end\"]\n",
    "    while end > entity[\"start\"] and is_white_space(text[end - 1]):\n",
    "        end -= 1\n",
    "    start = end - 1\n",
    "    while start > entity[\"start\"] and not is_white_space(text[start - 1]):\n",
    "        start -= 1\n",
    "    if start >= entity[\"start\"]:\n",
    "        last_token = text[start: end]\n",
    "    return last_token, start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fb7c1-a39c-4f72-9a04-000e53d73021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_first_token_of_entity(text, entity):\n",
    "    \"\"\" remove first token of entity \"\"\"\n",
    "    first_token, end = get_first_token_of_entity(text , entity)\n",
    "    while end < entity[\"end\"] and is_white_space(text[end]):\n",
    "        end += 1\n",
    "    entity[\"start\"] = end\n",
    "    entity[\"word\"] = text[entity[\"start\"]: entity[\"end\"]]\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ebabc-1d30-4c22-91ab-f6ee5bb762dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_last_token_of_entity(text, entity):\n",
    "    \"\"\" remove last token of entity \"\"\"\n",
    "    last_token, start = get_last_token_of_entity(text , entity)\n",
    "    while start > entity[\"start\"] and is_white_space(text[start - 1]):\n",
    "        start -= 1\n",
    "    entity[\"end\"] = start\n",
    "    entity[\"word\"] = text[entity[\"start\"]: entity[\"end\"]]\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee01dc-420c-497b-9db4-483b50a5523d",
   "metadata": {},
   "source": [
    "## 5. Expand and shrink entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f56830-84db-4e08-b64f-b509af7cb6d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_name_words = [    \"aan\", \"aandeel\", \"aangifte\", \"aankomende\", \"afgeschreven\", \"akte\", \"als\",\n",
    "                      \"amsterdam\", \"augs\", \"augustus\", \"beheerders\", \"besmet\", \"bevolking\", \"bij\",\n",
    "                      \"blijkens\", \"broeder\", \"cod\", \"college\", \"collegie\", \"comm\", \"commissariaat\",\n",
    "                      \"commissaris\", \"commissarissen\", \"conditie\", \"curator\", \"curators\", \"custodi\",\n",
    "                      \"dd\", \"ddo\", \"de\", \"dec\", \"decbr\", \"decemb\", \"december\", \"decemr\", \"decr\",\n",
    "                      \"den\", \"deszelfs\", \"dezer\", \"dispositie\",\"dood\", \"door\", \"erf\", \"erfenis\",\n",
    "                      \"erfgenaam\", \"erfgenamen\", \"erven\", \"etablisement\", \"evangelische\", \"exc\",\n",
    "                      \"executie\", \"executie\", \"expl\", \"exploicteur\", \"exploieteur\", \"extract\", \"fo\",\n",
    "                      \"fort\", \"gekocht\", \"gemanumitteerd\", \"gemanumitteerde\", \"gemeente\", \"genomen\",\n",
    "                      \"geref\", \"geregd\", \"geregistreerd\", \"geregtshof\", \"geresolutie\",\n",
    "                      \"genl.staten\", \"gergd\", \"gr\", \"gouv\", \"gouverment\", \"gouverments\", \"gouvern\",\n",
    "                      \"gouvernts\", \"gouvernement\", \"gouvr\", \"hervormde\", \"het\", \"hijpotheek\", \"hoc\",\n",
    "                      \"hoge\", \"hoofdgelden\", \"ik\", \"in\", \"ingevolge\", \"inlandsche\", \"innocente\",\n",
    "                      \"janij\", \"januarij\", \"julij\", \"journaal\", \"kolonie\", \"kolonien\", \"kommandant\",\n",
    "                      \"kurators\", \"lande\", \"landen\", \"landsbelastingen\", \"maart\", \"meerderjarigen\",\n",
    "                      \"mei\", \"minderj\", \"minderjarige\", \"minderjarigen\", \"minderje\", \"minderjn\",\n",
    "                      \"nieuw\", \"no\", \"notarieele\", \"novemb\", \"novr\", \"onder\", \"opgever\",\n",
    "                      \"overleden\", \"paramaribo\", \"per\", \"pub\", \"publieke\", \"raad\", \"resol\",\n",
    "                      \"resolutie\", \"resolutie9\", \"respect\", \"septbr\", \"septemr\", \"slaaf\", \"slaven\",\n",
    "                      \"slavin\", \"testament\", \"testamentaire\", \"uit\", \"van\", \"vendu\", \"vonnis\",\n",
    "                      \"vendumeester\", \"veiling\", \"verbonden\",\"verklaard\", \"verkocht\", \"verpand\",\n",
    "                      \"vmr\", \"volg\", \"voor\", \"voorden\", \"vrij\", \"vrijdom\", \"vrijgeworden\",\n",
    "                      \"weduwe\", \"weesmeesteren\", \"weesmeesters\", \"zie\", \"zijn\", \"zijne\" ]\n",
    "prefix_name_words = [ \"boedel\", \"erven\", \"gebrs\", \"weduwe\", ]\n",
    "suffix_name_words = [ \"'anavia\", \"(…)ing\", \"bol\", \"de\", \"den\", \"en\", \"gaander\", \"geb\", \"geboren\", \n",
    "                      \"gebn\", \"green\", \"heilbron\", \"helb\", \"lande\", \"laurence\", \"meijers\", \"mers\",\n",
    "                      \"n\", \"nepveu\", \"nom\", \"osse\", \"petram\", \"pret\", \"qq\", \"salomons\", \"sanches\",\n",
    "                      \"u\", \"ux\", \"van\", \"vlier\", \"wolff\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa789aa-6433-4908-b9ca-bffce710d438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_list_of_words(text):\n",
    "    line = \"\"\n",
    "    for token in text.split():\n",
    "        if len(token) + len(line) > 99:\n",
    "            print(line)\n",
    "            line = \"\"\n",
    "        if line == \"\":\n",
    "            line = 21 * \" \"\n",
    "        line += \" \" + token\n",
    "    if len(line) > 0:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569abf98-6c3f-4b7b-9b21-2f7a9c7b0d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_initial_non_entity_words(text, entity):\n",
    "    first_token, end = get_first_token_of_entity(text, entity)\n",
    "    if (not first_token.lower() in non_name_words and \n",
    "        not regex.sub(r\"[.,]$\", \"\", first_token).lower() in non_name_words and \n",
    "        not regex.search(r\"^[^a-zA-Z]+$\", first_token)):\n",
    "        return entity\n",
    "    else:\n",
    "        return remove_initial_non_entity_words(text, remove_first_token_of_entity(text, entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e1df7-ebfc-4d4b-987c-5e0773fb31bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_final_non_entity_words(text, entity):\n",
    "    last_token, start = get_last_token_of_entity(text, entity)\n",
    "    if (not last_token.lower() in non_name_words and \n",
    "        not regex.sub(r\"[.,]$\", \"\", last_token).lower() in non_name_words and \n",
    "        not regex.search(r\"^[^a-zA-Z]+$\", last_token)):\n",
    "        return entity\n",
    "    else:\n",
    "        return remove_final_non_entity_words(text, remove_last_token_of_entity(text, entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f98d8-7e91-423e-bddf-18bc1725e4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_initial_entity_words(text, entity, last_entity_end):\n",
    "    previous_token, start = get_token_before_entity(text, entity)\n",
    "    if (not previous_token.lower() in prefix_name_words and \n",
    "        not regex.sub(r\"[.,]+$\", \"\", previous_token).lower() in prefix_name_words and\n",
    "        not regex.search(r\"^[A-Z]\\.?$\", previous_token) and\n",
    "        not regex.search(r\"^[A-Z]\\.[A-Z]\\.$\", previous_token)):\n",
    "        return entity\n",
    "    elif start <= last_entity_end:\n",
    "        return entity\n",
    "    else:\n",
    "        entity[\"start\"] = start\n",
    "        return add_initial_entity_words(text, entity, last_entity_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f7828-38c8-47ee-8411-176a763d1f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_final_entity_words(text, entity, next_entity_start):\n",
    "    next_token, end = get_token_after_entity(text, entity)\n",
    "    if (not next_token.lower() in suffix_name_words and \n",
    "        not regex.sub(r\"[.,]$\", \"\", next_token).lower() in suffix_name_words and\n",
    "        not regex.search(r\"^[A-Z]\\.?$\", next_token)):\n",
    "        return entity\n",
    "    elif end > next_entity_start:\n",
    "        return entity\n",
    "    else:\n",
    "        entity[\"end\"] = end\n",
    "        return add_final_entity_words(text, entity, next_entity_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cef05f-8d9f-4593-938b-f3edf5d22108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_next_entity_start(entities, text, i):\n",
    "    if i + 1 >= len(entities):\n",
    "        return len(text)\n",
    "    else:\n",
    "        return entities[i + 1][\"start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a73230-b8cd-4cc8-8607-e3af07ba3656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shrink_entities(entities):\n",
    "    for index in entities:\n",
    "        text = data[DATA_FIELD][index]\n",
    "        for entity in entities[index]:\n",
    "            entity = remove_initial_non_entity_words(data[DATA_FIELD][index], entity)\n",
    "            entity = remove_final_non_entity_words(data[DATA_FIELD][index], entity)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13e020-6618-41f9-b694-8c54edd6b117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_entities(entities):\n",
    "    for index in entities:\n",
    "        last_entity_end = 0\n",
    "        text = data[DATA_FIELD][index]\n",
    "        next_entity_start = get_next_entity_start(entities[index], text, 0)\n",
    "        for i in range(0, len(entities[index])):\n",
    "            entity = entities[index][i]\n",
    "            if entity[\"start\"] < entity[\"end\"]:\n",
    "                entity = add_initial_entity_words(data[DATA_FIELD][index], entity, last_entity_end)\n",
    "                entity = add_final_entity_words(data[DATA_FIELD][index], entity, next_entity_start)\n",
    "                next_entity_start = get_next_entity_start(entities[index], text, i + 1)\n",
    "                last_entity_end = entity[\"end\"]\n",
    "                entity[\"word\"] = data[DATA_FIELD][index][entity[\"start\"]: entity[\"end\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80010a16-1005-41fe-a30b-32f3aa86e659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_overlapping_entities(entities):\n",
    "    for i in range(0, len(entities)):\n",
    "        for j in range(0, len(entities)):\n",
    "            if i != j and (entities[i][\"start\"] <= entities[j][\"start\"] and entities[i][\"end\"] >= entities[j][\"end\"]):\n",
    "                print(f\"deleting entity {j}\\n\")\n",
    "                entities.pop(j)\n",
    "                return entities\n",
    "            elif i != j and (entities[i][\"start\"] >= entities[j][\"start\"] and entities[i][\"end\"] <= entities[j][\"end\"]):\n",
    "                print(f\"deleting entity {i}\\n\")\n",
    "                entities.pop(i)\n",
    "                return entities\n",
    "            elif i < j and entities[i][\"start\"] < entities[j][\"end\"] and entities[i][\"end\"] > entities[j][\"start\"]:\n",
    "                print(\"A\", entities[i], \"\\nB\", entities[j], \"\\n\")\n",
    "                return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179eb190-3ff6-450b-bf33-1a22314ef866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_for_overlapping_entities(entities):\n",
    "    for index in entities:\n",
    "        entity_characters = [ False ] * len(data[DATA_FIELD][index])\n",
    "        for entity in entities[index]:\n",
    "            for i in range(entity[\"start\"], entity[\"end\"]):\n",
    "                if entity_characters[i]:\n",
    "                    print(f\"overlapping entities for index {index}! {entities[index]}\")\n",
    "                    entities[index] = find_overlapping_entities(entities[index])\n",
    "                    break\n",
    "                entity_characters[i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4429210-893e-4c33-9564-35ffd7b6891b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_for_overlapping_entities(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e6ba0-b5b3-461c-9617-58c24bf1539a",
   "metadata": {},
   "source": [
    "## 6. Combine entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ff93b-ea63-4de6-ab07-851501970e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_birth_names(entities):\n",
    "    for index in entities:\n",
    "        birth_words = [ \"geboren\", \"geb\", \"gebn\", \"en\", ]\n",
    "        entities_to_combine = []\n",
    "        for i in range(1, len(entities[index])):\n",
    "            last_token, start = get_last_token_of_entity(data[DATA_FIELD][index], entities[index][i-1])\n",
    "            if ((entities[index][i-1][\"end\"] == entities[index][i][\"start\"] or \n",
    "                 entities[index][i-1][\"end\"] + 1 == entities[index][i][\"start\"]) and \n",
    "                (last_token.lower() in birth_words or \n",
    "                 regex.sub(r\"[.,]$\", \"\", last_token).lower() in birth_words)):\n",
    "                entities_to_combine.append(i)\n",
    "        for i in range(len(entities_to_combine) - 1, -1, -1):\n",
    "            entities[index][entities_to_combine[i]-1][\"end\"] = entities[index][entities_to_combine[i]][\"end\"]\n",
    "            entities[index].pop(entities_to_combine[i])\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4cec0-ae06-46c3-97ac-428b8893fc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities, last_index = get_entities_from_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6da10-431a-482f-ba90-ea03557c6193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities = shrink_entities(entities)\n",
    "entities = expand_entities(entities)\n",
    "entities = add_birth_names(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd3f7b-a8b6-4d86-b086-f255ef6492f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_entities(entities):\n",
    "    for index in entities:\n",
    "        for entity in entities[index]:\n",
    "            text = data[DATA_FIELD][index][entity[\"start\"]: entity[\"end\"]]\n",
    "            if text != \"\":\n",
    "                print (index, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94285f29-e33d-49b3-bd34-1d69db93e0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_entities(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b617942-685b-4aa7-ad60-2331d0de1d51",
   "metadata": {},
   "source": [
    "## 7. Combine entities (old code)\n",
    "\n",
    "Combination words:\n",
    "* en\n",
    "* geboren, geb, gebn\n",
    "\n",
    "Unknown abbreviation: vmr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f5885-ee96-4d2e-b7ee-8ad01d415dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_born_as(text, entities):\n",
    "    for i in range(0, len(entities)-1):\n",
    "        if regex.search(r\"^\\s*(geb|gebn|geboren).*\\s*$\",\n",
    "                        text[entities[i][\"end\"]: entities[i+1][\"start\"]]):\n",
    "            entities[i][\"end\"] = entities[i+1][\"end\"]\n",
    "            entities[i+1][\"start\"] =  entities[i+1][\"end\"]\n",
    "    return [entity for entity in entities if entity[\"start\"] != entity[\"end\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3c7de-171f-4edf-ac69-61e693289b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX_WORDS = [ \"boedel\", \"erven\", \"geb\", \"gebn\", \"geboren\", \"weduwe\", ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028884ef-5e4f-4206-89a8-9e7fa9d226f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_labels(entities):\n",
    "    for i in range(0, len(entities)):\n",
    "        if regex.search(r\"(PERSON|GPE|FAC)\", entities[i][\"entity\"]):\n",
    "             entities[i][\"label\"] = \"entity\"\n",
    "        else:\n",
    "             entities[i][\"label\"] = \"other\"        \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59449021-8201-4088-bed1-0634297f6a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_info_text(text, index=-1):\n",
    "    entities_out = []\n",
    "    last_end = -99\n",
    "    mother_seen = False\n",
    "    entities_in = get_entities_from_text(text)\n",
    "    for entity_in in entities_in:\n",
    "        if regex.search(\"(PERSON|GPE|FAC)\", entity_in[\"entity\"]):\n",
    "            previous_token, start = get_previous_token(text, entity_in)\n",
    "            if previous_token.lower() in PREFIX_WORDS:\n",
    "                entity_in[\"start\"] = start\n",
    "            entity_in_string = text[entity_in[\"start\"]: entity_in[\"end\"]]\n",
    "            if regex.search(\"^geb\", entity_in_string) and (entity_in[\"start\"] == last_end + 1 or\n",
    "                                                      entity_in[\"start\"] == last_end + 2):\n",
    "                entities_in[-1] = (entities_in[-1][0], entities_in[-1][1] + \" \" + entity_in_string, entities[-1][2])\n",
    "                continue\n",
    "            role = \"eigenaar\"\n",
    "            previous_token, start = get_previous_token(text, entity_in)\n",
    "            if regex.search(\"geboren (uit|van)\", text, regex.IGNORECASE) and not mother_seen:\n",
    "                role = \"moeder\"\n",
    "                mother_seen = True\n",
    "            elif regex.search(\"(gemanumitteerd|vrij *geworden)\", text, regex.IGNORECASE):\n",
    "                role = \"vrijgemaakte\"\n",
    "            elif regex.search(\"genaamd\", previous_token, regex.IGNORECASE):\n",
    "                role = \"vrijgemaakte\"\n",
    "            elif regex.search(\"(slaaf|slavin|slaven)\", previous_token, regex.IGNORECASE):\n",
    "                role = \"slaafgemaakte\"\n",
    "            elif regex.search(r\"(plant\\b|plantage|plantaadje|ple\\b|houtvelling|divisie|district)\",\n",
    "                              previous_token, regex.IGNORECASE):\n",
    "                role = \"location\"\n",
    "            elif regex.search(\"(curator|deurwaarder|klerk|landschrijver|vendumeester)\",\n",
    "                              previous_token, regex.IGNORECASE):\n",
    "                role = \"ambtenaar\"\n",
    "            entities_out.append({\"index\": index, \n",
    "                                 \"start\": entity_in[\"start\"], \n",
    "                                 \"end\": entity_in[\"end\"], \n",
    "                                 \"role\": role, \n",
    "                                 \"previous_token\": previous_token,\n",
    "                                 \"label\": \"name\",\n",
    "                                 \"word\": text[entity_in[\"start\"]: entity_in[\"end\"]]})\n",
    "            last_end = entity_in[\"end\"]\n",
    "    return entities_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51149b-fce5-425f-a45c-5f2951f1532e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch_add_initials(text, entities):\n",
    "    for entity in entities:\n",
    "        previous_token, start = get_previous_token(text, entity)\n",
    "        while regex.search(r\"^[A-Z]\\.*$\", previous_token) or regex.search(r\"^([A-Z]\\.)+$\", previous_token):\n",
    "            entity[\"start\"] = start\n",
    "            previous_token, start = get_previous_token(text, entity)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b0f876-efd1-440e-9dee-c77d1b5c4ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #entities = patch_born_as(text, entities)\n",
    "    #entities = patch_add_initials(text, entities)\n",
    "    #entities = add_labels(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2f68d-6615-4866-aebc-099a674e92b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities_per_index = {}\n",
    "for entity in entities:\n",
    "    if entity[\"index\"] in entities_per_index:\n",
    "        entities_per_index[entity[\"index\"]].append(entity)\n",
    "    else:\n",
    "        entities_per_index[entity[\"index\"]] = [entity]\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if isinstance(row[DATA_FIELD], str) and index <= CUT_OFF:\n",
    "        print(index, end=\" \")\n",
    "        if index in entities_per_index:\n",
    "            utils.render_text(row[DATA_FIELD], entities_per_index[index])\n",
    "        else:\n",
    "            utils.render_text(row[DATA_FIELD], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e2043-f537-4e97-9a4f-464b0e83bb82",
   "metadata": {},
   "source": [
    "## 8. Cleanup names (old code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69540fc2-55f2-4807-9ecf-b0cdd5ede5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup_name(name_string):\n",
    "    name_string = regex.sub(r\"[.,]\\s*\", \" \", name_string)\n",
    "    return name_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56111af-333a-44d3-9bc8-8b433af94dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prefixes_to_delete = [ \"en\",  \"kurators\", \"notarieele\", \"slavin\", ]\n",
    "suffixes_to_delete = [ \"/\", \"bij executie\", \"dd\", \"per executie\", \"febrij\", \"kolonien\",\n",
    "                       \"landsbelastingen\", \"resolutie\", \"voor den vrijdom\", \"vrijdom\", \"den\",\n",
    "                       \"voor\", ]\n",
    "non_names = [ \"\", \",\", \"akte\", \"aug\", \"augs\", \"august\", \"augusts\", \"augustus\", \"boedel\",\n",
    "              \"boven\", \"custodi\", \"dec\", \"decbr\", \"decemb\", \"december\", \"decemr\",\"den lande\",\n",
    "              \"dezer\", \"erfenis\", \"erfgenaam\", \"erfgename\", \"erfgenamen\", \"executie\", \"febr\",\n",
    "              \"febrij\", \"fo\", \"folio\", \"gemanumitteerd\", \"Gemmanumitteerd\", \"genomen\",\n",
    "              \"geregistreerd\", \"geregd\", \"geresolutie\", \"gergd\", \"gouv\", \"gouv resol\", \"gouvern\",\n",
    "              \"gouvernement\", \"gouvernts\", \"hoofdgelden\", \"janij\", \"januarij\", \"julij\", \"junij\",\n",
    "              \"kolonien\", \"kurators\", \"lande\", \"lot no\", \"maart\", \"no\", \"notarieele\", \"novemb\",\n",
    "              \"october\", \"overleden\", \"overschrijving\", \"plant\", \"resolutie\", \"ruiling\", \"slaaf\",\n",
    "              \"suriname\", \"vendu\", \"vendumeester\", \"vrijdom\", \"zn\", \"zijne\", ]\n",
    "locations = [ \"batavia\", \"nickerie\", \"vreeland\", \"spieringshoek\", \"kroonenburg\", \"caledonia\",\n",
    "              \"molhoop\", \"libanon\", \"saltzhalen\", \"waterloo\", \"fairfield\", \"amsterdam\",\n",
    "              \"paramaribo\", \"paradize\", \"felix\", \"dordrecht\", \"tourtonne\", \"lochaber\",\n",
    "              \"leliendaal\", \"bremen\", \"lugtenburg\", \"saramacca\", \"zeezigt\", \"munnikkendam\",\n",
    "              \"zwarigheid\", \"katwijk\", \"hooijland\", \"poelwijk\", \"alkmaar\", \"waijamoe\",\n",
    "              \"petersburg\", \"johannesburg\", \"toledo\", \"ornamibo\", \"sardam\", \"coronie\", \"saksen\",\n",
    "              \"thorarica\", \"curaçao\", \"cottica\", \"andresa\", \"curacao\", \"suriname\", ]\n",
    "\n",
    "\n",
    "def get_names_from_string(name_string):\n",
    "    names = []\n",
    "    for non_name in non_names + locations:\n",
    "        match = regex.search(f\"^{non_name}[^a-zA-Z]*$\", name_string, regex.IGNORECASE)\n",
    "        if match:\n",
    "            return []\n",
    "    match = regex.search(f\"^(.*)\\\\s*[0-9]+\\\\s*(.*)$\", name_string, regex.IGNORECASE)\n",
    "    if match:\n",
    "        names.extend(get_names_from_string(match.group(1)))\n",
    "        names.extend(get_names_from_string(match.group(2)))\n",
    "        return names\n",
    "    for prefix in prefixes_to_delete:\n",
    "        match = regex.search(f\"^(.*)\\\\s+{prefix}\\\\s+(.*)$\", name_string, regex.IGNORECASE)\n",
    "        if match:\n",
    "            names.extend(get_names_from_string(match.group(1)))\n",
    "            names.extend(get_names_from_string(match.group(2)))\n",
    "            return names\n",
    "        match = regex.search(f\"^{prefix}\\\\s+(.*)$\", name_string, regex.IGNORECASE)\n",
    "        if match:\n",
    "            return get_names_from_string(match.group(1))\n",
    "    for suffix in suffixes_to_delete:\n",
    "        match = regex.search(f\"^(.*)\\\\s+{suffix}\\\\s+(.*)$\", name_string, regex.IGNORECASE)\n",
    "        if match:\n",
    "            names.extend(get_names_from_string(match.group(1)))\n",
    "            names.extend(get_names_from_string(match.group(2)))\n",
    "            return names\n",
    "        match = regex.search(f\"^(.*)\\\\s+{suffix}$\", name_string, regex.IGNORECASE)\n",
    "        if match:\n",
    "            return get_names_from_string(match.group(1))\n",
    "    names.append(name_string)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbe829-42f6-4d6e-8ee3-a3954468a060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_names_from_string_with_punctuation(entity, text):\n",
    "    entity_parts = []\n",
    "    word_parts = entity[\"word\"].split()\n",
    "    for word_part in word_parts:\n",
    "        if len(entity_parts) == 0:\n",
    "            entity_parts.append({\"start\": entity[\"start\"], \"end\": entity[\"start\"] + len(word_part), \"role\": entity[\"role\"]})\n",
    "        elif not regex.search(\"^(qq|en)$\", word_part) and len(entity_parts) > 0:\n",
    "            entity_parts[-1][\"end\"] += len(word_part) + 1\n",
    "        if regex.search(\"^(qq|en)$\", word_part) or (len(entity_parts) > 0 and\n",
    "                                                    text[entity_parts[-1][\"end\"] - 1] == \",\"):\n",
    "            entity_parts.append({\"start\": entity_parts[-1][\"end\"] + 1, \"end\": entity_parts[-1][\"end\"] + 1, \"role\": entity[\"role\"]})\n",
    "    return entity_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513cfaba-2868-4254-8815-eb03bb5959a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_name_default(entity, text):\n",
    "    if entity[\"start\"] >= entity[\"end\"]:\n",
    "        first_name_tokens = []\n",
    "        last_name_tokens = []\n",
    "    else:\n",
    "        name_tokens = data[DATA_FIELD][entity[\"index\"]].split()\n",
    "        first_name_tokens = name_tokens[:-1]\n",
    "        last_name_tokens = name_tokens[-1:]\n",
    "        if len(first_name_tokens) == 1 and first_name_tokens[0].lower() in non_names:\n",
    "            first_name_tokens, last_name_tokens = split_name_default(\" \".join(last_name_tokens))\n",
    "        elif len(last_name_tokens) == 1 and last_name_tokens[0].lower() in non_names:\n",
    "            first_name_tokens, last_name_tokens = split_name_default(\" \".join(first_name_tokens))\n",
    "    return first_name_tokens, last_name_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6e0b8-6f34-408f-8297-1ae253b22bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_name_words = [ \"bo\", \"d\", \"da\", \"de\", \"den\", \"der\", \"du\", \"geb\", \"gebn\", \"geboren\", \"nom\",\n",
    "                    \"prive\", \"privé\", \"v\", \"van\", \"ux\", ]\n",
    "\n",
    "def expand_multi_token_last_name(first_name_tokens, last_name_tokens):\n",
    "    for i in range(0, len(first_name_tokens)):\n",
    "        if (first_name_tokens[i].lower() in last_name_words and\n",
    "            (not len(first_name_tokens[i]) == 1 or\n",
    "             first_name_tokens[i].lower() == first_name_tokens[i])):\n",
    "            while len(first_name_tokens) > i:\n",
    "                last_name_tokens = [ first_name_tokens.pop(-1)] + last_name_tokens\n",
    "            break\n",
    "    return first_name_tokens, last_name_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750afcc-4441-4cec-8a07-c2ec5dad5d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def include_last_name_before_born_as(first_name_tokens, last_name_tokens):\n",
    "    if (len(last_name_tokens) > 0 and len(first_name_tokens) > 0 and\n",
    "        regex.search(r\"^(geb|gebn|geboren|beh|jr|sr)\\b\", last_name_tokens[0], regex.IGNORECASE)):\n",
    "        last_name_tokens = [first_name_tokens.pop(-1)] + last_name_tokens\n",
    "    return first_name_tokens, last_name_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ce978-aacc-4425-b72d-864c67ecee0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def include_last_name_before_nom_ux(first_name_tokens, last_name_tokens):\n",
    "    if (len(last_name_tokens) > 0 and len(first_name_tokens) > 1 and\n",
    "        regex.search(\"^ux\\\\b\", last_name_tokens[0], regex.IGNORECASE) and\n",
    "        regex.search(\"^(n|nom)\\\\b\", first_name_tokens[-1], regex.IGNORECASE)):\n",
    "        last_name_tokens = [first_name_tokens.pop(-1)] + last_name_tokens\n",
    "        last_name_tokens = [first_name_tokens.pop(-1)] + last_name_tokens\n",
    "    return first_name_tokens, last_name_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576397d6-f57e-4b00-a416-e0a8736ec475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_entity(string, label):\n",
    "    return { \"word\": string, \"start\": 0, \"end\": len(string), \"label\": label }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321cdf6-d9c9-4923-8c37-5e9f58897ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_name(entity, text):\n",
    "    first_name_tokens, last_name_tokens = split_name_default(entity, text)\n",
    "    #first_name_tokens, last_name_tokens = expand_multi_token_last_name(first_name_tokens,\n",
    "    #                                                                   last_name_tokens)\n",
    "    #first_name_tokens, last_name_tokens = include_last_name_before_born_as(first_name_tokens,\n",
    "    #                                                                       last_name_tokens)\n",
    "    #first_name_tokens, last_name_tokens = include_last_name_before_nom_ux(first_name_tokens,\n",
    "    #                                                                      last_name_tokens)\n",
    "    if role == \"eigenaar\" or len(first_name_tokens) != 0:\n",
    "        return make_entity(\" \".join(first_name_tokens), \"voornaam\"), make_entity(\" \".join(last_name_tokens), \"achternaam\")\n",
    "    else:\n",
    "        return make_entity(\" \".join(last_name_tokens), \"voornaam\"), make_entity(\" \".join(first_name_tokens), \"achternaam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486ba93-823e-49d0-a42f-a8a2ff621dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PREFIX_TOKENS = [ \"bl\", \"boedel\", \"erven\", \"mr\", \"we\", \"weduwe\", ]\n",
    "\n",
    "def get_prefix_tokens(first_name_tokens):\n",
    "    prefix_tokens = []\n",
    "    while len(first_name_tokens) > 0 and first_name_tokens[0].lower() in PREFIX_TOKENS:\n",
    "        prefix_tokens.append(first_name_tokens.pop(0))\n",
    "    return prefix_tokens, first_name_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609af5f7-12b8-4862-9b1a-27e9ca996736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFIX_TOKENS = [ \"d'\", \"da\", \"de\", \"del\", \"den\", \"der\", \"des\", \"du\", \"d'\", \"het\", \"la\", \"du\",\n",
    "                 \"l'\", \"la\", \"le\", \"'t\", \"ter\", \"v\", \"van\", \"von\" ]\n",
    "\n",
    "def get_infix_tokens(last_name_tokens):\n",
    "    infix_tokens = []\n",
    "    while len(last_name_tokens) > 0 and last_name_tokens[0].lower() in INFIX_TOKENS:\n",
    "        infix_tokens.append(last_name_tokens.pop(0))\n",
    "    return infix_tokens, last_name_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc150d8a-dd22-4aa1-b861-67181ce63092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUFFIX_TOKENS = [ \"beh\", \"cs\", \"jr\", \"nom\", \"n\", \"prive\", \"qq\", \"sr\", \"ux\" ]\n",
    "\n",
    "def get_suffix_tokens(last_name_tokens):\n",
    "    suffix_tokens = []\n",
    "    while len(last_name_tokens) > 0 and last_name_tokens[-1].lower() in SUFFIX_TOKENS:\n",
    "        suffix_tokens.insert(0, last_name_tokens.pop(-1))\n",
    "    return last_name_tokens, suffix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974511e-40d5-4449-93d4-49f0ab7adbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_name_in_five(first_name, last_name):\n",
    "    prefix_tokens, first_name_tokens = get_prefix_tokens(first_name.split())\n",
    "    infix_tokens, last_name_tokens = get_infix_tokens(last_name.split())\n",
    "    last_name_tokens, suffix_tokens = get_suffix_tokens(last_name_tokens)\n",
    "    return(make_entity(\" \".join(prefix_tokens), \"prefix\"), \n",
    "           make_entity(\" \".join(first_name_tokens), \"voornaam\"),\n",
    "           make_entity(\" \".join(infix_tokens), \"infix\"),\n",
    "           make_entity(\" \".join(last_name_tokens), \"achternaam\"),\n",
    "           make_entity(\" \".join(suffix_tokens), \"suffix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf8d8b-1ffd-4710-87a5-c12efe58391e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_names(results):\n",
    "    results.to_csv(DATA_FIELD + \".csv\", index=False, columns=results.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86023b5f-358a-4576-b4d0-dd239a83fbb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_names(entities):\n",
    "    table_out = []\n",
    "    for entity in [entity for entity in entities if entity[\"role\"] != \"location\"]:\n",
    "        text = data[DATA_FIELD][entity[\"index\"]]\n",
    "        for entity_part in get_names_from_string_with_punctuation(entity, text):\n",
    "            name_string = text[entity_part[\"start\"]: entity_part[\"end\"]]\n",
    "            first_name, last_name = split_name(entity_part, text)\n",
    "            prefix, first_name, infix, last_name, suffix = split_name_in_five(first_name[\"word\"],\n",
    "                                                                              last_name[\"word\"])\n",
    "            if first_name[\"word\"] != \"\" or last_name[\"word\"] != \"\":\n",
    "                table_out.append([entity[\"index\"], prefix[\"word\"], first_name[\"word\"], infix[\"word\"],\n",
    "                                                   last_name[\"word\"], suffix[\"word\"], entity[\"role\"],\n",
    "                                                   text])\n",
    "    return pd.DataFrame(table_out, columns=[\"id\", \"prefix\", \"voornaam\", \"infix\",\n",
    "                                            \"achternaam\", \"suffix\", \"rol\",\n",
    "                                            \"tekstbron\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44898bcc-582f-4bb0-96b7-3895bcd70ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = parse_names(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f376d-7fe5-4c97-9e83-dfbaa9389fdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_names(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca63f2a-f073-49d9-af7f-a56aed67d65d",
   "metadata": {},
   "source": [
    "## 9. Check names (old code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ffc1b-bd69-405b-836d-74611a1ad3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_preceding_tokens(names):\n",
    "    return pd.DataFrame(names)[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7491bdef-1712-4826-99dc-7cd33d6b3f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_names_without_first_name(results):\n",
    "    empty_first_name = []\n",
    "    for index, row in results.iterrows():\n",
    "        if row[\"first_name\"] == \"\":\n",
    "            empty_first_name.append(row[\"last_name\"])\n",
    "    return pd.DataFrame(empty_first_name).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a99f7d-b365-40de-9f29-90a8cd0ce778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_names_without_last_name(results):\n",
    "    empty_last_name = []\n",
    "    for index, row in results.iterrows():\n",
    "        if row[\"last_name\"] == \"\":\n",
    "            empty_last_name.append(row[\"first_name\"])\n",
    "    return pd.DataFrame(empty_last_name).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeff58c-6d37-4efc-8e09-5298d760d2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_first_names(results):\n",
    "    return results[\"first_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47690906-1ec5-46f0-81c3-230eab18be91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_last_names(results):\n",
    "    return results[\"last_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a9943-fea3-4364-ba7b-cb0f4735bbe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_string_in_results(results, string, field=\"source\"):\n",
    "    selected = []\n",
    "    for index,row in results.iterrows():\n",
    "        if regex.search(string, row[field], regex.IGNORECASE):\n",
    "            selected.append(row)\n",
    "    return pd.DataFrame(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faea3e-3a90-4d69-b380-66badcbe04da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_preceding_tokens(names).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f314725-9d4e-4d33-995e-1a82f6ca52a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "find_string_in_results(results, \"zn\", \"last_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926873c-2bf8-49e4-ae19-c7e8a528ba5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[DATA_FIELD][189873]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed1703-3949-464e-8033-9e41f764d737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_names_without_first_name(results).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836f19c-2a84-43bf-9955-a8d89500c620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_info_text(\"Vrij geworden en thans genaamd François Jacobus Hendrik Roosdijk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674dc50-8046-446c-b62e-682088315423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_deceased_name.get_entities_from_text(data[DATA_FIELD][3310])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41bfb2-a329-49fe-bc99-301d9aa991e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for text in data[DATA_FIELD]:\n",
    "    if isinstance(text, str) and regex.search(\"district\", text):\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360eae7-1623-4957-9e86-8b53b0781000",
   "metadata": {},
   "source": [
    "## 99. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5d0dd9-4e07-4623-b50b-c6cab8f4e399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8beb35-f0cf-498d-b560-9d5d3166493c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestNotebook(unittest.TestCase):    \n",
    "    def test_split_name(self):\n",
    "        self.assertEqual(split_name(\"boedel weduwe Adriana Augusta van Dam qq\", \"eigenaar\"),\n",
    "                         ('boedel weduwe Adriana Augusta', 'van Dam qq') )\n",
    "\n",
    "        \n",
    "    def test_split_name_in_five(self):\n",
    "        self.assertEqual(split_name_in_five(\"boedel weduwe Adriana Augusta\", \"van Dam qq\"),\n",
    "                         ('boedel weduwe', 'Adriana Augusta', 'van',  'Dam',  'qq') )\n",
    "\n",
    "        \n",
    "    def test_patch_born_as(self):\n",
    "        self.assertEqual(patch_born_as(\"Jansen geboren De Vries\", [{ \"start\": 0, \"end\": 6 }, { \"start\": 15, \"end\": 23 }]),\n",
    "                         [{\"start\": 0, \"end\": 23}] )\n",
    "        \n",
    "        \n",
    "    def test_get_token_before_entity(self):\n",
    "        self.assertEqual(get_token_before_entity(\"one two three\", { \"start\": 4, \"end\": 7 }),\n",
    "                         ( \"one\", 0 ))\n",
    "        self.assertEqual(get_token_before_entity(\"one two three\", { \"start\": 0, \"end\": 3 }),\n",
    "                         ( \"\", -1 ))\n",
    "        self.assertEqual(get_token_before_entity(\" one two three\", { \"start\": 1, \"end\": 4 }),\n",
    "                         ( \"\", -1 ))\n",
    "        self.assertEqual(get_token_before_entity(\"one two three\", { \"start\": 1, \"end\": 3 }),\n",
    "                         ( \"o\", 0 ))\n",
    "        \n",
    "        \n",
    "    def test_get_token_after_entity(self):\n",
    "        self.assertEqual(get_token_after_entity(\"one two three\", { \"start\": 4, \"end\": 7 }),\n",
    "                         ( \"three\", 13 ))\n",
    "        self.assertEqual(get_token_after_entity(\"one two three\", { \"start\": 8, \"end\": 13 }),\n",
    "                         ( \"\", 14 ))\n",
    "        self.assertEqual(get_token_after_entity(\"one two three \", { \"start\": 8, \"end\": 13 }),\n",
    "                         ( \"\", 15 ))\n",
    "        self.assertEqual(get_token_after_entity(\"one two three\", { \"start\": 8, \"end\": 12 }),\n",
    "                         ( \"e\", 13 ))\n",
    "        \n",
    "        \n",
    "    def test_get_first_token_of_entity(self):\n",
    "        self.assertEqual(get_first_token_of_entity(\"one two three\", { \"start\": 4, \"end\": 13 }),\n",
    "                         ( \"two\", 7 ))\n",
    "        self.assertEqual(get_first_token_of_entity(\"one two three\", { \"start\": 5, \"end\": 13 }),\n",
    "                         ( \"wo\", 7 ))\n",
    "        self.assertEqual(get_first_token_of_entity(\"one two three \", { \"start\": 3, \"end\": 13 }),\n",
    "                         ( \"two\", 7 ))\n",
    "        self.assertEqual(get_first_token_of_entity(\"one two three \", { \"start\": 3, \"end\": 7 }),\n",
    "                         ( \"two\", 7 ))\n",
    "        \n",
    "        \n",
    "    def test_get_last_token_of_entity(self):\n",
    "        self.assertEqual(get_last_token_of_entity(\"one two three\", { \"start\": 0, \"end\": 7 }),\n",
    "                         ( \"two\", 4 ))\n",
    "        self.assertEqual(get_last_token_of_entity(\"one two three\", { \"start\": 0, \"end\": 6 }),\n",
    "                         ( \"tw\", 4 ))\n",
    "        self.assertEqual(get_last_token_of_entity(\"one two three \", { \"start\": 0, \"end\": 8 }),\n",
    "                         ( \"two\", 4 ))\n",
    "        self.assertEqual(get_last_token_of_entity(\"one two three \", { \"start\": 4, \"end\": 7 }),\n",
    "                         ( \"two\", 4 ))\n",
    "\n",
    "        \n",
    "    def test_remove_first_token_of_entity(self):\n",
    "        self.assertEqual(remove_first_token_of_entity(\"one two three\", { \"start\": 4, \"end\": 13 }),\n",
    "                         { \"start\": 8, \"end\": 13, \"word\": \"three\" })\n",
    "        self.assertEqual(remove_first_token_of_entity(\"one two three\", { \"start\": 5, \"end\": 13 }),\n",
    "                         { \"start\": 8, \"end\": 13, \"word\": \"three\" })\n",
    "        self.assertEqual(remove_first_token_of_entity(\"one two three \", { \"start\": 3, \"end\": 13 }),\n",
    "                         { \"start\": 8, \"end\": 13, \"word\": \"three\" })\n",
    "        self.assertEqual(remove_first_token_of_entity(\"one two three \", { \"start\": 4, \"end\": 7 }),\n",
    "                         { \"start\": 7, \"end\": 7, \"word\": \"\" })\n",
    "        \n",
    "        \n",
    "    def test_remove_last_token_of_entity(self):\n",
    "        self.assertEqual(remove_last_token_of_entity(\"one two three\", { \"start\": 0, \"end\": 7 }),\n",
    "                         { \"start\": 0, \"end\": 3, \"word\": \"one\" })\n",
    "        self.assertEqual(remove_last_token_of_entity(\"one two three\", { \"start\": 0, \"end\": 6 }),\n",
    "                         { \"start\": 0, \"end\": 3, \"word\": \"one\" })\n",
    "        self.assertEqual(remove_last_token_of_entity(\"one two three \", { \"start\": 0, \"end\": 8 }),\n",
    "                         { \"start\": 0, \"end\": 3, \"word\": \"one\" })\n",
    "        self.assertEqual(remove_last_token_of_entity(\"one two three \", { \"start\": 4, \"end\": 7 }),\n",
    "                         { \"start\": 4, \"end\": 4, \"word\": \"\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c32fbe-3a83-4b4c-9c4c-0a11033f650a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7a40e-6d50-4fd2-b784-40b22f33386b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
